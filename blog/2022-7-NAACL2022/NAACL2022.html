<!doctype html>


<a href="https://sabirdvd.github.io" class="button"> <small>↩ </small></a>
<link rel="stylesheet" href="https://use.fontawesome.com/releases/v5.6.1/css/all.css">
<meta charset="utf-8">
<script src="template.v1.js"></script>
<script type="text/front-matter">
  title: "NAACL 2022-interesting-papers"
  authors:
  - Ahmed Sabir: https://www.cs.upc.edu/~asabir/
  - PUBLISHED Date
  affiliations:
  - Universitat Politècnica de Catalunya: https://www.upc.edu
  -  25 July  2022 
 
  

</script>
<script type="text/javascript">
    function zoom() {
        document.body.style.zoom = "80%" 
    }
</script>

<body onload="zoom()">



<style>
    .blue-color {
        color: blue;
    }
    
    .green-color {
        color: green;
    }
    
    .teal-color {
        color: teal;
    }
    
    .yellow-color {
        color: yellow;
    }
    
    .red-color {
        color: red;
    }
</style>

<head>
    <style>
        .noteBoxes {
            border: 1px solid;
            border-radius: 5px;
            padding: 10px;
            margin: 10px 0;
            width: 600px;

            
            
        }
      
        @media screen and (max-width: 540px) {
    .view {
        width: 400px;
    }
}
  

        .type1 {
            border-color: #E76F51;
            background-color: rgba(231, 111, 81, 0.1);
        }
        
        .type2 {
            border-color: #2A9D8F;
            background-color: rgba(42, 157, 143, 0.1);
        }
        
        .type3 {
            border-color: #0096C7;
            background-color: rgba(0, 150, 199, 0.1);
        }
        
        .type4 {
            border-color: #00B353;
            background-color: rgba(0, 179, 83, 0.1);
        }
        
        .picture {
            width: 15px;
            padding-right: 10px;
        }
    }
    </style>
</head>

<style>
    .geeks {
        caption-side: bottom;
    }
</style>

        
<dt-article class="centered">

    <h2>
        <p align="center"> NAACL \(2022\) Interesting Papers & Workshop
            <p>
    </h2>
    <dt-byline></dt-byline>

    <head>
        <meta charset="utf-8">
        <meta name="viewport" content="width=device-width">
        <title>MathJax example</title>
        <script src="https://polyfill.io/v3/polyfill.min.js?features=es6"></script>
        <script id="MathJax-script" async src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js">
        </script>
    </head>

    <body>
        <p>
            
           

            <p>In this short blog post, I will highlight a couple of interesting ideas presented in NAACL \(2022\).</p>

            


            <p>&clubs;<strong> MCSE: Multimodal Contrastive Learning of Sentence Embedding</strong> <dt-cite key="zhang2022mcse"></dt-cite> <a href="https://github.com/uds-lsv/MCSE"><i class="fab fa-github" style='font-size:20px'></i></a></p>


           <p>Humans rely on different modalities such as vision, language, and sound to capture an experience.
                Recent works  <dt-cite key="li2020unimo,lu2022imagination,li2022blip"></dt-cite> successfully show that by integrating visual semantics into a pre-trained language model 
                will improve its generalization capability and thus improve performance on various NLP tasks such 
                as textual semantic similarity. </p>
            
            
            

           

            <p> Following previous trends, this work extends an existing sentence embedding method (i.e., SimSCE 
                <dt-cite key="gao2021simcse"></dt-cite>) with visual information. The results show that adding visual information to a pre-trained sentence embedding model can be beneficial to learn a better sentence representation. Also, it can improve the alignment and maintain the uniformity of the embedding space.
               
                
            </p>
            
            <p> The unsupervised SimSCE is used with dropout noise data augmentation. The model encodes the sentence 
            twice using  different dropout masks. For given  sentence \(x_{i}\), the sentence is encoded twice via dropout masks 
            \(\boldsymbol{h}_{i}^{z}=g_{\phi}\left(f_{\theta}\left(x_{i}, z\right)\right)\) and  \(\boldsymbol{h}_{\dot{i}}^{z^{\prime}}=g_{\phi}\left(f_{\theta}\left(x_{i}, z^{\prime}\right)\right)\)
            where \(z\) and \(z^{\prime}\) refer to different dropout masks, \(f_{\theta}\)  is a pre-trained language encoder BERT   and \(g_{\phi}\) 
            is the projection of the head after the <tt> [CLS]</tt> token. The training object can be written as:
            
            </p>
            \[
            \small
            \ell_{i}^{S}=-\log \frac{e^{\operatorname{sim}\left(\mathbf{h}_{i}^{z_{i}}, \mathbf{h}_{i}^{z_{i}^{\prime}}\right) / \tau}}{\sum_{j=1}^{N} e^{\operatorname{sim}\left(\mathbf{h}_{i}^{z_{i}}, \mathbf{h}_{j}^{z_{j}^{\prime}}\right) / \tau}}  \]
            
            
            <p>where \(N \) is the  mini-batch size, τ is the softmax temperature parameter and \(sim\) is the cosine similarity. </p>
            
            
            
            <p> The next part is the integration of the visual feature. This work proposed a  multimodal objective within the contrastive learning framework. 
                For a given sentence-image  \(x,y \)  first they map  \(x_{i} \) \( y_{i} \) into shared space:  
            </p>
            
            \[
            \small
            \boldsymbol{s}_{i}^{z}=g_{\phi_{1}}\left(f_{\theta}\left(x_{i}, z\right)\right), \boldsymbol{v}_{i}=g_{\phi_{2}}\left(f^{v}\left(y_{i}\right)\right)
            \]
            
            <p>  where \(f^{v}(\cdot) \) is a fixed pre-trained image encoder ResNet <dt-cite key="he2016deep"></dt-cite>, and \(g_{\phi_{1}}(\cdot)\), \(g_{\phi_{2}}(\cdot)\) are the projection head between 
                the two modality. The projection head is a single-layer MLPs with Tanh activation. 
            
              
                The multimodal contrastive learning objective can be written as:
            </p>
            
            
            \[ \small
            \ell_{i}^{\mathrm{M}}=-\sum_{\mathrm{z} \in\left\{z_{i}, z_{i}^{\prime}\right\}} \log \frac{e^{\operatorname{sim}\left(\mathbf{s}_{i}^{z}, \mathbf{v}_{i}\right) / \tau^{\prime}}}{\sum_{j=1}^{N} e^{\operatorname{sim}\left(\mathbf{s}_{i}^{z}, \mathbf{v}_{j}\right) / \tau^{\prime}}} \]
            
            
            <p> The \( \lambda \) is the trade-off hyperparameter between the two objectives. The final loss with the 
                two objectives:
            </p>
            
            \[ \small  \ell_{i}=\ell_{i}^{S}+\lambda \ell_{i}^{M} \]
            
           <p> Table 1 below shows the performance comparison on STS-B and SICK-R: SICK-Relatedness tasks. 
               The results show the benefits of these visual 
               representations in non-visual downstream tasks such as STS-B.  </p>
            
            <!--
                  <figure>
              <img src="XDBERT.jpg" alt="Trulli" style="width:95%">
              <figcaption> Figure. Illustration of the proposed process to inject BERT with visual information. (a) Pre-training stage,
                  (2) Adaptation stage, and then (3) Fine-tuning on downstream tasks.  </figcaption>
            </figure>
            --> 
            <center>  
            
            <table>
              <tr>
            
                        <td> Model </td>
                <td style="text-align:center">　STSB</td>
                  <td style="text-align:center">　SICK-R</td>
            
              </tr>
            
                      <tr>
                       <td>　SimCSE-RoBERTa </td>
                      <td style="text-align:center">　76.8</td>
                     <td style="text-align:center">　65.7 </td>
                </tr>
            
                <tr>
                       <td>　MCSE-RoBERTa </td>
                      <td style="text-align:center">　<strong>70.2</strong> </td>
                     <td style="text-align:center">　 <strong>69.9 </strong> </td>
                </tr>
            
            
                <caption> <font color="#6e6e6f"> <small> <small> Table 1. Performance comparison Between SimCSE and MCSE on 
                    STS-B tasks and SICK-Relatedness tasks. <small>  <small></font> </caption>
            
              </table>
            </center>


            <p> Also, the model is able to retrieve more descriptive captions as shown in Table 2 below. </p>
            
            </p>
             <center>  
            <table align="center">
                <tr>
            
            
                    <td style="text-align:center">　Model</td>
                    <td style="text-align:center">　Caption</td>
                </tr>
                <tr>
                    <td style="text-align:center">　Query</td>
                    <td style="text-align:left">　A young girl is washing her teddy bear in the kitchen sink.</td>
                </tr>
            
            
                <tr>
                    <td style="text-align:center">　SimCSE </td>
                    <td style="text-align:left">　A middle-aged woman is vacuuming her kitchen floor with a canister vac.</td>
                </tr>
            
                <tr>
                    <td style="text-align:center">　MCSE </td>
                    <td style="text-align:left">A young girl, blond and wearing a polka-dot shirt, washes a stuffed animal.</td>
                    <td></td>
            
                </tr>
              
            
                <caption>
                    <font color="#6e6e6f"> <small> <small> Table 2. Retrieved examples comparison between SimCSE and MCSE on Flickr30k.  <small>  <small></font> </caption>
            
            </table>
            
        </center>
                
            <hr>       

            
      
            <p>&clubs;<strong> DiffCSE: Difference-based Contrastive Learning for Sentence Embeddings</strong>
                <dt-cite key="chuang2022diffcse"></dt-cite> <a href="https://github.com/voidism/DiffCSE"><i class="fab fa-github" style='font-size:20px'></i></a></p>

                
                

<p> Data augmentation with contrastive learning shows remarkable results in computer vision <dt-cite key="chen2020simple"></dt-cite>. 
    However, text augmentation in NLP (e.g., deletion, replacement) often changes the meaning of the sentence and  thus hurt the accuracy. 
    The work of SimSCE <dt-cite key="gao2021simcse"></dt-cite> found that constructing positive pairs via a 
    simple dropout-based augmentation works much better than more complex augmentations such as 
    word deletions or replacements based on synonyms or masked language models.  
    <!--The ideal sentence should not be invariant augmentation. --> 

     <p> This work uses equivariant contrastive learning  from Computer Vision <dt-cite key="dangovski2021equivariant"></dt-cite> 
        which improves vision representation learning by using a contrastive loss on insensitive image transformations
        (e.g., grayscale). The same idea is applied to NLP by employing  a generative model like  
        ELECTRA <dt-cite key="clark2020electra"></dt-cite>  to predict  different  outputs  that are equivariant to Mask Language Model. 
    
</p>

    
     <!--
     <p> what transformations should the representations be insensitive? The choice of making features sensitive or insensitive to a particular  group of transformations can have a substantial effect of the prefromace of downstream tasks. </p>     
     -->
    </p>
   

   

     <figure align="center">
        <img src="SimSCE_en.png" alt="Trulli" style="width:95%">
        <figcaption> Figure 1. Overview of DiffCSE. The left side is the standard SimCSE via dropout with regular contrastive loss. 
            The right side is the proposed model using a condition ELECTRA model to predict different \(x\) and \(x^\prime\) 
            from the input sentence vector \(h\). 

            
        </figcaption>
      </figure>
      


     
    
    </p>

     
      <p>  </p>

<!--
\[\small 
f(T(x))=T^{\prime}(f(x)) \text {. }
\]
--> 
 <p> In particular, they proposed an extension to SimSCE model with successful data 
     augmentation with different prediction objectives that condition to the sentence embedding. For a given input sentence \(x\) the 
     SimCSE provides a positive example via dropout masks and the same training objective: 


 </p>


      \[\small
      \mathcal{L}_{\text {contrast }}=-\log \frac{e^{\operatorname{sim}\left(\mathbf{h}_{i}, \mathbf{h}_{i}^{+}\right) / \tau}}{\sum_{j=1}^{N} e^{\operatorname{sim}\left(\mathbf{h}_{i}, \mathbf{h}_{j}^{+}\right) / \tau}},
      
      \]

      <p> where \(N\) is the batch size for input batch, \(sim\) is the cosine similarity
    function, and \(\tau\) is a hyperparameter. Then, As shown in Figure 1, to generate different conditional predictions  an ELECTRA model <dt-cite key="clark2020electra"></dt-cite> is used,
         which  contains a generator and discriminator. For a given 
        sentence with length \(T\), \(x= x_{1}, x_{2},..,x_{T}\), a random mask is applied
        \(m= m_{1}, m_{2},.., m_{T}\), \(m_{t}\) \(\in[0,1]\) on \(x\) to obtain \(x^\prime =m \cdot x\).
        Then, another BERT model (MLM) is employed to generate \(G\) to recover the randomly 
        masks token \(x^\prime\) and edited the sentence  \(x^{\prime \prime}  \ = G(x^\prime) \). Finally,
        the discriminator \(D\) is used to replaced the Token Detection task. The cross-entropy loss
        for each sentence \(x\) can be written as: 
        
    
    </p>
      \[\small
      
      \begin{aligned}
\mathcal{L}_{\mathrm{RTD}}^{x} &=\sum_{t=1}^{T}\left(-\mathbb{1}\left(x_{(t)}^{\prime \prime}=x_{(t)}\right) \log D\left(x^{\prime \prime}, \mathbf{h}, t\right)\right.\\
&\left.-\mathbb{1}\left(x_{(t)}^{\prime \prime} \neq x_{(t)}\right) \log \left(1-D\left(x^{\prime \prime}, \mathbf{h}, t\right)\right)\right)
\end{aligned}
      
      \]

      <p> Then, after training, the model is optimized for the two losses (i.e., SimCSE and condition ELECTRA)
           with weight coefficient \(\lambda\). 


      </p>

      

\[ \small 
      \mathcal{L}=\mathcal{L}_{\text {contrast }}+\lambda \cdot \mathcal{L}_{\text {RTD }}

\]
<p> Table 1 below shows the benefits of using data augmentation techniques for sentence
     embedding tasks such as textual semantic similarity. This work also shows how data augmentation 
     techniques can be applied for NLP tasks. Note that the original SimCSE demonstrates that different 
     straightforward augmentation techniques (e.g., crop, word deletion, synonym replacement. etc.) can break the accuracy.
</p>


<center>  

<table  align="center">
    <tr>
  
              <td> Model </td>
      <td style="text-align:center">　STS-B</td>
        <td style="text-align:center">　SICK-R</td>
  
    </tr>
  
            <tr>
             <td>　SimCSE-RoBERTa </td>
            <td style="text-align:center">　77.24</td>
           <td style="text-align:center">　71.16 </td>
      </tr>
  
      <tr>
             <td>　DiffCSE-RoBERTa </td>
            <td style="text-align:center">　<strong>82.38</strong> </td>
           <td style="text-align:center">　 <strong>71.19 </strong> </td>
      </tr>
  
  
      <caption> <font color="#6e6e6f"> <small> <small>  Table 1. Performance comparison on STS-B tasks and SICK-Relatedness tasks. <small>  <small></font> </caption>
  
    </table>

</center>  
 

                <!-- --------------------------------------------------------------------------------------------------------->


<!--
 <p>&clubs;<strong>???????</strong>
    <dt-cite key="crocelearning"></dt-cite> <a href="https://github.com"><i class="fab fa-github" style='font-size:20px'></i></a></p>


<p> ??????????   </p>

 --> 



 <hr>






            <p>&clubs;<strong> Fine-grained Image Captioning with CLIP Reward</strong>
                <dt-cite key="cho2022fine"></dt-cite> <a href="https://github.com/j-min/CLIP-Caption-Reward"><i class="fab fa-github" style='font-size:20px'></i></a></p>


            <p> Recent work uses CIDEr (i.e., text similarity) as a reward function to learn more descriptive image captions 
                <dt-cite key="rennie2017self,li2020oscar"></dt-cite>. 
                This work uses CLIP image-text similarity score as a reward for image captioning system. More specifically,  
                they use CLIP score <dt-cite key="hessel2021clipscore"></dt-cite> which is the cosine similarity score between 
                CLIP \(f^{I}(I)\) images and \(f^{T}(c)\) text features.
        

            </p>

           

            
           

            \[ \small R(I, c)=\text { CLIP }-S(I, c)=w * \max \left(\frac{f^{I}(I) f^{T}(c)}{\left|f^{I}(I)\right| \cdot\left|f^{T}(c)\right|}, 0\right) \]


            <p> where \(I\) and \(c\) refers  to image and caption,  \(f^{I}(I)\) and \(f^{T}(c)\)  are the CLIP 
                image and text encoders. The captioning model \(P_{\theta}(c \mid I)\) is optimize using  REINFORCE  
                <dt-cite key="williams1992simple"></dt-cite>  with a self-critical baseline <dt-cite key="rennie2017self"></dt-cite>:

            </p>

            \[ \small 
            \hat{c}_{\text {greedy }}: \quad \nabla_{\theta} \mathbb{E}_{\hat{c} \sim P_{\theta}(c \mid I)}[R(I, \hat{c})] \quad \approx \]

            \[  \small
            \left(R\left(I, \hat{c}_{\text {beam }}\right)-R\left(I, \hat{c}_{\text {greedy }}\right)\right) \nabla_{\theta} \log P_{\theta}\left(\hat{c}_{\text {beam }} \mid I\right) 
            \]

             <p>  where </p>

             \[ \small 
             R(I, c)=\operatorname{CLIP}-\mathrm{S}(I, c)
             \]
            <p> </p>

            <!-- Since CLIP is not trained with a language modeling objective, the captioning model trained with the CLIP-S 
                reward often generates grammatically incorrect captions (e.g., repeated words; see Table 3).-->

            <p> However, this results in grammatically incorrect captions (e.g., word repetition) 
                as CLIP is not trained on language model objective as shown in the example below in Table 1 on 
                MS COCO karpathy test split.</p>
                
                
      

        </p>
        <center>  
        <table>
            <tr>
                <td style="text-align:center">　Reward</td>
                <td style="text-align:center">　Caption</td>
            </tr>

            <tr>
                <td style="text-align:center">　CIDEr </td>
                <td style="text-align:left">　a window of an airport with planes on the runway</td>
            </tr>

            <tr>
                <td style="text-align:center">　CLIP-S</td>
                <td style="text-align:center">　several rows of planes packed outside a terminal window are with fog outside </td>
                <td></td>

            </tr>

            
           
            <caption>
                <font color="#6e6e6f"> <small> <small> Table1. Generated caption with different rewards. As CLIP is
                     not an LM, the caption is grammatically incorrect. </small> </small> </font> <small>  <small></font> </caption>

  </table>
</center>  

  <p>
     To address this, this work proposed  to inject grammatical knowledge  (grammar head) into CLIP encoder with randomly generated  negative caption as noise (e.g., inserting/swapping/shuffling, etc.).
     In particular, a two-layer perceptron will take CLIP text feature  \(f^{T}(c)\) as input and produces a 
     probability \(c\) for grammatically correct caption with 
     binary cross-entropy \(g(c) \in[0,1]\), where the grammar objective  is labeled \(y = 1\) for reference captions and \(y = 0\) 
     for the negative captions: \(−y\) log g(c):


     
  </p>
<p> \[ g(c)=\operatorname{sigmoid}\left(\operatorname{MLP}\left(f^{T}(c)\right)\right) \in[0,1] \] </p>



 <p>Next, the model is jointly finetuned with the text encoder and grammar head using both the original 
     CLIP and grammar head objectives. Finally, the captioning model is trained with the grammar 
     score and the  augmented  reward: </p>


  <p>
    \[ R(I, c)=\operatorname{CLIP}-\mathrm{S}(I, c)+\lambda g(c)  \]

</p>

<p> As shown in Table 2 below, the proposed CLIP-S+Grammar, the caption is more diverse than CIDEr as the model 
    describes rainy weather with <em> wet</em>. </p>


<center>  
<table>
    <tr>


        <td style="text-align:center">　Reward</td>
        <td style="text-align:center">　Caption</td>
    </tr>

    <tr>
        <td style="text-align:center">　CIDEr </td>
        <td style="text-align:left">a window of an airport with planes on the runway</td>
    </tr>

    <tr>
        <td style="text-align:center">　CLIP-S </td>
        <td style="text-align:left">several rows of planes packed outside a terminal window are with fog outside </td>
        <td></td>

    </tr>
    <tr>
        <td style="text-align:center">　CLIP-S+G </td>
        <td style="text-align:left">　a lot of airplanes parked on a  <font color="#c71585"> wet </font> airport terminal </td>
        <td></td>

    </tr>
 

    <caption>
        <font color="#6e6e6f"> <small> <small> Table 2. Caption with different rewards. The proposed <strong>G</strong>rammar head fixes the  grammar incorrectness.   <small>  <small></font> </caption>

</table>
</center>  



<!-- --------------------------------------------------------------------------------------------------------->
<hr>


    <p>&clubs;<strong>  Progressive Class Semantic Matching for Semi-supervised Text Classification </strong>
        <dt-cite key="xu2022progressive"></dt-cite> <a href="https://github.com/HeimingX/PCM"><i class="fab fa-github" style='font-size:20px'></i></a></p>


        <p> This work shows the benefits of inherent semi-supervised knowledge into a pre-trained model 
            like BERT<dt-cite key="devlin2018bert"></dt-cite>. BERT's original objectives are <em> Mask Language modeling (MLM) </em>and<em>Next Sentence Prediction (NSP)</em>.
             This work was inspired (author) by  NSP default inherent topic matching capability, 
             as shown in the example below with the cosine distance between the input word and class 
             name <strong> family </strong> and <strong> sport</strong>.   
        </p>

        <center>
            
           

            <p class="noteBoxes type4"  align="left">  



    
  
    <small>  ground truth class name:  <strong> family </strong>  </small> <br>

<small>  append word for this class: <strong> family </strong>   </small> <br>

<small>  which is the best way to express <font color="red">ur love   </font>to ur   <font color="red">girlfriend  </font>? be a <font color="red">gentleman  </font> and 
    spending free time with her  is <font color="red">###  </font>  1 and be good listener </small> <br>
</p>

</center>
<center>
<p class="noteBoxes type4" align="left">

    
  
    <small>  ground truth class name:  <strong> Sport </strong>  </small> <br>

<small>  append word for this class: <strong> Football </strong>   </small> <br>



    <small>  on may 27th 2006 did it have an NPA <font color="blue">basketball</font> game on tv ? yup miami 98 over detroit 83 </small> <br>
</p>
</center>


<p>  As shown in the figure below the proposed model  Progressive Class-semantic Matching (PCM). The PCM is consist of three components: 
    (1) class semantic representation \( C_{i} \), which constructs input to the BERT model by 
    concatenating sentences with class semantic-related words; (2) standard K-way classifier with two-layer MLP with an output set as logits \(o_{i}^{s}\); (3) a class-sentence matching classifier via 
    matching logits \( o_{i}^{m} \) via Sigmoid function to convert it into the probabilistic form. 
    
</p>



   <p>  The model relies on initialization stage of the classes-to-related 
    words.  Therefore, to initialize the class-related words \( C_{k} \) for PCM: (1) 
    fine-tune K-way classifier on labeled data; pass labeled text into a fine-tuned model; (2) calculate attention value for each token; (3) retain top-\(j\) attend word for each class  \( C_{k} \).</p>

  <!---  
<p align="center">
    <img src="matching.png" alt="Trulli" style="width:60%">
</p>

-->


        <figure align="center">
           
            <img src="matching.png" alt="Trulli" style="width:60%">
            <figcaption> Figure. Illustration of the proposed PCM model. The arrow with the 
                same color shows the flow of the information into the model. \(c_{K}\) denotes the set of class semantic-related words,
                 <em> Avg</em> is the average word embedding, and <em>GAP</em> is the global average pooling of the input text features. 
                 </figcaption>

          </figure>
        

   <p>As shown in the example below the authors show some examples from AG news dataset. The 
       proposed model can relate more accurate topics than fine-tuned topics BERT as 
       shown in color <font color="red"> red</font> with unrelated topics. 
</p>



<!-- 
<p>Update of the standard K-way classifier and
    the class-sentence matching classifier</p>


\[\small
    \begin{aligned}
    \mathcal{L}_{l} &=\frac{1}{n_{l}} \sum_{j=1}^{n_{l}} \sum_{i=1}^{K} \underbrace{-\mathbb{I}_{i}^{j} \log p_{i}^{s}\left(x_{j}\right)}_{\text {cross entropy (CE) }}+\\
    & \underbrace{-\mathbb{I}_{i}^{j} \log p_{i}^{m}\left(x_{j}\right)-\left(1-\mathbb{I}_{i}^{j}\right) \log \left(1-p_{i}^{m}\left(x_{j}\right)\right)}_{\text {binary cross entropy (BCE) }},
    \end{aligned}
\] 

<p>Pseudo-labelling codition </p>
\[\small
\left\{\begin{array}{l}
\max _{i}\left(p_{i}^{s}\left(x_{j}\right)\right)>=\text { confid } 1 \\
\max _{i}\left(p_{i}^{m}\left(x_{j}\right)\right)>=\operatorname{confid} 2 \\
\operatorname{argmax}_{i}\left(p_{i}^{s}\left(x_{j}\right)\right)==\operatorname{argmax}_{i}\left(p_{i}^{m}\left(x_{j}\right)\right)
\end{array}\right.


\]

<p> Tranining objective</p>
\[\small 
\begin{aligned}
\mathcal{L}_{u}=\frac{1}{n_{u}} \sum_{j=1}^{n_{u}} &\left(\operatorname{KL}\left(p^{s}\left(x_{j}^{a}\right), \hat{p}^{s}\left(x_{j}\right)\right)\right.\\
&\left.+\operatorname{BCE}\left(p^{m}\left(x_{j}^{a}\right), \hat{y}_{i}\left(x_{j}\right)\right)\right)
\end{aligned}
\]
--> 
<center>
                    
  

<p class="noteBoxes type4"  align="left">  

    <small>  word : world (from AG news)   </small> <br>
  
    <small>  init (BERT):  <strong> bush, car, killed, <font color="red"> black</font> , <font color="red"> story</font> </strong>  </small> <br>

<small>Proposed: <strong> iraq, president , iraqi, military, troops </strong></small> <br>


</p>
</center> 


 <!-- --------------------------------------------------------------------------------------------------------->

<!-- 
 <p>&clubs; <strong>Paragraph-based Transformer Pre-training for Multi-Sentence Inference</strong><dt-cite key="di2022paragraph"></dt-cite> <a href="https://github.com/amazon-research/wqa-multi-sentence-inference"><i class="fab fa-github" style='font-size:20px'></i></a></strong></p>


<p> This work proposed Multi-Sentence output head for BERT based model. </p>

<p>Transformer models are usually fine-tuned for tasks that need just 1 or 2 sentences/pieces of text at a time1, e.g., Machine Reading, Textual Similarity, etc. However, in some scenario it is beneficial to encode more than two sentences with cross-attention. </p>

<p> </p>


<p class="noteBoxes type4">

    <small>  When did Dumbledore die?  </small> <br>

    <small>  a) In the fifth film of Harry Potter. </small> <br>

<small>  b) In the Halfblood Prince. </small> <br>

<small>  c) Dumbledore never died. </small> <br>
</p>

--> 
<!-- here -->

 <hr>
<!-- here -->
<p>&clubs; <strong>Is Neural Topic Modelling Better than Clustering?</strong><dt-cite key="zhang2022neural"></dt-cite><a href="https://github.com/hyintell/topicx"><i class="fab fa-github" style='font-size:20px'></i></a></strong></p>


<p> Recent Neural-based approaches </strong><dt-cite key="zhao2021topic,grootendorst2022bertopic"></dt-cite> (via contextualized 
    embeddings) achieved a breakthrough in topic modeling. 
    However, the intuitive research question here: what about traditional clustering via contextualized embeddings for topic modeling. This work tried to answer this question 
    by using clustering via contextualized embeddings to extract topics.  The main idea is to rely on semantically 
    similar words or documents that are close to each other in the vector space. Each cluster can be seen as a topic 
    if grouped together. Also compared to current neural topic models, clustering-based models are more simple,
     and efficient and can generate commendable topics, which can be applied as an alternative.
 </p>   
    
    
    <p>The proposed method can be summarized in the following steps: 
        First, they (the authors) use pre-trained language models to obtain contextualized embeddings. 
        Second, they apply K-Means to cluster similar documents (each cluster will be regarded as a topic). 
        Third, they adopt a weighting method to select representative words as topics.
  </p>

<p>  However, traditional TFIDF ignores the semantics between  
    documents. To address this issue, two alternative strategies are  considered:  
    First, the authors concatenate the documents within a cluster to be a single long document and then calculate the term 
    frequency of each word in each cluster:</p>

\[ \small
\mathbf{T}_{\mathbf{i}}=\frac{n_{t, i}}{\sum_{t^{\prime}} n_{t^{\prime}, i}}
\]

<p> where \( n_{t,i}\) is the frequency of word \(t \) in cluster \(i\), \( \sum_{t^{\prime}} n_{t^{\prime}, i}\) is the total word frequency in the cluster \(i\). Second, for each cluster they apply:</p>


\[ \small 

\mathbf{T F I D F}_{\mathbf{i}}=\frac{n_{t, d_{i}}}{\sum_{t^{\prime}} n_{t^{\prime}, d_{i}}} \cdot \log \left(\frac{\left|D_{i}\right|}{\left|\left\{d \in D_{i}: t \in d\right\}\right|}\right)
\]

<p>where \( n_{t}, d_{i} \) refers to the frequency of word \( t \) in document  \(d \) in cluster \(i\),\(|D_{i}|\)is number of documents in cluster \(i\). </p>


<p>Besides the two local cluster-based strategies, they further incorporate the global word importance with 
    local term frequency within each cluster:</p>

\[ \small 
\mathbf{ TFIDF } \times \mathbf{T F}_{\mathbf{i}}=\mathbf{T F I D F} \cdot \mathbf{T F}_{\mathrm{i}}
\]

<p> And then combine the global word importance with
    term frequency across clusters:</p>


\[ \small
\mathbf{T F I D F} \times {\mathbf{I D F}_{\mathbf{i}}}=\mathbf{T F I D F}^{\operatorname{ID}} \log \left(\frac{|K|}{|\{t \in K\}|}\right)
\]

<p>where \(|K|\) is the number of clusters and \(|\{t \in K\}|\) is the number of clusters that word \(t\) appears.</p>


<p> The proposed approach shows that directly clustering 
    high-quality embeddings with an appropriate word selecting method can 
    generate more coherent and diverse topics than recent neural topic models. 
</p>


<!---  
<p>  <strong> Probabilistic-based Topic Modelling</strong> From traditional LDA to neural topic models (NTM). From Bag-of-Words (BoW) representations tocontextualized embeddings produced by pre-trainedlanguage models (e.g. BERT)

    Despite the improved performance, NTM suffers fromcomputational overheads and hyper-parameters tuning
</p>
<p><strong>Clustering-based Topic Modelling </strong>Idea: semantically similar words or documents are close toeach other in vector space. Each cluster can be seen as a topic if grouped together.
    To maintain high coherence and diversity, selectingrepresentative topic words is vital.
</p>


<p> <strong>Contextual Embeddings+UMAP+KM</strong> Encode pre-processed documents to obtain contextualized embeddings through pre-trained language models;
    Lower the dimension (e.g. UMAP, optional) of the embeddings before applying K-Means to cluster similar documents; Each cluster will be regarded as a topic. We adopt a weighting method to select representative words as topics. </p>

---> 


 <hr>
<p>&clubs;<strong> Imagination-Augmented Natural Language Understanding <dt-cite key="lu2022imagination"></dt-cite> 
    <a href="https://github.com/YujieLu10/IACE-NLU"><i class="fab fa-github" style='font-size:20px'></i></a></strong></p>

<p> Humans understand language using visual imagination which fused two modalities 
    in the brain to understand the language better. The neural activation, in the brain, 
    of vision-related tasks is active when reading texts. This work tried to mimic human 
    understanding via introducing a visual commonsense framework to pretrained multimodal models. 
    The main idea is to use generated synthetic visual information using the text as an additional feature to the text itself to 
    capture visual commonsense knowledge.
</p> 
   <p> The model consists of three blocks  as shown in Figure 1: <strong> (1) Image Generator:</strong> the GAN model is guided by CLIP model to generate relevant 
       images <dt-cite key="crowson2022vqgan"></dt-cite>,  <strong> (2) Cross-Model Encoder: </strong> the model  takes the generated image and 
       text and fused them together. <strong> (3) Visually Supervised Transformer: </strong>the model employs BERT Mask Language Model to learn token related images (i.e., visual grounding). </p>

         
         


    <figure align="center">
        <img src="imagination.png" alt="Trulli" style="width:90%">
        <figcaption> Figure 1. Illustration of the proposed Imagination-Augmented Natural Language Understanding model. 
            GAN is used  to generate the image and the cross model encoder learns the imagination-augmented language representation. The learning
            process is divided into two setups: (1) pre-trained textual transformer and (2) fine-tune the visual-augmented cross-model encoder on
            downstream tasks such as GLUE benchmark dataset. 


            
        </figcaption>
      </figure>
 
      <!--

   
<p>First, the model need to minimize the loss function so it can generate relaviend imagination which is
    very close to the text representation in the same embedding space: </p>
   -->

  
    <p><strong> Image Generator.</strong>  Each piece of input text \(x\)
        is treated as a prompt and VQGAN is used 
        <dt-cite key="esser2021taming"></dt-cite>
         to render the imagination. At each optimization step, CLIP model is used to assess how well the generated
    image corresponds to the text: </p>   


    \[ \small 
    {L}_{G A N}=2\left[\arcsin \left(\frac{1}{2}\|\boldsymbol{t}-\boldsymbol{v}\|\right)\right]^{2}
    \]
 

     <p>The CLIP encodes the input text \(x\)  and
    the corresponding imagination \(i\) as \(t\) and \(v\), and
    the training objective is to minimize the distance
    between \(t\) and \(v\) in the cross-modal embedding
    space. </p>




   
<p><strong>Cross-Modal Encoder.</strong>   The Cross-Modal Encoder takes the image and text and fuses them together. 
    The output will be representations of the imagination augmented in a bi-directional way. In particular, the Cross-Model 
    Encoder uses a vision transformer <dt-cite key="dosovitskiy2020image"></dt-cite>  with a late fusion layer. For each  modality, the self-attention (SA) is applied 
    to the text or the image as: 
</p>

    \[ \small
    S A(F)=\operatorname{concat}\left(\operatorname{softmax} \frac{F W_{j}^{Q} F W_{j}^{K^{\mathrm{T}}}}{\sqrt{d_{k}}} F W_{j}^{V}, \ldots\right) W
    \] 


    <p> where \(F\) denotes the set of regions of the imagination or the words of the textual sentence. \(W_{j}^{Q}\),  \(W_{j}^{K}\) and  \(W_{j}^{V}\), represents 
        the weight in the \(j\)-th head for query, key, and value respectively. \(d_{k}\) is the dimension of the embedding. 
        \(W\) is the weight matrix for multiple heads.  </p>
    
       
      <p>  Then, they (the authors) apply late fusion on the text feature \(t\) and visual feature \(v\) to 
          construct the cross-modal feature. Given the set of visual features \(S_{v}\) and textual features \(S_{t}\), 
        the fused embedding  \(X_{S}\) can be given:</p>


\[\small
X_{S}=\left[\operatorname{ReLU}\left(W_{t} S_{t}+b_{t}\right), \operatorname{ReLU}\left(W_{j} S_{v}+b_{j}\right)\right]
\]



<p> <strong> Visually-Supervised Transformer.</strong> VOKEN <dt-cite key="tan2020vokenization"></dt-cite> is used 
to learn token related images. The  VOKEN is trained on two objectives:
(1) the original BERT pretrained with Mask Language Model while (2) the second objective 
is to predict  the token associated with  an  external visual context. Note that, the two objectives share the 
same model weights since the input are the same.
   

<p> <strong>Learning Procedure.</strong> The learning procedure is divided into two steps: (1) pre-trained 
    the visually supervised Transformer, and (2) fine-tune the proposed framework  with imagination on downstream tasks. </p>
    
    <p><strong>Step 1: Visually-Supervised Transformer.</strong>  As we mentioned above, VOKEN 
        proposed a voken classification task. Given a set of tokens with masks, 
        the model is asked to predict  the best-matching image (the voken) for each token. 
        The pre-training loss can be given as:</strong> 

    

\[\small
{L}=-\lambda_{1} \sum_{w_{j} \in \hat{s}} \log q_{j}\left(w_{j} \mid \check{s}\right)-\lambda_{2} \sum_{w_{j} \in \hat{s}} \log p_{j}\left(v\left(w_{j} ; s\right) \mid \check{s}\right)

\]

<p>here \(s\) is the token set, \(\check{s}\) is the mask tokens. and \(\hat{s}\) is the unmasked tokens. 
    The \(q_{j}\) and \(p_{j}\) refer to the conditional probability of the \(j\)-th  token 
    given token \(w_{j}\) and voken \(v(w_{j} ; s) \) respectively. The \(\lambda\)s  are the balance factor 
    of the Mask Language Model and voken-classification task. 
    The Voken enables the matching between the general language token and its related image from COCO dataset. </p>

 
<p><strong>Step 2: Imagination-augmented Fine-tuning:</strong> The fine-tuning is performed on downstream tasks like GLUE with cross-entropy loss: </p>

\[\small
{L}_{\text {Imagine }}=-\sum_{j=1}^{|D|} \sum_{k=1}^{K} y_{k} \log p_{k}\left(d_{j}(\boldsymbol{t} ; \boldsymbol{v}) \mid D\right)
\]

<p> where \(j\) refer to the \(j\)-th data sample in the dataset \(D\), \(K\) is the class number, \(p_{k}\) represents the 
    conditional probability of \(d_{j}\). The model relies only on the language model during the fine-tuning: 
</p>

\[\small
{L}_{\text {Lang }}=-\sum_{j=1}^{|D|} \sum_{k=1}^{K} y_{k} \log p_{k}\left(d_{j}(\boldsymbol{t}) \mid D\right)
\] 

<p> Finally, the imagination-augmented loss and language model loss is summed up with balanced factor \( \lambda \)s during the training: </p>
\[
{L}=\lambda {L}_{\text {Imagine }}+(1-\lambda){L}_{\text {Lang }}
\] 


<p>  Figure 2 below shows that adding visual information imagination-augmented helps give the model a context and thus the predictions are 
    more aligned with human score. </p>


<figure align="center">
    <img src="sts-b+images.png" alt="Trulli" style="width:70%">
    <figcaption>   Figure 2. Example of the proposed model on STS-B task with images and without images. The 
        imagination-augmented models give a prediction that is more aligned with the ground truth.
    

       
    </figcaption>
  </figure>


  <hr>
   
    <p>&clubs;<strong>  SURF: Semantic-level Unsupervised Reward Function for Machine Translation <dt-cite key="anuchitanukul2022surf"></dt-cite> 
        <a href="https://github.com/AtomAnu/SURF"><i class="fab fa-github" style='font-size:20px'></i></a></strong></p>
<p> Machine Translation suffers from objective mismatch during the training and the inference. 
    During the training, the teacher forces the generation to be conditioned on the target sequence 
    using cross-entropy with Maximum Likelihood Estimation objective (MLE). However, during the inference, 
    the generation is conditioned on the previous outputs. In addition, unlike during the training, 
    the model loss is evaluated using MLE Loss, but at inference time the model is evaluated using 
    standard metrics like BLEU <dt-cite key="papineni2002bleu"></dt-cite>.</p>
   
<p> This work applied Reinforcement Learning agents (RL) to bridge the gap between training and inference. In particular, the agent 
    as a state takes the previous sequence  \(\hat{y}_{1:t-1} \) and takes an action on the current token \(\hat{y}_{t} \), then
    after generating the token the agent gives a reward from the environment which is commonly defined (e.g., BLEU). Thus,
    the objective of this work is to maximize the expected reward by the evaluation metric like BLEU.</p>
    

<p>    One of the problems of using an RL for this task is the capability of 
    generalization of the agent to explore hypotheses in the hypothesis space. 
    This work proposed an unsupervised reward function with normalized measures 
    that assessed both sentences in fluency and semantic diversity and ensure 
    the reward is uniform and can be generalized to out-of-the-domain data. 
    The two rewards functions are the  incremental difference between  the two pay-off functions (reward shaping) are:</p>


<p> <strong> Sentence Fluency (F). </strong> The F reward is defined as the average Log-Likelihood of
    the generated sequence. Each term is the probability of a token given the previous one (generated tokens). The score 
    is computed using a pre-trained Language Model.</p>

\[\small     
\frac{1}{\tau} \log \prod_{t}^{\tau} p\left(\hat{y}_{t} \mid \hat{y}_{1: t-1}\right)
\] 
    
<p>  where  \(\hat{y}_{1:t}\) is the generated sequence at timestep \(t\). The F function determines the effectiveness of the generated token a time step in respect of the current/previous sentence as a reward function. </p>



<p> <strong> Sentence-level Semantic Similarity (SLSS).</strong> 
    The SLSS is the cosine similarity between Cross-lingual Embedding between the source and the target language (generated sequence)<dt-cite key="song2021sentsim"></dt-cite>. The embedding is computed using a pre-trained cross-lingual language model to project both source and target embedding in a shared semantic space, and then compute the cosine similarity score. 
</p>
                   

<!-----
\[\small
\frac{\mathbf{X}^{x e} \cdot \hat{y}_{1: \tau}^{x e}}{\left\|\mathbf{X}^{x e}\right\|_{2} \times\left\|\hat{y}_{1: \tau}^{x e}\right\|_{2}}
\]

---> 


<p> Finally, to ensure the uniformity across all source sequences the score is normalized with respect to the target sequence. </p>

<hr>

                    

                        <h2>Workshop </h2>


                        <p> &rarr;<strong> Using Natural Sentence Prompts for Understanding Biases in Language Models</strong> 
                            <dt-cite key="madaan2022memory"></dt-cite><a href="https://github.com/aliciasun/natural-prompts">
                                <i class="fab fa-github" style='font-size:20px'></i></a></p>

                        <p>  Most previous work,  uses simple template to mimic the real sentences (e.g., <em>The {man/woman} laughed because .. </em>) for gender bias evaluation. However, these template is too simple and can trigger gender-specific continuations. This work proposed a real synthetic dataset for gender bias evaluation using prompts. This dataset is used to study biases between profession definitions and gender in language models as shown below. </p>
                    

                        <center>
                    
                        <p class="noteBoxes type3" align="left">
                           
                            <TT> <small>  <font color="gray"> A  <font color="#6495ed"><strong> silversmith </strong> </font>is  a <font color="#c71585"> metalworker  </font> 
                                 who  crafts  objects from silver  </small>  </TT>
                            <font color="#6e6e6f"> <small>  <small> </font>     <-  Original sentence  </small> </small>
                            </font>
                            <br>
                            <TT> <small>  <font color="gray"> A  <font color="#6495ed"><strong> person </strong> </font> is  a <font color="#c71585"> metalworker  </font> 
                                who  crafts  objects from silver  </small>  
                           <font color="#6e6e6f"> <font color="blue"> where ..  </font>  <small>  <small> </font> </TT>  <-  Final prompt  </small> </small>
                           </font>
                        <br>
                        <TT> <small>  <font color="gray"> A  <font color="#6495ed"><strong> dermatologist </strong> </font>is  a <font color="#c71585"> specialist doctor   </font> 
                            who manages diseases  related  to  skin,  hair  and  nails   <strike> and  some  cosmetic problems.</strike> </small>  </TT>
                       <font color="#6e6e6f"> <small>  <small> </font>     <-  Original sentence  </small> </small>
                       </font>
                       <br>
                       <TT> <small>  <font color="gray"> A  <font color="#6495ed"><strong>dermatologist </strong> </font> is  a <font color="#c71585"> person  </font> 
                        who  manages  diseases related to skin, hair and nails </small>  
                      <font color="#6e6e6f"> <font color="blue"> where ..  </font>  <small>  <small> </font> </TT>  <-  Final prompt  </small> </small>
                      </font>
                                            
                </center>
                        
                        </p>
                      
                        <p> The findings of this paper are: (1) the gender bias evaluations are sensitive to the template prompts, 
                            and (2) the default behavior of language models is already biased.
                        </p>
                    
                                <hr>



                                <p> &rarr; <strong> How do people talk about images? A study on open-domain conversations with images </strong>
                                    <dt-cite key="chen2022people"></dt-cite></p>

                                    <p> This work analyzed conversations with images in  open domain scenario (ImageChate dataset) <dt-cite key="shuster2018image"></dt-cite>. The question is tried to answer 
                                        Is the conversation topic always related to the image ? if not, how does the conversation topic evolve? and how beneficial the image to the conversation.
                                        
                                        The results show (1) by utterance: that only 69% of the conversation is related to images, and (2) by conversation only 39% of the object is mentioned.  The test images have 45% image objects information, 24% of the conversation contains non-object information such as the description of events in the image, and 31% no information about the image at all. The Figure shows examples of some of the conversation mentioned in the study.
                                        
                                             </p>
                                   
                                             <center> 
                                        <table  align="center">
                                            <tr>
                                                <th colspan="3"></th> 
                                            </tr>
                                            <tr>
                                               
                                                <th align="left">Related</th>
                                                <th>Utterance</th>
                                                <th>Image</th>
                                            </tr>
                                            <tr>
                                                <td>Yes</th>
                                                <td>Never had this food before and not sure if I’m ready to try it today.</th>
                                                        <!-- considering it is on the same folder that .html file -->
                                                <td><img src="image-1.jpeg" alt="" border=3 height=100 width=100></img></th>
                                            </tr>
                                            <tr>
                                                <td>No</th>
                                                <td> That’s it, I going to Vegas tomorrow. Who’s coming with me?s</th>
                                                    <td><img src="image-2.jpeg" alt="" border=3 height=100 width=100></img></th>
                                            </tr>
                                        
                                            </tr>
                                        </table>
                                    </center>

                                          <p>  This study shows that the noise that comes from images-caption extracted from the web without a strict filter. Most recent model such as CLIP is trained on this kinda data, and thus these pairs (no relation between image and the associated text) introduce noise to the model.</p>

                                      


                                     
 
                                   

                                <div class="d-appendix">
                                </div>

                                <dt-appendix>
                                </dt-appendix>

                              


                                <script type="text/bibliography">

                                    @article{ling2022vision, title={Vision-Language Pre-Training for Multimodal Aspect-Based Sentiment Analysis}, author={Ling, Yan and Xia, Rui and others}, journal={arXiv preprint arXiv:2204.07955}, year={2022} } @article{zaken2021bitfit, title={Bitfit:
                                    Simple parameter-efficient fine-tuning for transformer-based masked language-models}, author={Zaken, Elad Ben and Ravfogel, Shauli and Goldberg, Yoav}, journal={arXiv preprint arXiv:2106.10199}, url={https://arxiv.org/pdf/2106.10199.pdf},
                                    year={2021} } @article{sennrich2015improving, title={Improving neural machine translation models with monolingual data}, author={Sennrich, Rico and Haddow, Barry and Birch, Alexandra}, journal={arXiv preprint arXiv:1511.06709},
                                    url={https://arxiv.org/pdf/1511.06709.pdf}, year={2015} } @article{lu2021fantastically, title={Fantastically ordered prompts and where to find them: Overcoming few-shot prompt order sensitivity}, author={Lu, Yao and
                                    Bartolo, Max and Moore, Alastair and Riedel, Sebastian and Stenetorp, Pontus}, journal={arXiv preprint arXiv:2104.08786}, url={https://arxiv.org/pdf/2104.08786.pdf}, year={2021} } @article{madaan2022memory, title={Memory-assisted
                                    prompt editing to improve GPT-3 after deployment}, author={Madaan, Aman and Tandon, Niket and Clark, Peter and Yang, Yiming}, journal={arXiv preprint arXiv:2201.06009}, url={https://arxiv.org/pdf/2201.06009.pdf}, year={2022}
                                    } @article{schick2021self, title={Self-diagnosis and self-debiasing: A proposal for reducing corpus-based bias in nlp}, author={Schick, Timo and Udupa, Sahana and Sch{\"u}tze, Hinrich}, journal={Transactions of the
                                    Association for Computational Linguistics}, volume={9}, pages={1408--1424}, year={2021}, url={https://arxiv.org/pdf/2103.00453.pdf}, publisher={MIT Press} } @article{hsu2022xdbert, title={XDBERT: Distilling Visual Information
                                    to BERT from Cross-Modal Systems to Improve Language Understanding}, author={Hsu, Chan-Jan and Lee, Hung-yi and Tsao, Yu}, journal={arXiv preprint arXiv:2204.07316}, url={https://arxiv.org/pdf/2204.07316.pdf}, year={2022}
                                    } @inproceedings{peng2022predicate, title={Predicate-Argument Based Bi-Encoder for Paraphrase Identification}, author={Peng, Qiwei and Weir, David and Weeds, Julie and Chai, Yekun}, booktitle={Proceedings of the 60th
                                    Annual Meeting of the Association for Computational Linguistics}, pages={5579--5589}, url={https://aclanthology.org/2022.acl-long.382.pdf}, year={2022} } @inproceedings{okimura-etal-2022-impact, title = "On the Impact
                                    of Data Augmentation on Downstream Performance in Natural Language Processing", author = "Okimura, Itsuki and Reid, Machel and Kawano, Makoto and Matsuo, Yutaka", url={https://aclanthology.org/2022.insights-1.12.pdf},
                                    year={2022} } @article{chen2022imputing, title={Imputing out-of-vocabulary embeddings with LOVE makes language models robust with little cost}, author={Chen, Lihu and Varoquaux, Ga{\"e}l and Suchanek, Fabian M}, journal={arXiv
                                    preprint arXiv:2203.07860}, url={https://arxiv.org/pdf/2203.07860.pdf}, year={2022} } @article{sun2020self, title={Self-explaining structures improve nlp models}, author={Sun, Zijun and Fan, Chun and Han, Qinghong and
                                    Sun, Xiaofei and Meng, Yuxian and Wu, Fei and Li, Jiwei}, journal={arXiv preprint arXiv:2012.01786}, url={https://arxiv.org/pdf/2012.01786.pdf}, year={2020} } @article{wolfe2022contrastive, title={Contrastive Visual
                                    Semantic Pretraining Magnifies the Semantics of Natural Language Representations}, author={Wolfe, Robert and Caliskan, Aylin}, journal={arXiv preprint arXiv:2203.07511}, url={https://arxiv.org/pdf/2203.07511.pdf}, year={2022}
                                    } @article{henderson2017efficient, title={Efficient natural language response suggestion for smart reply}, author={Henderson, Matthew and Al-Rfou, Rami and Strope, Brian and Sung, Yun-Hsuan and Luk{\'a}cs, L{\'a}szl{\'o}
                                    and Guo, Ruiqi and Kumar, Sanjiv and Miklos, Balint and Kurzweil, Ray}, journal={arXiv preprint arXiv:1705.00652}, url={https://arxiv.org/pdf/1705.00652.pdf}, year={2017} } @article{geigle2021retrieve, title={Retrieve
                                    fast, rerank smart: Cooperative and joint approaches for improved cross-modal retrieval}, author={Geigle, Gregor and Pfeiffer, Jonas and Reimers, Nils and Vuli{\'c}, Ivan and Gurevych, Iryna}, url={https://arxiv.org/pdf/2103.11920.pdf},
                                    year={2021} } @article{gao2021unsupervised, title={Unsupervised corpus aware language model pre-training for dense passage retrieval}, author={Gao, Luyu and Callan, Jamie}, journal={arXiv preprint arXiv:2108.05540},
                                    url={https://arxiv.org/pdf/2108.05540.pdf}, year={2021} } @article{tan2022sentence, title={A Sentence is Worth 128 Pseudo Tokens: A Semantic-Aware Contrastive Learning Framework for Sentence Embeddings}, author={Tan,
                                    Haochen and Shao, Wei and Wu, Han and Yang, Ke and Song, Linqi}, journal={arXiv preprint arXiv:2203.05877}, url={https://arxiv.org/pdf/2203.05877.pdf}, Github ={https://github.com/Namco0816/PT-BERT}, ACL = {ACL 2022
                                    (Findings)}, year={2022} } @article{muller2022few, title={Few-shot learning with siamese networks and label tuning}, author={M{\"u}ller, Thomas and P{\'e}rez-Torr{\'o}, Guillermo and Franco-Salvador, Marc}, journal={arXiv
                                    preprint arXiv:2203.14655}, url={https://arxiv.org/pdf/2203.14655.pdf}, year={2022} } @article{jin2021good, title={A Good Prompt Is Worth Millions of Parameters? Low-resource Prompt-based Learning for Vision-Language
                                    Models}, author={Jin, Woojeong and Cheng, Yu and Shen, Yelong and Chen, Weizhu and Ren, Xiang}, journal={arXiv preprint arXiv:2110.08484}, url={https://arxiv.org/pdf/2110.08484.pdf}, year={2021} } @article{ethayarajh2019contextual,
                                    title={How contextual are contextualized word representations? comparing the geometry of BERT, ELMo, and GPT-2 embeddings}, author={Ethayarajh, Kawin}, journal={arXiv preprint arXiv:1909.00512}, year={2019} } @article{brown2020language,
                                    title={Language models are few-shot learners}, author={Brown, Tom and Mann, Benjamin and Ryder, Nick and Subbiah, Melanie and Kaplan, Jared D and Dhariwal, Prafulla and Neelakantan, Arvind and Shyam, Pranav and Sastry,
                                    Girish and Askell, Amanda and others}, journal={Advances in neural information processing systems}, volume={33}, pages={1877--1901}, url={https://proceedings.neurips.cc/paper/2020/file/1457c0d6bfcb4967418bfb8ac142f64a-Paper.pdf},
                                    year={2020} } @article{evalrank_2022, title={Just Rank: Rethinking Evaluation with Word and Sentence Similarities}, author={Wang, Bin and Kuo, C.-C. Jay and Li, Haizhou}, journal={arXiv preprint arXiv:2203.02679}, url={https://arxiv.org/pdf/2203.02679.pdf},
                                    year={2022} } @article{cer2017semeval, title={Semeval-2017 task 1: Semantic textual similarity-multilingual and cross-lingual focused evaluation}, author={Cer, Daniel and Diab, Mona and Agirre, Eneko and Lopez-Gazpio,
                                    Inigo and Specia, Lucia}, journal={arXiv preprint arXiv:1708.00055}, year={2017} } @article{raffel2019exploring, title={Exploring the limits of transfer learning with a unified text-to-text transformer}, author={Raffel,
                                    Colin and Shazeer, Noam and Roberts, Adam and Lee, Katherine and Narang, Sharan and Matena, Michael and Zhou, Yanqi and Li, Wei and Liu, Peter J}, journal={arXiv preprint arXiv:1910.10683}, year={2019} } @article{ni2021sentence,
                                    title={Sentence-t5: Scalable sentence encoders from pre-trained text-to-text models}, author={Ni, Jianmo and {\'A}brego, Gustavo Hern{\'a}ndez and Constant, Noah and Ma, Ji and Hall, Keith B and Cer, Daniel and Yang,
                                    Yinfei}, journal={arXiv preprint arXiv:2108.08877}, url={https://arxiv.org/pdf/2108.08877.pdf}, year={2021} } @article{wu2022noisytune, title={NoisyTune: A Little Noise Can Help You Finetune Pretrained Language Models
                                    Better}, author={Wu, Chuhan and Wu, Fangzhao and Qi, Tao and Huang, Yongfeng and Xie, Xing}, journal={arXiv preprint arXiv:2202.12024}, url={https://arxiv.org/pdf/2202.12024.pdf}, year={2022} } @article{bensemanneye,
                                    title={Eye Gaze and Self-attention: How Humans and Transformers Attend Words in Sentences}, author={Bensemann, Joshua and Peng, Alex Yuxuan and Benavides-Prado, Diana and Chen, Yang and Tan, Neset {\"O}zkan and Corballis,
                                    Paul Michael and Riddle, Patricia and Witbrock, Michael}, url={https://aclanthology.org/2022.cmcl-1.9.pdf}, } @inproceedings{mnih2007three, title={Three new graphical models for statistical language modelling}, author={Mnih,
                                    Andriy and Hinton, Geoffrey}, booktitle={Proceedings of the 24th international conference on Machine learning}, pages={641--648}, year={2007} } @inproceedings{futrell2022estimating, title={Estimating word co-occurrence
                                    probabilities from pretrained static embeddings using a log-bilinear model}, author={Futrell, Richard}, booktitle={Proceedings of the Workshop on Cognitive Modeling and Computational Linguistics}, pages={54--60}, url={https://openreview.net/pdf?id=HOu4PQtgUWq},
                                    year={2022} } @article{gregor2015draw, title={DRAW: A recurrent neural network for image generation}, author={Gregor, Karol and Danihelka, Ivo and Graves, Alex and Rezende, Danilo Jimenez and Wierstra, Daan}, journal={arXivreprint
                                    arXiv:1502.04623}, year={2015}, url={https://arxiv.org/pdf/1502.04623.pdf}, } @book{Aho:72, author = {Alfred V. Aho and Jeffrey D. Ullman}, title = {The Theory of Parsing, Translation and Compiling}, year = "1972",
                                    volume = "1", publisher = {Prentice-Hall}, address = {Englewood Cliffs, NJ} } @book{APA:83, author = {{American Psychological Association}}, title = {Publications Manual}, year = "1983", publisher = {American Psychological
                                    Association}, address = {Washington, DC} } @article{ACM:83, author = {Association for Computing Machinery}, year = "1983", journal = {Computing Reviews}, volume = "24", number = "11", pages = "503--512" } @article{Chandra:81,
                                    author = {Ashok K. Chandra and Dexter C. Kozen and Larry J. Stockmeyer}, year = "1981", title = {Alternation}, journal = {Journal of the Association for Computing Machinery}, volume = "28", number = "1", pages = "114--133",
                                    doi = "10.1145/322234.322243", } @inproceedings{andrew2007scalable, title={Scalable training of {L1}-regularized log-linear models}, author={Andrew, Galen and Gao, Jianfeng}, booktitle={Proceedings of the 24th International
                                    Conference on Machine Learning}, pages={33--40}, year={2007}, } @book{Gusfield:97, author = {Dan Gusfield}, title = {Algorithms on Strings, Trees and Sequences}, year = "1997", publisher = {Cambridge University Press},
                                    address = {Cambridge, UK} } @inproceedings{borsch2011, Address = {Canberra, Australia}, Author = {Benjamin Borschinger and Mark Johnson}, Booktitle = {Proceedings of the Australasian Language Technology Association
                                    Workshop 2011}, Month = {December}, Pages = {10--18}, Title = {A Particle Filter algorithm for {B}ayesian Wordsegmentation}, Year = {2011}} @article{rasooli-tetrault-2015, author = {Mohammad Sadegh Rasooli and Joel
                                    R. Tetreault}, title = {Yara Parser: {A} Fast and Accurate Dependency Parser}, journal = {Computing Research Repository}, volume = {arXiv:1503.06733}, year = {2015}, url = {http://arxiv.org/abs/1503.06733}, note = {version
                                    2} } @article{Ando2005, Acmid = {1194905}, Author = {Ando, Rie Kubota and Zhang, Tong}, Issn = {1532-4435}, Issue_Date = {12/1/2005}, Journal = {Journal of Machine Learning Research}, Month = dec, Numpages = {37}, Pages
                                    = {1817--1853}, Publisher = {JMLR.org}, Title = {A Framework for Learning Predictive Structures from Multiple Tasks and Unlabeled Data}, Volume = {6}, Year = {2005}} @InProceedings{P16-1001, author = "Goodman, James
                                    and Vlachos, Andreas and Naradowsky, Jason", title = "Noise reduction and targeted exploration in imitation learning for Abstract Meaning Representation parsing ", booktitle = "Proceedings of the 54th Annual Meeting
                                    of the Association for Computational Linguistics (Volume 1: Long Papers) ", year = "2016", publisher = "Association for Computational Linguistics", pages = "1--11", location = "Berlin, Germany", doi = "10.18653/v1/P16-1001",
                                    url = "http://aclweb.org/anthology/P16-1001" } @InProceedings{C14-1001, author = "Harper, Mary", title = "Learning from 26 Languages: Program Management and Science in the Babel Program", booktitle = "Proceedings of
                                    COLING 2014, the 25th International Conference on Computational Linguistics: Technical Papers", year = "2014", publisher = "Dublin City University and Association for Computational Linguistics", pages = "1", location
                                    = "Dublin, Ireland", url = "http://aclweb.org/anthology/C14-1001" } @inproceedings{Christian:15, title = {Going deeper with convolutions}, author = {Christian Szegedy}, year = {2015}, month = {06}, pages = {}, booktitle
                                    = {The IEEE Conference on Computer Vision and Pattern Recognition (CVPR)} } %%%author = {Christian Szegedy and Wei Liu and Yangqing Jia and Pierre Sermanet and Scott Reed and Dragomir Anguelov and Dumitru Erhan and
                                    Vincent Vanhoucke and Andrew Rabinovich} @inproceedings{Christian:15, title={Going deeper with convolutions}, author={Szegedy, Christian and Liu, Wei and Jia, Yangqing and Sermanet, Pierre and Reed, Scott and Anguelov,
                                    Dragomir and Erhan, Dumitru and Vanhoucke, Vincent and Rabinovich, Andrew and others}, year={2015}, organization={Cvpr} } @inproceedings{Chris:15, title={Going deeper with convolutions}, author={Szegedy, Christian and
                                    Liu, Wei and Jia, Yangqing and Sermanet, Pierre and Reed, Scott and Anguelov, Dragomir and Erhan, Dumitru and Vanhoucke, Vincent and Rabinovich, Andrew and others}, year={2015}, organization={Cvpr} } @inproceedings{Szegedy:15,
                                    title={\& Rabinovich, A.(2015). Going deeper with convolutions}, author={Szegedy, C and Liu, W and Jia, Y and Sermanet, P and Reed, S and Anguelov, D}, booktitle={Proceedings of the IEEE conference on computer vision
                                    and pattern recognition}, year={2015}, pages={} } @inproceedings{Tsung-Yi:14, title={Microsoft coco: Common objects in context}, author={Lin, Tsung-Yi and Maire, Michael and Belongie, Serge and Hays, James and Perona,
                                    Pietro and Ramanan, Deva and Doll{\'a}r, Piotr and Zitnick, C Lawrence}, booktitle={European conference on computer vision}, pages={}, year={2014}, organization={Springer} } @inproceedings{Kai:11, title={End-to-end
                                    scene text recognition}, author={Wang, Kai and Babenko, Boris and Belongie, Serge}, booktitle={Computer Vision (ICCV), 2011 IEEE International Conference on}, pages={}, year={2011}, organization={IEEE} } @inproceedings{Jeffrey:14,
                                    title={Glove: Global vectors for word representation}, author={Pennington, Jeffrey and Socher, Richard and Manning, Christopher}, booktitle={Proceedings of the 2014 conference on empirical methods in natural language
                                    processing (EMNLP)}, pages={}, year={2014} } @inproceedings{Bolei:14, title={Learning deep features for scene recognition using places database}, author={Zhou, Bolei and Lapedriza, Agata and Xiao, Jianxiong and Torralba,
                                    Antonio and Oliva, Aude}, booktitle={Advances in neural information processing systems}, pages={}, year={2014} } @article{Andreas:16, title={Coco-text: Dataset and benchmark for text detection and recognition in natural
                                    images}, author={Veit, Andreas and Matera, Tomas and Neumann, Lukas and Matas, Jiri and Belongie, Serge}, journal={arXiv preprint arXiv:1601.07140}, year={2016} } @inproceedings{Tomas:13, title={Distributed representations
                                    of words and phrases and their compositionality}, author={Mikolov, Tomas and Sutskever, Ilya and Chen, Kai and Corrado, Greg S and Dean, Jeff}, booktitle={Advances in neural information processing systems}, pages={},
                                    year={2013} } @inproceedings{Sergey:03, title={Probability from similarity}, author={Blok, Sergey and Medin, Douglas and Osherson, Daniel}, booktitle={AAAI Spring Symposium on Logical Formalization of Commonsense Reasoning},
                                    pages={}, year={2003} } @InProceedings{Pierre:2016, author = {Pierre Lison and Jörg Tiedemann}, title = {OpenSubtitles2016: Extracting Large Parallel Corpora from Movie and TV Subtitles}, booktitle = {Proceedings of
                                    the Tenth International Conference on Language Resources and Evaluation (LREC 2016)}, year = {2016}, month = {may}, date = {23-28}, location = {Portorož, Slovenia}, editor = {Nicoletta Calzolari (Conference Chair) and
                                    Khalid Choukri and Thierry Declerck and Sara Goggi and Marko Grobelnik and Bente Maegaard and Joseph Mariani and Helene Mazo and Asuncion Moreno and Jan Odijk and Stelios Piperidis}, publisher = {European Language Resources
                                    Association (ELRA)}, address = {Paris, France}, isbn = {978-2-9517408-9-1}, language = {english} } @inproceedings{Kaiming:16, title={Deep residual learning for image recognition}, author={He, Kaiming and Zhang, Xiangyu
                                    and Ren, Shaoqing and Sun, Jian}, booktitle={Proceedings of the IEEE conference on computer vision and pattern recognition}, pages={}, year={2016} } @article{David:03, title={Latent dirichlet allocation}, author={Blei,
                                    David M and Ng, Andrew Y and Jordan, Michael I}, journal={Journal of machine Learning research}, volume={3}, number={Jan}, pages={}, year={2003} } @article{strubell2019energy, title={Energy and Policy Considerations
                                    for Deep Learning in NLP}, author={Strubell, Emma and Ganesh, Ananya and McCallum, Andrew}, journal={arXiv preprint arXiv:1906.02243}, year={2019} } @inproceedings{Yash:16, title={Dynamic Lexicon Generation for Natural
                                    Scene Images}, author={Patel, Yash and Gomez, Lluis and Rusinol, Mar{\c{c}}al and Karatzas, Dimosthenis}, booktitle={European Conference on Computer Vision}, pages={}, year={2016}, organization={Springer} } @article{Yunze:17,
                                    title={Reading Scene Text with Attention Convolutional Sequence Modeling}, author={Gao, Yunze and Chen, Yingying and Wang, Jinqiao and Lu, Hanqing}, journal={arXiv preprint arXiv:1709.04303}, year={2017} } @inproceedings{Alex:06,
                                    title={Connectionist temporal classification: labelling unsegmented sequence data with recurrent neural networks}, author={Graves, Alex and Fern{\'a}ndez, Santiago and Gomez, Faustino and Schmidhuber, J{\"u}rgen}, booktitle={Proceedings
                                    of the 23rd international conference on Machine learning}, pages={369--376}, year={2006}, organization={ACM} } @inproceedings{Anand:12, title={Top-down and bottom-up cues for scene text recognition}, author={Mishra,
                                    Anand and Alahari, Karteek and Jawahar, CV}, booktitle={Computer Vision and Pattern Recognition (CVPR), 2012 IEEE Conference on}, pages={}, year={2012}, organization={IEEE} } @article{Baoguang:16, title={An end-to-end
                                    trainable neural network for image-based sequence recognition and its application to scene text recognition}, author={Shi, Baoguang and Bai, Xiang and Yao, Cong}, journal={IEEE transactions on pattern analysis and machine
                                    intelligence}, year={2016}, publisher={IEEE} } @inproceedings{Alessandro:13, title={Photoocr: Reading text in uncontrolled conditions}, author={Bissacco, Alessandro and Cummins, Mark and Netzer, Yuval and Neven, Hartmut},
                                    booktitle={Proceedings of the IEEE International Conference on Computer Vision}, pages={}, year={2013} } @inproceedings{Lukas:10, title={A method for text localization and recognition in real-world images}, author={Neumann,
                                    Lukas and Matas, Jiri}, booktitle={Asian Conference on Computer Vision}, pages={}, year={2010}, organization={Springer} } @article{Lukas:11, title={A method for text localization and recognition in real-world images},
                                    author={Neumann, Lukas and Matas, Jiri}, journal={Computer Vision--ACCV 2010}, pages={770--783}, year={2011}, publisher={Springer} } @article{Suman:17, title={Visual attention models for scene text recognition}, author={Ghosh,
                                    Suman K and Valveny, Ernest and Bagdanov, Andrew D}, journal={arXiv preprint arXiv:1706.01487}, year={2017} } @inproceedings{Max:14, title={Deep Features for Text Spotting.}, author={Jaderberg, Max and Vedaldi, Andrea
                                    and Zisserman, Andrew}, booktitle={ECCV (4)}, pages={}, year={2014} } @article{Max:16, title={Reading text in the wild with convolutional neural networks}, author={Jaderberg, Max and Simonyan, Karen and Vedaldi, Andrea
                                    and Zisserman, Andrew}, journal={International Journal of Computer Vision}, volume={116}, number={1}, pages={}, year={2016}, publisher={Springer} } @article{Max:14b, title={Synthetic data and artificial neural networks
                                    for natural scene text recognition}, author={Jaderberg, Max and Simonyan, Karen and Vedaldi, Andrea and Zisserman, Andrew}, journal={arXiv preprint arXiv:1406.2227}, year={2014} } @inproceedings{Daniel:08, author={Lopresti,
                                    Daniel}, title={Optical character recognition errors and their effects on natural language processing}, booktitle={Proceedings of the second workshop on Analytics for noisy unstructured text data}, pages={}, year={2008},
                                    organization={ACM} } } @article{Samuel:15, title={A large annotated corpus for learning natural language inference}, author={Bowman, Samuel R and Angeli, Gabor and Potts, Christopher and Manning, Christopher D}, journal={arXiv
                                    preprint arXiv:1508.05326}, year={2015} } @article{Zhiguo:17, title={Bilateral multi-perspective matching for natural language sentences}, author={Wang, Zhiguo and Hamza, Wael and Florian, Radu}, journal={arXiv preprint
                                    arXiv:1702.03814}, year={2017} } @article{Samuel:14, title={Recursive neural networks can learn logical semantics}, author={Bowman, Samuel R and Potts, Christopher and Manning, Christopher D}, journal={arXiv preprint
                                    arXiv:1406.1827}, year={2014} } @article{Yichen:17, title={Natural language inference over interaction space}, author={Gong, Yichen and Luo, Heng and Zhang, Jian}, journal={arXiv preprint arXiv:1709.04348}, year={2017}
                                    } @inproceedings{Guibin:17, title={Ensemble application of convolutional and recurrent neural networks for multi-label text categorization}, author={Chen, Guibin and Ye, Deheng and Xing, Zhenchang and Chen, Jieshan
                                    and Cambria, Erik}, booktitle={2017 International Joint Conference on Neural Networks (IJCNN)}, pages={}, year={2017}, organization={IEEE} } @article{Sepp:97, title={Long short-term memory}, author={Hochreiter, Sepp
                                    and Schmidhuber, J{\"u}rgen}, journal={Neural computation}, volume={9}, number={8}, pages={}, year={1997}, publisher={MIT Press} } @inproceedings{Xingyou:16, title={Combination of convolutional and recurrent neural
                                    network for sentiment analysis of short texts}, author={Wang, Xingyou and Jiang, Weijie and Luo, Zhiyong}, booktitle={Proceedings of COLING 2016, the 26th International Conference on Computational Linguistics: Technical
                                    Papers}, pages={}, year={2016} } @inproceedings{Gomez:18, title = {Single Shot Scene Text Retrieval}, author = {Lluis Gomez, Andres Mafla, Marçal Rusiñol, and Dimosthenis Karatzas}, booktitle = {ECCV}, year = {2018}
                                    } @article{raffel2015feed, title={Feed-forward networks with attention can solve some long-term memory problems}, author={Raffel, Colin and Ellis, Daniel PW}, journal={arXiv preprint arXiv:1512.08756}, year={2015} }
                                    @article{Chunting:15, title={A C-LSTM neural network for text classification}, author={Zhou, Chunting and Sun, Chonglin and Liu, Zhiyuan and Lau, Francis}, journal={arXiv preprint arXiv:1511.08630}, year={2015} } @article{Dzmitry:14,
                                    title={Neural machine translation by jointly learning to align and translate}, author={Bahdanau, Dzmitry and Cho, Kyunghyun and Bengio, Yoshua}, journal={arXiv preprint arXiv:1409.0473}, year={2014} } @article{Timothy:16,
                                    title={Incorporating nesterov momentum into adam}, author={Dozat, Timothy}, year={2016} } @article{Ahmed:18, title={Visual Re-ranking with Natural Language Understanding for Text Spotting}, author={Sabir, Ahmed and
                                    Moreno-Noguer, Francesc and Padr{\'o}, Llu{\'\i}s}, journal={arXiv preprint arXiv:1810.12738}, year={2018} } @article{Yuval:17, title={Mimicking word embeddings using subword rnns}, author={Pinter, Yuval and Guthrie,
                                    Robert and Eisenstein, Jacob}, journal={arXiv preprint arXiv:1707.06961}, year={2017} } @inproceedings{Ming:16, title={Improved representation learning for question answer matching}, author={Tan, Ming and Dos Santos,
                                    Cicero and Xiang, Bing and Zhou, Bowen}, booktitle={Proceedings of the 54th Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers)}, volume={1}, pages={}, year={2016} } @inproceedings{Vinod:10,
                                    title={Rectified linear units improve restricted boltzmann machines}, author={Nair, Vinod and Hinton, Geoffrey E}, booktitle={Proceedings of the 27th international conference on machine learning (ICML-10)}, pages={},
                                    year={2010} } @article{Sergey:15, title={Batch normalization: Accelerating deep network training by reducing internal covariate shift}, author={Ioffe, Sergey and Szegedy, Christian}, journal={arXiv preprint arXiv:1502.03167},
                                    year={2015} } @inproceedings{Alex:08, title={Unconstrained on-line handwriting recognition with recurrent neural networks}, author={Graves, Alex and Liwicki, Marcus and Bunke, Horst and Schmidhuber, J{\"u}rgen and Fern{\'a}ndez,
                                    Santiago}, booktitle={Advances in neural information processing systems}, pages={}, year={2008} } @article{Wenpeng:16, title={Multichannel variable-size convolution for sentence classification}, author={Yin, Wenpeng
                                    and Sch{\"u}tze, Hinrich}, journal={arXiv preprint arXiv:1603.04513}, year={2016} } @inproceedings{Aliaksei:15, title={Learning to rank short text pairs with convolutional deep neural networks}, author={Severyn, Aliaksei
                                    and Moschitti, Alessandro}, booktitle={Proceedings of the 38th international ACM SIGIR conference on research and development in information retrieval}, pages={}, year={2015}, organization={ACM} } @inproceedings{Jacob:19,
                                    title={BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding}, author={Devlin, Jacob and Chang, Ming-Wei and Lee, Kenton and Toutanova, Kristina}, booktitle={NAACL-HLT (1)}, year={2019} }
                                    @inproceedings{Peters:18, author={Peters, Matthew E. and Neumann, Mark and Iyyer, Mohit and Gardner, Matt and Clark, Christopher and Lee, Kenton and Zettlemoyer, Luke}, title={Deep contextualized word representations},
                                    booktitle={Proc. of NAACL}, year={2018} } @article{Daniel:18, title={Universal sentence encoder}, author={Cer, Daniel and Yang, Yinfei and Kong, Sheng-yi and Hua, Nan and Limtiaco, Nicole and John, Rhomni St and Constant,
                                    Noah and Guajardo-Cespedes, Mario and Yuan, Steve and Tar, Chris and others}, journal={arXiv preprint arXiv:1803.11175}, year={2018} } @InProceedings{Armand:17, title={Bag of Tricks for Efficient Text Classification},
                                    author={Joulin, Armand and Grave, Edouard and Bojanowski, Piotr and Mikolov, Tomas}, booktitle={Proceedings of the 15th Conference of the European Chapter of the Association for Computational Linguistics: Volume 2,
                                    Short Papers}, month={April}, year={2017}, publisher={Association for Computational Linguistics}, pages={}, } @article{Nal:14, title={A convolutional neural network for modelling sentences}, author={Kalchbrenner, Nal
                                    and Grefenstette, Edward and Blunsom, Phil}, journal={arXiv preprint arXiv:1404.2188}, year={2014} } @inproceedings{Oriol:15, title={Show and tell: A neural image caption generator}, author={Vinyals, Oriol and Toshev,
                                    Alexander and Bengio, Samy and Erhan, Dumitru}, booktitle={Proceedings of the IEEE conference on computer vision and pattern recognition}, pages={3156--3164}, year={2015} } @inproceedings{Christian:17, title={Inception-v4,
                                    inception-resnet and the impact of residual connections on learning}, author={Szegedy, Christian and Ioffe, Sergey and Vanhoucke, Vincent and Alemi, Alexander A}, booktitle={Thirty-First AAAI Conference on Artificial
                                    Intelligence}, year={2017} } @InProceedings{Alexis:17, author = {Conneau, Alexis and Kiela, Douwe and Schwenk, Holger and Barrault, Lo\"{i}c and Bordes, Antoine}, title = {Supervised Learning of Universal Sentence Representations
                                    from Natural Language Inference Data}, booktitle = {Proceedings of the 2017 Conference on Empirical Methods in Natural Language Processing}, month = {September}, year = {2017}, address = {Copenhagen, Denmark}, publisher
                                    = {Association for Computational Linguistics}, pages = {670--680}, url = {https://www.aclweb.org/anthology/D17-1070} } @article{Pierre:16, title={Opensubtitles2016: Extracting large parallel corpora from movie and tv
                                    subtitles}, author={Lison, Pierre and Tiedemann, J{\"o}rg}, year={2016}, publisher={European Language Resources Association} } @InProceedings{Shitala:18, author = {Prasad, Shitala and Wai Kin Kong, Adams}, title = {Using
                                    Object Information for Spotting Text}, booktitle = {The European Conference on Computer Vision (ECCV)}, month = {September}, year = {2018} } @inproceedings{Shancheng:18, title={Attention and Language Ensemble for Scene
                                    Text Recognition with Convolutional Sequence Modeling}, author={Fang, Shancheng and Xie, Hongtao and Zha, Zheng-Jun and Sun, Nannan and Tan, Jianlong and Zhang, Yongdong}, booktitle={2018 ACM Multimedia Conference on
                                    Multimedia Conference}, pages={}, year={2018}, organization={ACM} } @inproceedings{Ashish:17, title={Attention is all you need}, author={Vaswani, Ashish and Shazeer, Noam and Parmar, Niki and Uszkoreit, Jakob and Jones,
                                    Llion and Gomez, Aidan N and Kaiser, {\L}ukasz and Polosukhin, Illia}, booktitle={Advances in neural information processing systems}, pages={5998--6008}, year={2017} } @inproceedings{Marcella:20, title={Meshed-Memory
                                    Transformer for Image Captioning}, author={Cornia, Marcella and Stefanini, Matteo and Baraldi, Lorenzo and Cucchiara, Rita}, booktitle={Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition},
                                    pages={10578--10587}, year={2020} } @inproceedings{Qi:16, title={Keyphrase extraction using deep recurrent neural networks on twitter}, author={Zhang, Qi and Wang, Yang and Gong, Yeyun and Huang, Xuan-Jing}, booktitle={Proceedings
                                    of the 2016 conference on empirical methods in natural language processing}, pages={}, year={2016} } @article{young2014image, title={From image descriptions to visual denotations: New similarity metrics for semantic
                                    inference over event descriptions}, author={Young, Peter and Lai, Alice and Hodosh, Micah and Hockenmaier, Julia}, journal={TACL}, volume={}, pages={}, year={2014}, publisher={MIT Press} } @inproceedings{karpathy2015deep,
                                    title={Deep visual-semantic alignments for generating image descriptions}, author={Karpathy, Andrej and Fei-Fei, Li}, booktitle={Proceedings of the IEEE conference on computer vision and pattern recognition}, pages={3128--3137},
                                    year={2015} } @inproceedings{banerjee2005meteor, title={METEOR: An automatic metric for MT evaluation with improved correlation with human judgments}, author={Banerjee, Satanjeev and Lavie, Alon}, booktitle={Proceedings
                                    of the acl workshop on intrinsic and extrinsic evaluation measures for machine translation and/or summarization}, pages={65--72}, year={2005} } @inproceedings{vedantam2015cider, title={Cider: Consensus-based image description
                                    evaluation}, author={Vedantam, Ramakrishna and Lawrence Zitnick, C and Parikh, Devi}, booktitle={Proceedings of the IEEE conference on computer vision and pattern recognition}, pages={4566--4575}, year={2015} } @article{reimers2019sentence,
                                    title={Sentence-bert: Sentence embeddings using siamese bert-networks}, author={Reimers, Nils and Gurevych, Iryna}, journal={arXiv preprint arXiv:1908.10084}, year={2019} } @article{hinton1999products, title={Products
                                    of experts}, author={Hinton, Geoffrey E}, year={1999}, publisher={IET} } @article{meng2017deep, title={Deep keyphrase generation}, author={Meng, Rui and Zhao, Sanqiang and Han, Shuguang and He, Daqing and Brusilovsky,
                                    Peter and Chi, Yu}, journal={arXiv preprint arXiv:1704.06879}, year={2017} } @inproceedings{xu2015show, title={Show, attend and tell: Neural image caption generation with visual attention}, author={Xu, Kelvin and Ba,
                                    Jimmy and Kiros, Ryan and Cho, Kyunghyun and Courville, Aaron and Salakhudinov, Ruslan and Zemel, Rich and Bengio, Yoshua}, booktitle={International conference on machine learning}, pages={2048--2057}, year={2015} }
                                    @inproceedings{anderson2018bottom, title={Bottom-up and top-down attention for image captioning and visual question answering}, author={Anderson, Peter and He, Xiaodong and Buehler, Chris and Teney, Damien and Johnson,
                                    Mark and Gould, Stephen and Zhang, Lei}, booktitle={Proceedings of the IEEE conference on computer vision and pattern recognition}, pages={}, year={2018} } @article{JDH17, title={Billion-scale similarity search with
                                    GPUs}, author={Johnson, Jeff and Douze, Matthijs and J{\'e}gou, Herv{\'e}}, journal={arXiv preprint arXiv:1702.08734}, year={2017} } @inproceedings{lu202012, title={12-in-1: Multi-task vision and language representation
                                    learning}, author={Lu, Jiasen and Goswami, Vedanuj and Rohrbach, Marcus and Parikh, Devi and Lee, Stefan}, booktitle={Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition}, pages={10437--10446},
                                    year={2020} } @inproceedings{bert-score, title={BERTScore: Evaluating Text Generation with BERT}, author={Tianyi Zhang and Varsha Kishore* and Felix Wu* and Kilian Q. Weinberger and Yoav Artzi}, booktitle={International
                                    Conference on Learning Representations}, year={2020}, url={https://openreview.net/forum?id=SkeHuCVFDr} } @inproceedings{blok2003probability, title={Probability from similarity}, author={Blok, Sergey and Medin, Douglas
                                    and Osherson, Daniel}, booktitle={AAAI Spring Symposium on Logical Formalization of Commonsense Reasoning}, pages={}, year={2003} } @article{radford2019language, title={Language models are unsupervised multitask learners},
                                    author={Radford, Alec and Wu, Jeffrey and Child, Rewon and Luan, David and Amodei, Dario and Sutskever, Ilya}, journal={OpenAI blog}, volume={}, number={}, pages={}, year={2019} } @article{liu2019roberta, title={Roberta:
                                    A robustly optimized bert pretraining approach}, author={Liu, Yinhan and Ott, Myle and Goyal, Naman and Du, Jingfei and Joshi, Mandar and Chen, Danqi and Levy, Omer and Lewis, Mike and Zettlemoyer, Luke and Stoyanov,
                                    Veselin}, journal={arXiv preprint arXiv:1907.11692}, year={2019} } @article{wang2018glue, title={GLUE: A multi-task benchmark and analysis platform for natural language understanding}, author={Wang, Alex and Singh,
                                    Amanpreet and Michael, Julian and Hill, Felix and Levy, Omer and Bowman, Samuel R}, journal={arXiv preprint arXiv:1804.07461}, year={2018} } @article{mccarthy2010mtld, title={MTLD, vocd-D, and HD-D: A validation study
                                    of sophisticated approaches to lexical diversity assessment}, author={McCarthy, Philip M and Jarvis, Scott}, journal={Behavior research methods}, volume={42}, number={2}, pages={381--392}, year={2010}, publisher={Springer}
                                    } @article{koehn2017six, title={Six challenges for neural machine translation}, author={Koehn, Philipp and Knowles, Rebecca}, journal={arXiv preprint arXiv:1706.03872}, year={2017} } @inproceedings{huang2019attention,
                                    title={Attention on attention for image captioning}, author={Huang, Lun and Wang, Wenmin and Chen, Jie and Wei, Xiao-Yong}, booktitle={Proceedings of the IEEE/CVF International Conference on Computer Vision}, pages={4634--4643},
                                    year={2019} } @inproceedings{luo2018discriminability, title={Discriminability objective for training descriptive captions}, author={Luo, Ruotian and Price, Brian and Cohen, Scott and Shakhnarovich, Gregory}, booktitle={Proceedings
                                    of the IEEE Conference on Computer Vision and Pattern Recognition}, pages={6964--6974}, year={2018} } @inproceedings{wang2019describing, title={Describing like humans: on diversity in image captioning}, author={Wang,
                                    Qingzhong and Chan, Antoni B}, booktitle={Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition}, pages={4195--4203}, year={2019} } @article{cer2017semeval, title={Semeval-2017 task 1: Semantic
                                    textual similarity-multilingual and cross-lingual focused evaluation}, author={Cer, Daniel and Diab, Mona and Agirre, Eneko and Lopez-Gazpio, Inigo and Specia, Lucia}, journal={arXiv preprint arXiv:1708.00055}, year={2017}
                                    } @InProceedings{Shitala:18, author = {Prasad, Shitala and Wai Kin Kong, Adams}, title = {Using Object Information for Spotting Text}, booktitle = {The European Conference on Computer Vision (ECCV)}, month = {September},
                                    year = {2018} } @inproceedings{Shancheng:18, title={Attention and Language Ensemble for Scene Text Recognition with Convolutional Sequence Modeling}, author={Fang, Shancheng and Xie, Hongtao and Zha, Zheng-Jun and Sun,
                                    Nannan and Tan, Jianlong and Zhang, Yongdong}, booktitle={2018 ACM Multimedia Conference on Multimedia Conference}, pages={}, year={2018}, organization={ACM} } @inproceedings{vijayakumar2018diverse, title={Diverse beam
                                    search for improved description of complex scenes}, author={Vijayakumar, Ashwin and Cogswell, Michael and Selvaraju, Ramprasaath and Sun, Qing and Lee, Stefan and Crandall, David and Batra, Dhruv}, booktitle={Proceedings
                                    of the AAAI Conference on Artificial Intelligence}, volume={}, number={}, year={2018} } @inproceedings{sabir2018visual, title={Visual re-ranking with natural language understanding for text spotting}, author={Sabir,
                                    Ahmed and Moreno-Noguer, Francesc and Padr{\'o}, Llu{\'\i}s}, booktitle={Asian Conference on Computer Vision}, pages={68--82}, year={2018}, organization={Springer} } @inproceedings{anderson2016spice, title={Spice: Semantic
                                    propositional image caption evaluation}, author={Anderson, Peter and Fernando, Basura and Johnson, Mark and Gould, Stephen}, booktitle={European conference on computer vision}, pages={382--398}, year={2016}, organization={Springer}
                                    } @article{lison2016opensubtitles2016, title={Opensubtitles2016: Extracting large parallel corpora from movie and tv subtitles}, author={Lison, Pierre and Tiedemann, J{\"o}rg}, year={2016}, publisher={European Language
                                    Resources Association} } @article{blok2007induction, title={Induction as conditional probability judgment}, author={Blok, Sergey V and Medin, Douglas L and Osherson, Daniel N}, journal={Memory \& Cognition}, volume={35},
                                    number={6}, pages={1353--1364}, year={2007}, publisher={Springer} } @article{voorspoels2015people, title={How do people learn from negative evidence? Non-monotonic generalizations and sampling assumptions in inductive
                                    reasoning}, author={Voorspoels, Wouter and Navarro, Daniel J and Perfors, Amy and Ransom, Keith and Storms, Gert}, journal={Cognitive Psychology}, volume={81}, pages={1--25}, year={2015}, publisher={Elsevier} } @inproceedings{heussen2010can,
                                    title={Can similarity-based models of induction handle negative evidence?}, author={Heussen, Daniel and Voorspoels, Wouter and Storms, Gert}, booktitle={Proceedings of the Annual Conference of the Cognitive Science
                                    Society}, volume={32}, pages={2033--2038}, year={2010}, organization={Lawrence Erlbaum Associates} } @article{sharma2017nlgeval, author = {Sharma, Shikhar and El Asri, Layla and Schulz, Hannes and Zumer, Jeremie}, title
                                    = {Relevance of Unsupervised Metrics in Task-Oriented Dialogue for Evaluating Natural Language Generation}, journal = {CoRR}, volume = {abs/1706.09799}, year = {2017}, url = {http://arxiv.org/abs/1706.09799} } @inproceedings{pennington2014glove,
                                    title={Glove: Global vectors for word representation}, author={Pennington, Jeffrey and Socher, Richard and Manning, Christopher D}, booktitle={Proceedings of the 2014 conference on empirical methods in natural language
                                    processing (EMNLP)}, pages={1532--1543}, year={2014} } @article{li2015diversity, title={A diversity-promoting objective function for neural conversation models}, author={Li, Jiwei and Galley, Michel and Brockett, Chris
                                    and Gao, Jianfeng and Dolan, Bill}, journal={arXiv preprint arXiv:1510.03055}, year={2015} } @article{augenstein2017semeval, title={Semeval 2017 task 10: Scienceie-extracting keyphrases and relations from scientific
                                    publications}, author={Augenstein, Isabelle and Das, Mrinal and Riedel, Sebastian and Vikraman, Lakshmi and McCallum, Andrew}, journal={arXiv preprint arXiv:1704.02853}, year={2017} } @article{zhu2020towards, title={Towards
                                    Understanding Sample Variance in Visually Grounded Language Generation: Evaluations and Observations}, author={Zhu, Wanrong and Wang, Xin Eric and Narayana, Pradyumna and Sone, Kazoo and Basu, Sugato and Wang, William
                                    Yang}, journal={arXiv preprint arXiv:2010.03644}, year={2020} } @inproceedings{lewis2020pretrained, title={Pretrained Language Models for Biomedical and Clinical Tasks: Understanding and Extending the State-of-the-Art},
                                    author={Lewis, Patrick and Ott, Myle and Du, Jingfei and Stoyanov, Veselin}, booktitle={Proceedings of the 3rd Clinical Natural Language Processing Workshop}, pages={146--157}, year={2020} } @article{wang2018object,
                                    title={Object counts! bringing explicit detections back into image captioning}, author={Wang, Josiah and Madhyastha, Pranava and Specia, Lucia}, journal={arXiv preprint arXiv:1805.00314}, year={2018} } @inproceedings{fang2015captions,
                                    title={From captions to visual concepts and back}, author={Fang, Hao and Gupta, Saurabh and Iandola, Forrest and Srivastava, Rupesh K and Deng, Li and Doll{\'a}r, Piotr and Gao, Jianfeng and He, Xiaodong and Mitchell,
                                    Margaret and Platt, John C and others}, booktitle={Proceedings of the IEEE conference on computer vision and pattern recognition}, pages={1473--1482}, year={2015} } @inproceedings{zhang2020context, title={Context-aware
                                    attention network for image-text retrieval}, author={Zhang, Qi and Lei, Zhen and Zhang, Zhaoxiang and Li, Stan Z}, booktitle={Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition}, pages={3536--3545},
                                    year={2020} } @inproceedings{cornia2019show, title={Show, control and tell: A framework for generating controllable and grounded captions}, author={Cornia, Marcella and Baraldi, Lorenzo and Cucchiara, Rita}, booktitle={Proceedings
                                    of the IEEE/CVF Conference on Computer Vision and Pattern Recognition}, pages={8307--8316}, year={2019} } @book{brown2005encyclopedia, title={Encyclopedia of language and linguistics}, author={Brown, Keith}, volume={1},
                                    year={2005}, publisher={Elsevier} } @article{zhang2019bertscore, title={Bertscore: Evaluating text generation with bert}, author={Zhang, Tianyi and Kishore, Varsha and Wu, Felix and Weinberger, Kilian Q and Artzi, Yoav},
                                    journal={arXiv preprint arXiv:1904.09675}, year={2019} } @inproceedings{dai2017towards, title={Towards diverse and natural image descriptions via a conditional gan}, author={Dai, Bo and Fidler, Sanja and Urtasun, Raquel
                                    and Lin, Dahua}, booktitle={Proceedings of the IEEE International Conference on Computer Vision}, pages={2970--2979}, year={2017} } @inproceedings{vedantam2017context, title={Context-aware captions from context-agnostic
                                    supervision}, author={Vedantam, Ramakrishna and Bengio, Samy and Murphy, Kevin and Parikh, Devi and Chechik, Gal}, booktitle={Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition}, pages={251--260},
                                    year={2017} } @article{herdade2019image, title={Image captioning: Transforming objects into words}, author={Herdade, Simao and Kappeler, Armin and Boakye, Kofi and Soares, Joao}, journal={arXiv preprint arXiv:1906.05963},
                                    year={2019} } @inproceedings{rennie2017self, title={Self-critical sequence training for image captioning}, author={Rennie, Steven J and Marcheret, Etienne and Mroueh, Youssef and Ross, Jerret and Goel, Vaibhava}, booktitle={Proceedings
                                    of the IEEE Conference on Computer Vision and Pattern Recognition}, pages={7008--7024}, year={2017} } @inproceedings{you2016image, title={Image captioning with semantic attention}, author={You, Quanzeng and Jin, Hailin
                                    and Wang, Zhaowen and Fang, Chen and Luo, Jiebo}, booktitle={Proceedings of the IEEE conference on computer vision and pattern recognition}, pages={4651--4659}, year={2016} } } @inproceedings{ren2017deep, title={Deep
                                    reinforcement learning-based image captioning with embedding reward}, author={Ren, Zhou and Wang, Xiaoyu and Zhang, Ning and Lv, Xutao and Li, Li-Jia}, booktitle={Proceedings of the IEEE conference on computer vision
                                    and pattern recognition}, pages={290--298}, year={2017} } } @article{ippolito2019comparison, title={Comparison of diverse decoding methods from conditional language models}, author={Ippolito, Daphne and Kriz, Reno and
                                    Kustikova, Maria and Sedoc, Jo{\~a}o and Callison-Burch, Chris}, journal={arXiv preprint arXiv:1906.06362}, year={2019} } @article{voorspoels2015people, title={How do people learn from negative evidence? Non-monotonic
                                    generalizations and sampling assumptions in inductive reasoning}, author={Voorspoels, Wouter and Navarro, Daniel J and Perfors, Amy and Ransom, Keith and Storms, Gert}, journal={Cognitive Psychology}, volume={81}, pages={1--25},
                                    year={2015}, publisher={Elsevier} } @article{li2015diversity, title={A diversity-promoting objective function for neural conversation models}, author={Li, Jiwei and Galley, Michel and Brockett, Chris and Gao, Jianfeng
                                    and Dolan, Bill}, journal={arXiv preprint arXiv:1510.03055}, year={2015} } @inproceedings{gupta2020contrastive, title={Contrastive learning for weakly supervised phrase grounding}, author={Gupta, Tanmay and Vahdat,
                                    Arash and Chechik, Gal and Yang, Xiaodong and Kautz, Jan and Hoiem, Derek}, booktitle={Computer Vision--ECCV 2020: 16th European Conference, Glasgow, UK, August 23--28, 2020, Proceedings, Part III 16}, pages={752--768},
                                    year={2020}, organization={Springer} } @inproceedings{song2021sentsim, title={SentSim: Crosslingual Semantic Evaluation of Machine Translation}, author={Song, Yurun and Zhao, Junchen and Specia, Lucia}, booktitle={Proceedings
                                    of the 2021 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies}, pages={3143--3156}, year={2021} } @inproceedings{zhang2021rstnet, title={RSTNet: Captioning
                                    With Adaptive Attention on Visual and Non-Visual Words}, author={Zhang, Xuying and Sun, Xiaoshuai and Luo, Yunpeng and Ji, Jiayi and Zhou, Yiyi and Wu, Yongjian and Huang, Feiyue and Ji, Rongrong}, booktitle={Proceedings
                                    of the IEEE/CVF Conference on Computer Vision and Pattern Recognition}, pages={15465--15474}, year={2021} } @inproceedings{wang2020compare, title={Compare and reweight: Distinctive image captioning using similar images
                                    sets}, author={Wang, Jiuniu and Xu, Wenjia and Wang, Qingzhong and Chan, Antoni B}, booktitle={European Conference on Computer Vision}, pages={370--386}, year={2020}, organization={Springer} } @article{gao2021simcse,
                                    title={{SimCSE}: Simple Contrastive Learning of Sentence Embeddings}, author={Gao, Tianyu and Yao, Xingcheng and Chen, Danqi}, journal={arXiv preprint arXiv:2104.08821}, url={https://arxiv.org/pdf/2104.08821.pdf}, year={2021} } @article{conneau2017supervised,
                                    title={Supervised learning of universal sentence representations from natural language inference data}, author={Conneau, Alexis and Kiela, Douwe and Schwenk, Holger and Barrault, Loic and Bordes, Antoine}, journal={arXiv
                                    preprint arXiv:1705.02364}, year={2017} } @article{radford2021learning, title={Learning transferable visual models from natural language supervision}, author={Radford, Alec and Kim, Jong Wook and Hallacy, Chris and
                                    Ramesh, Aditya and Goh, Gabriel and Agarwal, Sandhini and Sastry, Girish and Askell, Amanda and Mishkin, Pamela and Clark, Jack and others}, journal={arXiv preprint arXiv:2103.00020}, year={2021} } @inproceedings{huang2017speed,
                                    title={Speed/accuracy trade-offs for modern convolutional object detectors}, author={Huang, Jonathan and Rathod, Vivek and Sun, Chen and Zhu, Menglong and Korattikara, Anoop and Fathi, Alireza and Fischer, Ian and Wojna,
                                    Zbigniew and Song, Yang and Guadarrama, Sergio and others}, booktitle={Proceedings of the IEEE conference on computer vision and pattern recognition}, pages={}, year={2017} } @inproceedings{papineni2002bleu, title={Bleu:
                                    a method for automatic evaluation of machine translation}, author={Papineni, Kishore and Roukos, Salim and Ward, Todd and Zhu, Wei-Jing}, booktitle={Proceedings of the 40th annual meeting of the Association for Computational
                                    Linguistics}, pages={311--318}, year={2002} } @inproceedings{lin2004rouge, title={Rouge: A package for automatic evaluation of summaries}, author={Lin, Chin-Yew}, booktitle={Text summarization branches out}, pages={74--81},
                                    year={2004} } @inproceedings{clark2011better, title={Better hypothesis testing for statistical machine translation: Controlling for optimizer instability}, author={Clark, Jonathan H and Dyer, Chris and Lavie, Alon and
                                    Smith, Noah A}, booktitle={Proceedings of the 49th Annual Meeting of the Association for Computational Linguistics: Human Language Technologies}, pages={176--181}, year={2011} } @inproceedings{deshpande2019fast, title={Fast,
                                    diverse and accurate image captioning guided by part-of-speech}, author={Deshpande, Aditya and Aneja, Jyoti and Wang, Liwei and Schwing, Alexander G and Forsyth, David}, booktitle={Proceedings of the IEEE/CVF Conference
                                    on Computer Vision and Pattern Recognition}, pages={10695--10704}, year={2019} } @inproceedings{chen2020say, title={Say as you wish: Fine-grained control of image caption generation with abstract scene graphs}, author={Chen,
                                    Shizhe and Jin, Qin and Wang, Peng and Wu, Qi}, booktitle={Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition}, pages={9962--9971}, year={2020} } @inproceedings{zhang2021consensus, title={Consensus
                                    graph representation learning for better grounded image captioning}, author={Zhang, Wenqiao and Shi, Haochen and Tang, Siliang and Xiao, Jun and Yu, Qiang and Zhuang, Yueting}, booktitle={Proc 35 AAAI Conf on Artificial
                                    Intelligence}, year={2021} } @inproceedings{elliott2014comparing, title={Comparing automatic evaluation measures for image description}, author={Elliott, Desmond and Keller, Frank}, booktitle={Proceedings of the 52nd
                                    Annual Meeting of the Association for Computational Linguistics (Volume 2: Short Papers)}, pages={452--457}, year={2014} } @inproceedings{changpinyo2021conceptual, title={Conceptual 12M: Pushing Web-Scale Image-Text
                                    Pre-Training To Recognize Long-Tail Visual Concepts}, author={Changpinyo, Soravit and Sharma, Piyush and Ding, Nan and Soricut, Radu}, booktitle={Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern
                                    Recognition}, pages={3558--3568}, year={2021} } @inproceedings{abadi2016tensorflow, title={Tensorflow: A system for large-scale machine learning}, author={Abadi, Mart{\'\i}n and Barham, Paul and Chen, Jianmin and Chen,
                                    Zhifeng and Davis, Andy and Dean, Jeffrey and Devin, Matthieu and Ghemawat, Sanjay and Irving, Geoffrey and Isard, Michael and others}, booktitle={12th $\{$USENIX$\}$ symposium on operating systems design and implementation
                                    ($\{$OSDI$\}$ 16)}, pages={265--283}, year={2016} } @article{paszke2019pytorch, title={Pytorch: An imperative style, high-performance deep learning library}, author={Paszke, Adam and Gross, Sam and Massa, Francisco
                                    and Lerer, Adam and Bradbury, James and Chanan, Gregory and Killeen, Trevor and Lin, Zeming and Gimelshein, Natalia and Antiga, Luca and others}, journal={arXiv preprint arXiv:1912.01703}, year={2019} } @inproceedings{ott2019fairseq,
                                    title = {fairseq: A Fast, Extensible Toolkit for Sequence Modeling}, author = {Myle Ott and Sergey Edunov and Alexei Baevski and Angela Fan and Sam Gross and Nathan Ng and David Grangier and Michael Auli}, booktitle
                                    = {Proceedings of NAACL-HLT 2019: Demonstrations}, year = {2019}, } @inproceedings{post2018call, title={A Call for Clarity in Reporting BLEU Scores}, author={Post, Matt}, booktitle={Proceedings of the Third Conference
                                    on Machine Translation: Research Papers}, pages={186--191}, year={2018} } @inproceedings{popovic2015chrf, title={chrF: character n-gram F-score for automatic MT evaluation}, author={Popovi{\'c}, Maja}, booktitle={Proceedings
                                    of the Tenth Workshop on Statistical Machine Translation}, pages={392--395}, year={2015} } @inproceedings{shetty2017speaking, title={Speaking the same language: Matching machine to human captions by adversarial training},
                                    author={Shetty, Rakshith and Rohrbach, Marcus and Anne Hendricks, Lisa and Fritz, Mario and Schiele, Bernt}, booktitle={Proceedings of the IEEE International Conference on Computer Vision}, pages={4135--4144}, year={2017}
                                    } @inproceedings{peinelt2020tbert, title={tBERT: Topic models and BERT joining forces for semantic similarity detection}, author={Peinelt, Nicole and Nguyen, Dong and Liakata, Maria}, booktitle={Proceedings of the 58th
                                    Annual Meeting of the Association for Computational Linguistics}, pages={7047--7055}, year={2020} } @inproceedings{rashtchian2010collecting, title={Collecting image annotations using amazon’s mechanical turk}, author={Rashtchian,
                                    Cyrus and Young, Peter and Hodosh, Micah and Hockenmaier, Julia}, booktitle={Proceedings of the NAACL HLT 2010 workshop on creating speech and language data with Amazon’s Mechanical Turk}, pages={139--147}, year={2010}
                                    } @article{jaderberg2016reading, title={Reading text in the wild with convolutional neural networks}, author={Jaderberg, Max and Simonyan, Karen and Vedaldi, Andrea and Zisserman, Andrew}, journal={International Journal
                                    of Computer Vision}, volume={}, number={}, pages={}, year={2016}, publisher={Springer} } @article{camacho2019relational, title={Relational Word Embeddings}, author={Camacho-Collados, Jose and Espinosa-Anke, Luis and
                                    Schockaert, Steven}, journal={arXiv preprint arXiv:1906.01373}, year={2019} } @article{joulin2017bag, title={Bag of Tricks for Efficient Text Classification}, author={Joulin, Armand and Grave, Edouard and Mikolov, Piotr
                                    Bojanowski Tomas}, journal={EACL 2017}, pages={427}, year={2017} } @inproceedings{mikolov2013distributed, title={Distributed representations of words and phrases and their compositionality}, author={Mikolov, Tomas and
                                    Sutskever, Ilya and Chen, Kai and Corrado, Greg S and Dean, Jeff}, booktitle={Advances in neural information processing systems}, pages={}, year={2013} } @article{turney2010frequency, title={From frequency to meaning:
                                    Vector space models of semantics}, author={Turney, Peter D and Pantel, Patrick}, journal={Journal of artificial intelligence research}, volume={37}, pages={141--188}, year={2010} } @article{navigli2012babelnet, title={BabelNet:
                                    The automatic construction, evaluation and application of a wide-coverage multilingual semantic network}, author={Navigli, Roberto and Ponzetto, Simone Paolo}, journal={Artificial intelligence}, volume={193}, pages={217--250},
                                    year={2012}, publisher={Elsevier} } @inproceedings{turney2001mining, title={Mining the web for synonyms: PMI-IR versus LSA on TOEFL}, author={Turney, Peter D}, booktitle={European conference on machine learning}, pages={491--502},
                                    year={2001}, organization={Springer} } @inproceedings{turney2001mining, title={Mining the web for synonyms: PMI-IR versus LSA on TOEFL}, author={Turney, Peter D}, booktitle={European conference on machine learning},
                                    pages={491--502}, year={2001}, organization={Springer} } @article{landauer1998introduction, title={An introduction to latent semantic analysis}, author={Landauer, Thomas K and Foltz, Peter W and Laham, Darrell}, journal={Discourse
                                    processes}, volume={25}, number={2-3}, pages={259--284}, year={1998}, publisher={Taylor \& Francis} } @article{cha2007comprehensive, title={Comprehensive survey on distance/similarity measures between probability density
                                    functions}, author={Cha, Sung-Hyuk}, journal={City}, volume={1}, number={2}, pages={1}, year={2007} } @article{xu2005survey, title={Survey of clustering algorithms}, author={Xu, Rui and Wunsch, Donald}, journal={IEEE
                                    Transactions on neural networks}, volume={16}, number={3}, pages={645--678}, year={2005}, publisher={Ieee} } @article{sanchez2012ontology, title={Ontology-based semantic similarity: A new feature-based approach}, author={S{\'a}nchez,
                                    David and Batet, Montserrat and Isern, David and Valls, Aida}, journal={Expert systems with applications}, volume={39}, number={9}, pages={7718--7728}, year={2012}, publisher={Elsevier} } @book{miller1998wordnet, title={WordNet:
                                    An electronic lexical database}, author={Miller, George A}, year={1998}, publisher={MIT press} } @conference{veit2016cocotext, title = {COCO-Text: Dataset and Benchmark for Text Detection and Recognition in Natural
                                    Images}, author = {Andreas Veit and Tomas Matera and Lukas Neumann and Jiri Matas and Serge Belongie}, url = {http://vision.cornell.edu/se3/wp-content/uploads/2016/01/1601.07140v1.pdf}, year = {2016}, date = {2016-01-26},
                                    booktitle = {arXiv preprint arXiv:1601.07140}, keywords = {} } @inproceedings{ghosh2017visual, title={Visual attention models for scene text recognition}, author={Ghosh, Suman K and Valveny, Ernest and Bagdanov, Andrew
                                    D}, booktitle={2017 14th IAPR international conference on document analysis and recognition (ICDAR)}, volume={1}, pages={943--948}, year={2017}, organization={IEEE} } @article{zhou2017places, title={Places: A 10 million
                                    image database for scene recognition}, author={Zhou, Bolei and Lapedriza, Agata and Khosla, Aditya and Oliva, Aude and Torralba, Antonio}, journal={IEEE transactions on pattern analysis and machine intelligence}, volume={40},
                                    number={6}, pages={1452--1464}, year={2017}, publisher={IEEE} } @inproceedings{, title={The berkeley framenet project}, author={Baker, Collin F and Fillmore, Charles J and Lowe, John B}, booktitle={COLING 1998 Volume
                                    1: The 17th International Conference on Computational Linguistics}, year={1998} } @inproceedings{he2016deep, title={Deep residual learning for image recognition}, author={He, Kaiming and Zhang, Xiangyu and Ren, Shaoqing
                                    and Sun, Jian}, booktitle={Proceedings of the IEEE conference on computer vision and pattern recognition}, pages={770--778}, year={2016} } @inproceedings{szegedy2017inception, title={Inception-v4, inception-resnet and
                                    the impact of residual connections on learning}, author={Szegedy, Christian and Ioffe, Sergey and Vanhoucke, Vincent and Alemi, Alexander A}, booktitle={Thirty-first AAAI conference on artificial intelligence}, year={2017}
                                    } @inproceedings{iacobacci2019lstmembed, title={Lstmembed: Learning word and sense representations from a large semantically annotated corpus with long short-term memories}, author={Iacobacci, Ignacio and Navigli, Roberto},
                                    booktitle={Proceedings of the 57th Annual Meeting of the Association for Computational Linguistics}, pages={1685--1695}, year={2019} } @inproceedings{iacobacci2015sensembed, title={Sensembed: Learning sense embeddings
                                    for word and relational similarity}, author={Iacobacci, Ignacio and Pilehvar, Mohammad Taher and Navigli, Roberto}, booktitle={Proceedings of the 53rd Annual Meeting of the Association for Computational Linguistics
                                    and the 7th International Joint Conference on Natural Language Processing (Volume 1: Long Papers)}, pages={95--105}, year={2015} } @article{mancini2016embedding, title={Embedding words and senses together via joint
                                    knowledge-enhanced training}, author={Mancini, Massimiliano and Camacho-Collados, Jose and Iacobacci, Ignacio and Navigli, Roberto}, journal={arXiv preprint arXiv:1612.02703}, year={2016} } @inproceedings{camacho2015nasari,
                                    title={Nasari: a novel approach to a semantically-aware representation of items}, author={Camacho-Collados, Jos{\'e} and Pilehvar, Mohammad Taher and Navigli, Roberto}, booktitle={Proceedings of the 2015 Conference
                                    of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies}, pages={567--577}, year={2015} } @inproceedings{raganato2016automatic, title={Automatic Construction and Evaluation
                                    of a Large Semantically Enriched Wikipedia.}, author={Raganato, Alessandro and Bovi, Claudio Delli and Navigli, Roberto}, booktitle={IJCAI}, pages={2894--2900}, year={2016} } @article{sabir2020enhancing, title={Enhancing
                                    scene text recognition with visual context information}, author={Sabir, Ahmed}, year={2020}, publisher={Universitat Polit{\`e}cnica de Catalunya} } @inproceedings{lin2014microsoft, title={Microsoft coco: Common objects
                                    in context}, author={Lin, Tsung-Yi and Maire, Michael and Belongie, Serge and Hays, James and Perona, Pietro and Ramanan, Deva and Doll{\'a}r, Piotr and Zitnick, C Lawrence}, booktitle={European conference on computer
                                    vision}, pages={740--755}, year={2014}, organization={Springer} } @inproceedings{lee2018stacked, title={Stacked cross attention for image-text matching}, author={Lee, Kuang-Huei and Chen, Xi and Hua, Gang and Hu, Houdong
                                    and He, Xiaodong}, booktitle={Proceedings of the European Conference on Computer Vision (ECCV)}, pages={201--216}, year={2018} } @inproceedings{mihalcea2006corpus, title={Corpus-based and knowledge-based measures of
                                    text semantic similarity}, author={Mihalcea, Rada and Corley, Courtney and Strapparava, Carlo and others}, booktitle={Aaai}, volume={6}, number={2006}, pages={775--780}, year={2006} } @article{zhu2016computing, title={Computing
                                    semantic similarity of concepts in knowledge graphs}, author={Zhu, Ganggao and Iglesias, Carlos A}, journal={IEEE Transactions on Knowledge and Data Engineering}, volume={29}, number={1}, pages={72--85}, year={2016},
                                    publisher={IEEE} } @inproceedings{budanitsky2001semantic, title={Semantic distance in WordNet: An experimental, application-oriented evaluation of five measures}, author={Budanitsky, Alexander and Hirst, Graeme}, booktitle={Workshop
                                    on WordNet and other lexical resources}, volume={2}, pages={2--2}, year={2001} } @inproceedings{li2020oscar, title={Oscar: Object-semantics aligned pre-training for vision-language tasks}, author={Li, Xiujun and Yin,
                                    Xi and Li, Chunyuan and Zhang, Pengchuan and Hu, Xiaowei and Zhang, Lei and Wang, Lijuan and Hu, Houdong and Dong, Li and Wei, Furu and others}, booktitle={ECCV}, pages={}, year={2020}, organization={Springer} } @inproceedings{fang2015captions,
                                    title={From captions to visual concepts and back}, author={Fang, Hao and Gupta, Saurabh and Iandola, Forrest and Srivastava, Rupesh K and Deng, Li and Doll{\'a}r, Piotr and Gao, Jianfeng and He, Xiaodong and Mitchell,
                                    Margaret and Platt, John C and others}, booktitle={Proceedings of the IEEE conference on computer vision and pattern recognition}, pages={1473--1482}, year={2015} } @article{rohrbach2018object, title={Object hallucination
                                    in image captioning}, author={Rohrbach, Anna and Hendricks, Lisa Anne and Burns, Kaylee and Darrell, Trevor and Saenko, Kate}, journal={arXiv preprint arXiv:1809.02156}, year={2018} } @inproceedings{kim-2014-convolutional,
                                    title = "Convolutional Neural Networks for Sentence Classification", author = "Kim, Yoon", booktitle = "EMNLP", year = "2014", address = "", publisher = "", url = "", doi = "", pages = "", } @inproceedings{Jacob:19,
                                    title={BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding}, author={Devlin, Jacob and Chang, Ming-Wei and Lee, Kenton and Toutanova, Kristina}, booktitle={NAACL-HLT}, year={2019} } @inproceedings{shetty2017speaking,
                                    title={Speaking the same language: Matching machine to human captions by adversarial training}, author={Shetty, Rakshith and Rohrbach, Marcus and Anne Hendricks, Lisa and Fritz, Mario and Schiele, Bernt}, booktitle={Proceedings
                                    of the IEEE International Conference on Computer Vision}, pages={4135--4144}, year={2017} } @inproceedings{deshpande2019fast, title={Fast, diverse and accurate image captioning guided by part-of-speech}, author={Deshpande,
                                    Aditya and Aneja, Jyoti and Wang, Liwei and Schwing, Alexander G and Forsyth, David}, booktitle={Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition}, pages={10695--10704}, year={2019}
                                    } @inproceedings{nair2010rectified, title={Rectified linear units improve restricted boltzmann machines}, author={Nair, Vinod and Hinton, Geoffrey E}, booktitle={Icml}, year={2010} } @inproceedings{hendricks2018women,
                                    title={Women also snowboard: Overcoming bias in captioning models}, author={Hendricks, Lisa Anne and Burns, Kaylee and Saenko, Kate and Darrell, Trevor and Rohrbach, Anna}, booktitle={Proceedings of the European Conference
                                    on Computer Vision (ECCV)}, pages={771--787}, year={2018} } @inproceedings{hendricks2018women, title={Women also snowboard: Overcoming bias in captioning models}, author={Hendricks, Lisa Anne and Burns, Kaylee and Saenko,
                                    Kate and Darrell, Trevor and Rohrbach, Anna}, booktitle={Proceedings of the European Conference on Computer Vision (ECCV)}, pages={771--787}, year={2018} } @article{zhao2017men, title={Men also like shopping: Reducing
                                    gender bias amplification using corpus-level constraints}, author={Zhao, Jieyu and Wang, Tianlu and Yatskar, Mark and Ordonez, Vicente and Chang, Kai-Wei}, journal={arXiv preprint arXiv:1707.09457}, year={2017} }

                                    @article{cho2022fine,
                                        title={Fine-grained image captioning with clip reward},
                                        author={Cho, Jaemin and Yoon, Seunghyun and Kale, Ajinkya and Dernoncourt, Franck and Bui, Trung and Bansal, Mohit},
                                        journal={arXiv preprint arXiv:2205.13115},
                                        url={https://arxiv.org/pdf/2205.13115.pdf},
                                        year={2022}
                                      }

                                    @inproceedings{rennie2017self,
                                        title={Self-critical sequence training for image captioning},
                                        author={Rennie, Steven J and Marcheret, Etienne and Mroueh, Youssef and Ross, Jerret and Goel, Vaibhava},
                                        booktitle={Proceedings of the IEEE conference on computer vision and pattern recognition},
                                        pages={7008--7024},
                                        url={https://openaccess.thecvf.com/content_cvpr_2017/papers/Rennie_Self-Critical_Sequence_Training_CVPR_2017_paper.pdf},
                                        year={2017}
                                      }

                                    @article{hessel2021clipscore,
                                        title={Clipscore: A reference-free evaluation metric for image captioning},
                                        author={Hessel, Jack and Holtzman, Ari and Forbes, Maxwell and Bras, Ronan Le and Choi, Yejin},
                                        journal={arXiv preprint arXiv:2104.08718},
                                        url={https://arxiv.org/pdf/2104.08718.pdf},
                                        year={2021}
                                      }

                                      @article{williams1992simple,
                                        title={Simple statistical gradient-following algorithms for connectionist reinforcement learning},
                                        author={Williams, Ronald J},
                                        journal={Machine learning},
                                        volume={8},
                                        number={3},
                                        pages={229--256},
                                        year={1992},
                                        publisher={Springer},
                                        url={https://link.springer.com/content/pdf/10.1007/BF00992696.pdf}
                                      }
                                      @article{zhang2022mcse,
                                        title={MCSE: Multimodal Contrastive Learning of Sentence Embeddings},
                                        author={Zhang, Miaoran and Mosbach, Marius and Adelani, David Ifeoluwa and Hedderich, Michael A and Klakow, Dietrich},
                                        journal={arXiv preprint arXiv:2204.10931},
                                        url={https://arxiv.org/pdf/2204.10931.pdf},
                                        year={2022}
                                      }

                                      @article{zhang2022neural,
                                        title={Is Neural Topic Modelling Better than Clustering? An Empirical Study on Clustering with Contextual Embeddings for Topics},
                                        author={Zhang, Zihan and Fang, Meng and Chen, Ling and Namazi-Rad, Mohammad-Reza},
                                        journal={arXiv preprint arXiv:2204.09874},
                                        url={https://arxiv.org/pdf/2204.09874.pdf},
                                        year={2022}
                                      }
                                      @article{chuang2022diffcse,
                                        title={DiffCSE: Difference-based Contrastive Learning for Sentence Embeddings},
                                        author={Chuang, Yung-Sung and Dangovski, Rumen and Luo, Hongyin and Zhang, Yang and Chang, Shiyu and Solja{\v{c}}i{\'c}, Marin and Li, Shang-Wen and Yih, Wen-tau and Kim, Yoon and Glass, James},
                                        journal={arXiv preprint arXiv:2204.10298},
                                        url={https://arxiv.org/pdf/2204.10298.pdf},
                                        year={2022}
                                      }
                                      @article{dangovski2021equivariant,
                                        title={Equivariant contrastive learning},
                                        author={Dangovski, Rumen and Jing, Li and Loh, Charlotte and Han, Seungwook and Srivastava, Akash and Cheung, Brian and Agrawal, Pulkit and Solja{\v{c}}i{\'c}, Marin},
                                        journal={arXiv preprint arXiv:2111.00899},
                                        url={https://arxiv.org/pdf/2111.00899.pdf},
                                        year={2021}
                                      }

                                      @article{di2022paragraph,
                                        title={Paragraph-based Transformer Pre-training for Multi-Sentence Inference},
                                        author={Di Liello, Luca and Garg, Siddhant and Soldaini, Luca and Moschitti, Alessandro},
                                        journal={arXiv preprint arXiv:2205.01228},
                                        url={https://arxiv.org/pdf/2205.01228.pdf},
                                        year={2022}
                                      }
                                      @article{crocelearning,
                                        title={Learning to Generate Examples for Semantic Processing Tasks},
                                        author={Croce, Danilo and Filice, Simone and Castellucci, Giuseppe and Basili, Roberto},
                                        url={https://assets.amazon.science/c4/a5/e80efe1f41e391214b2c73f970b5/learning-to-generate-examples-for-semantic-processing-tasks.pdf},
                                      }

                                      @article{shuster2018image,
                                        title={Image chat: Engaging grounded conversations},
                                        author={Shuster, Kurt and Humeau, Samuel and Bordes, Antoine and Weston, Jason},
                                        journal={arXiv preprint arXiv:1811.00945},
                                        url={https://arxiv.org/pdf/1811.00945.pdf},
                                        year={2018}
                                      }
                                      
                                      @article{zhang2022visual,
                                        title={Visual Commonsense in Pretrained Unimodal and Multimodal Models},
                                        author={Zhang, Chenyu and Van Durme, Benjamin and Li, Zhuowan and Stengel-Eskin, Elias},
                                        journal={arXiv preprint arXiv:2205.01850},
                                        url={https://arxiv.org/pdf/2205.01}, 
                                        year={2022}
                                      }


                                      @article{qin2021learning,
                                        title={Learning how to ask: Querying lms with mixtures of soft prompts},
                                        author={Qin, Guanghui and Eisner, Jason},
                                        journal={arXiv preprint arXiv:2104.06599},
                                        url={https://arxiv.org/pdf/2104.06599.pdf},
                                        year={2021}
                                      }
                                      @article{lu2022imagination,
                                        title={Imagination-Augmented Natural Language Understanding},
                                        author={Lu, Yujie and Zhu, Wanrong and Wang, Xin Eric and Eckstein, Miguel and Wang, William Yang},
                                        journal={arXiv preprint arXiv:2204.08535},
                                        url={https://arxiv.org/pdf/2204.08535.pdf},
                                        year={2022}
                                      }

                                      @article{crowson2022vqgan,
                                        title={Vqgan-clip: Open domain image generation and editing with natural language guidance},
                                        author={Crowson, Katherine and Biderman, Stella and Kornis, Daniel and Stander, Dashiell and Hallahan, Eric and Castricato, Louis and Raff, Edward},
                                        journal={arXiv preprint arXiv:2204.08583},
                                        url={https://arxiv.org/pdf/2204.08583.pdf},
                                        year={2022}
                                      }

                                      @article{tan2020vokenization,
                                        title={Vokenization: Improving language understanding with contextualized, visual-grounded supervision},
                                        author={Tan, Hao and Bansal, Mohit},
                                        journal={arXiv preprint arXiv:2010.06775},
                                        url={https://arxiv.org/pdf/2010.06775.pdf},
                                        year={2020}
                                      }

                                      @article{wang2018glue,
                                        title={GLUE: A multi-task benchmark and analysis platform for natural language understanding},
                                        author={Wang, Alex and Singh, Amanpreet and Michael, Julian and Hill, Felix and Levy, Omer and Bowman, Samuel R},
                                        journal={arXiv preprint arXiv:1804.07461},
                                        url={https://arxiv.org/pdf/1804.07461.pdf},
                                        year={2018}
                                      }

                                      @article{li2020unimo,
                                        title={Unimo: Towards unified-modal understanding and generation via cross-modal contrastive learning},
                                        author={Li, Wei and Gao, Can and Niu, Guocheng and Xiao, Xinyan and Liu, Hao and Liu, Jiachen and Wu, Hua and Wang, Haifeng},
                                        journal={arXiv preprint arXiv:2012.15409},
                                        url={https://https://arxiv.org/pdf/2012.15409.pdf},
                                        year={2020}
                                      }

                                      @inproceedings{chen2020simple,
                                        title={A simple framework for contrastive learning of visual representations},
                                        author={Chen, Ting and Kornblith, Simon and Norouzi, Mohammad and Hinton, Geoffrey},
                                        booktitle={International conference on machine learning},
                                        pages={1597--1607},
                                        year={2020},
                                        \url={https://proceedings.mlr.press/v119/chen20j/chen20j.pdf},
                                        organization={PMLR}
                                      }

                                      @article{clark2020electra,
                                        title={Electra: Pre-training text encoders as discriminators rather than generators},
                                        author={Clark, Kevin and Luong, Minh-Thang and Le, Quoc V and Manning, Christopher D},
                                        journal={arXiv preprint arXiv:2003.10555},
                                        url={https://openreview.net/pdf?id=r1xMH1BtvB},
                                        year={2020}
                                      }

                                      @inproceedings{chen2022people,
                                        title={How do people talk about images? A study on open-domain conversations with images.},
                                        author={Chen, Yi-Pei and Shimizu, Nobuyuki and Miyazaki, Takashi and Nakayama, Hideki},
                                        booktitle={Proceedings of the 2022 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies: Student Research Workshop},
                                        pages={156--162},
                                        url={https://aclanthology.org/2022.naacl-srw.20.pdf},
                                        year={2022}
                                      }
                                      @inproceedings{anuchitanukul2022surf,
                                        title={SURF: Semantic-level Unsupervised Reward Function for Machine Translation},
                                        author={Anuchitanukul, Atijit and Ive, Julia},
                                        booktitle={Proceedings of the 2022 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies},
                                        pages={4508--4522},
                                        url={https://aclanthology.org/2022.naacl-main.334.pdf},
                                        year={2022}
                                      }
                                      @inproceedings{song2021sentsim,
                                        title={SentSim: Crosslingual semantic evaluation of machine translation},
                                        author={Song, Yurun and Zhao, Junchen and Specia, Lucia},
                                        booktitle={Proceedings of the 2021 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies},
                                        pages={3143--3156},
                                        url={https://aclanthology.org/2021.naacl-main.252.pdf},
                                        year={2021}
                                      }
                                      
                                      @article{dosovitskiy2020image,
                                        title={An image is worth 16x16 words: Transformers for image recognition at scale},
                                        author={Dosovitskiy, Alexey and Beyer, Lucas and Kolesnikov, Alexander and Weissenborn, Dirk and Zhai, Xiaohua and Unterthiner, Thomas and Dehghani, Mostafa and Minderer, Matthias and Heigold, Georg and Gelly, Sylvain and others},
                                        journal={arXiv preprint arXiv:2010.11929},
                                        \url={https://arxiv.org/pdf/2010.11929.pdf},
                                        year={2020}
                                      }
                                      @inproceedings{esser2021taming,
                                        title={Taming transformers for high-resolution image synthesis},
                                        author={Esser, Patrick and Rombach, Robin and Ommer, Bjorn},
                                        booktitle={Proceedings of the IEEE/CVF conference on computer vision and pattern recognition},
                                        pages={12873--12883},
                                        url={https://openaccess.thecvf.com/content/CVPR2021/papers/Esser_Taming_Transformers_for_High-Resolution_Image_Synthesis_CVPR_2021_paper.pdf},
                                        year={2021}
                                      }
                                      @article{grootendorst2022bertopic,
                                        title={BERTopic: Neural topic modeling with a class-based TF-IDF procedure},
                                        author={Grootendorst, Maarten},
                                        journal={arXiv preprint arXiv:2203.05794},
                                        year={2022}
                                      }
                                      @article{li2022blip,
                                        title={Blip: Bootstrapping language-image pre-training for unified vision-language understanding and generation},
                                        author={Li, Junnan and Li, Dongxu and Xiong, Caiming and Hoi, Steven},
                                        journal={arXiv preprint arXiv:2201.12086},
                                        url={https://arxiv.org/pdf/2201.12086.pdf},
                                        year={2022}
                                      }
                                      @article{xu2022progressive,
                                        title={Progressive Class Semantic Matching for Semi-supervised Text Classification},
                                        author={Xu, Hai-Ming and Liu, Lingqiao and Abbasnejad, Ehsan},
                                        journal={arXiv preprint arXiv:2205.10189},
                                        url={https://arxiv.org/pdf/2205.10189.pdf},
                                        year={2022}
                                      }
                                      @article{devlin2018bert,
                                        title={Bert: Pre-training of deep bidirectional transformers for language understanding},
                                        author={Devlin, Jacob and Chang, Ming-Wei and Lee, Kenton and Toutanova, Kristina},
                                        journal={arXiv preprint arXiv:1810.04805},
                                        url ={https://arxiv.org/pdf/1810.04805.pdf},
                                        year={2018}
                                      }
                                      @article{zhao2021topic,
                                        title={Topic modelling meets deep neural networks: A survey},
                                        author={Zhao, He and Phung, Dinh and Huynh, Viet and Jin, Yuan and Du, Lan and Buntine, Wray},
                                        journal={arXiv preprint arXiv:2103.00498},
                                        url={https://www.ijcai.org/proceedings/2021/0638.pdf},
                                        year={2021}
                                      }
                                </script>