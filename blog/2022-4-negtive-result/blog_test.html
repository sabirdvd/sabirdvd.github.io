<!DOCTYPE html><html lang="en"><head><meta charset="utf-8"><meta name="viewport" content="width=1200"><meta http-equiv="X-UA-Compatible" content="IE=Edge,chrome=1"><script>
window.addEventListener('WebComponentsReady', function() {
  console.warn('WebComponentsReady');
  const loaderTag = document.createElement('script');
  loaderTag.src = 'https://distill.pub/template.v2.js';
  document.head.insertBefore(loaderTag, document.head.firstChild);
});
</script><script src="https://cdnjs.cloudflare.com/ajax/libs/webcomponentsjs/1.0.17/webcomponents-loader.js"></script>


  <style id="distill-prerendered-styles" type="text/css">/*
 * Copyright 2018 The Distill Template Authors
 *
 * Licensed under the Apache License, Version 2.0 (the "License");
 * you may not use this file except in compliance with the License.
 * You may obtain a copy of the License at
 *
 *      http://www.apache.org/licenses/LICENSE-2.0
 *
 * Unless required by applicable law or agreed to in writing, software
 * distributed under the License is distributed on an "AS IS" BASIS,
 * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
 * See the License for the specific language governing permissions and
 * limitations under the License.
 */

html {
  font-size: 14px;
	line-height: 1.6em;
  /* font-family: "Libre Franklin", "Helvetica Neue", sans-serif; */
  font-family: -apple-system, BlinkMacSystemFont, "Segoe UI", Roboto, Oxygen, Ubuntu, Cantarell, "Fira Sans", "Droid Sans", "Helvetica Neue", Arial, sans-serif;
  /*, "Apple Color Emoji", "Segoe UI Emoji", "Segoe UI Symbol";*/
  text-size-adjust: 100%;
  -ms-text-size-adjust: 100%;
  -webkit-text-size-adjust: 100%;
}

@media(min-width: 768px) {
  html {
    font-size: 16px;
  }
}

body {
  margin: 0;
}

a {
  color: #004276;
}

figure {
  margin: 0;
}

table {
	border-collapse: collapse;
	border-spacing: 0;
}

table th {
	text-align: left;
}

table thead {
  border-bottom: 1px solid rgba(0, 0, 0, 0.05);
}

table thead th {
  padding-bottom: 0.5em;
}

table tbody :first-child td {
  padding-top: 0.5em;
}

pre {
  overflow: auto;
  max-width: 100%;
}

p {
  margin-top: 0;
  margin-bottom: 1em;
}

sup, sub {
  vertical-align: baseline;
  position: relative;
  top: -0.4em;
  line-height: 1em;
}

sub {
  top: 0.4em;
}

.kicker,
.marker {
  font-size: 15px;
  font-weight: 600;
  color: rgba(0, 0, 0, 0.5);
}


/* Headline */

@media(min-width: 1024px) {
  d-title h1 span {
    display: block;
  }
}

/* Figure */

figure {
  position: relative;
  margin-bottom: 2.5em;
  margin-top: 1.5em;
}

figcaption+figure {

}

figure img {
  width: 100%;
}

figure svg text,
figure svg tspan {
}

figcaption,
.figcaption {
  color: rgba(0, 0, 0, 0.6);
  font-size: 12px;
  line-height: 1.5em;
}

@media(min-width: 1024px) {
figcaption,
.figcaption {
    font-size: 13px;
  }
}

figure.external img {
  background: white;
  border: 1px solid rgba(0, 0, 0, 0.1);
  box-shadow: 0 1px 8px rgba(0, 0, 0, 0.1);
  padding: 18px;
  box-sizing: border-box;
}

figcaption a {
  color: rgba(0, 0, 0, 0.6);
}

figcaption b,
figcaption strong, {
  font-weight: 600;
  color: rgba(0, 0, 0, 1.0);
}
/*
 * Copyright 2018 The Distill Template Authors
 *
 * Licensed under the Apache License, Version 2.0 (the "License");
 * you may not use this file except in compliance with the License.
 * You may obtain a copy of the License at
 *
 *      http://www.apache.org/licenses/LICENSE-2.0
 *
 * Unless required by applicable law or agreed to in writing, software
 * distributed under the License is distributed on an "AS IS" BASIS,
 * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
 * See the License for the specific language governing permissions and
 * limitations under the License.
 */

@supports not (display: grid) {
  .base-grid,
  distill-header,
  d-title,
  d-abstract,
  d-article,
  d-appendix,
  distill-appendix,
  d-byline,
  d-footnote-list,
  d-citation-list,
  distill-footer {
    display: block;
    padding: 8px;
  }
}

.base-grid,
distill-header,
d-title,
d-abstract,
d-article,
d-appendix,
distill-appendix,
d-byline,
d-footnote-list,
d-citation-list,
distill-footer {
  display: grid;
  justify-items: stretch;
  grid-template-columns: [screen-start] 8px [page-start kicker-start text-start gutter-start middle-start] 1fr 1fr 1fr 1fr 1fr 1fr 1fr 1fr [text-end page-end gutter-end kicker-end middle-end] 8px [screen-end];
  grid-column-gap: 8px;
}

.grid {
  display: grid;
  grid-column-gap: 8px;
}

@media(min-width: 768px) {
  .base-grid,
  distill-header,
  d-title,
  d-abstract,
  d-article,
  d-appendix,
  distill-appendix,
  d-byline,
  d-footnote-list,
  d-citation-list,
  distill-footer {
    grid-template-columns: [screen-start] 1fr [page-start kicker-start middle-start text-start] 45px 45px 45px 45px 45px 45px 45px 45px [ kicker-end text-end gutter-start] 45px [middle-end] 45px [page-end gutter-end] 1fr [screen-end];
    grid-column-gap: 16px;
  }

  .grid {
    grid-column-gap: 16px;
  }
}

@media(min-width: 1000px) {
  .base-grid,
  distill-header,
  d-title,
  d-abstract,
  d-article,
  d-appendix,
  distill-appendix,
  d-byline,
  d-footnote-list,
  d-citation-list,
  distill-footer {
    grid-template-columns: [screen-start] 1fr [page-start kicker-start] 50px [middle-start] 50px [text-start kicker-end] 50px 50px 50px 50px 50px 50px 50px 50px [text-end gutter-start] 50px [middle-end] 50px [page-end gutter-end] 1fr [screen-end];
    grid-column-gap: 16px;
  }

  .grid {
    grid-column-gap: 16px;
  }
}

@media(min-width: 1180px) {
  .base-grid,
  distill-header,
  d-title,
  d-abstract,
  d-article,
  d-appendix,
  distill-appendix,
  d-byline,
  d-footnote-list,
  d-citation-list,
  distill-footer {
    grid-template-columns: [screen-start] 1fr [page-start kicker-start] 60px [middle-start] 60px [text-start kicker-end] 60px 60px 60px 60px 60px 60px 60px 60px [text-end gutter-start] 60px [middle-end] 60px [page-end gutter-end] 1fr [screen-end];
    grid-column-gap: 32px;
  }

  .grid {
    grid-column-gap: 32px;
  }
}




.base-grid {
  grid-column: screen;
}

/* .l-body,
d-article > *  {
  grid-column: text;
}

.l-page,
d-title > *,
d-figure {
  grid-column: page;
} */

.l-gutter {
  grid-column: gutter;
}

.l-text,
.l-body {
  grid-column: text;
}

.l-page {
  grid-column: page;
}

.l-body-outset {
  grid-column: middle;
}

.l-page-outset {
  grid-column: page;
}

.l-screen {
  grid-column: screen;
}

.l-screen-inset {
  grid-column: screen;
  padding-left: 16px;
  padding-left: 16px;
}


/* Aside */

d-article aside {
  grid-column: gutter;
  font-size: 12px;
  line-height: 1.6em;
  color: rgba(0, 0, 0, 0.6)
}

@media(min-width: 768px) {
  aside {
    grid-column: gutter;
  }

  .side {
    grid-column: gutter;
  }
}
/*
 * Copyright 2018 The Distill Template Authors
 *
 * Licensed under the Apache License, Version 2.0 (the "License");
 * you may not use this file except in compliance with the License.
 * You may obtain a copy of the License at
 *
 *      http://www.apache.org/licenses/LICENSE-2.0
 *
 * Unless required by applicable law or agreed to in writing, software
 * distributed under the License is distributed on an "AS IS" BASIS,
 * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
 * See the License for the specific language governing permissions and
 * limitations under the License.
 */

d-title {
  padding: 2rem 0 1.5rem;
  contain: layout style;
  overflow-x: hidden;
}

@media(min-width: 768px) {
  d-title {
    padding: 4rem 0 1.5rem;
  }
}

d-title h1 {
  grid-column: text;
  font-size: 40px;
  font-weight: 700;
  line-height: 1.1em;
  margin: 0 0 0.5rem;
}

@media(min-width: 768px) {
  d-title h1 {
    font-size: 50px;
  }
}

d-title p {
  font-weight: 300;
  font-size: 1.2rem;
  line-height: 1.55em;
  grid-column: text;
}

d-title .status {
  margin-top: 0px;
  font-size: 12px;
  color: #009688;
  opacity: 0.8;
  grid-column: kicker;
}

d-title .status span {
  line-height: 1;
  display: inline-block;
  padding: 6px 0;
  border-bottom: 1px solid #80cbc4;
  font-size: 11px;
  text-transform: uppercase;
}
/*
 * Copyright 2018 The Distill Template Authors
 *
 * Licensed under the Apache License, Version 2.0 (the "License");
 * you may not use this file except in compliance with the License.
 * You may obtain a copy of the License at
 *
 *      http://www.apache.org/licenses/LICENSE-2.0
 *
 * Unless required by applicable law or agreed to in writing, software
 * distributed under the License is distributed on an "AS IS" BASIS,
 * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
 * See the License for the specific language governing permissions and
 * limitations under the License.
 */

d-byline {
  contain: style;
  overflow: hidden;
  border-top: 1px solid rgba(0, 0, 0, 0.1);
  font-size: 0.8rem;
  line-height: 1.8em;
  padding: 1.5rem 0;
  min-height: 1.8em;
}


d-byline .byline {
  grid-template-columns: 1fr 1fr;
  grid-column: text;
}

@media(min-width: 768px) {
  d-byline .byline {
    grid-template-columns: 1fr 1fr 1fr 1fr;
  }
}

d-byline .authors-affiliations {
  grid-column-end: span 2;
  grid-template-columns: 1fr 1fr;
  margin-bottom: 1em;
}

@media(min-width: 768px) {
  d-byline .authors-affiliations {
    margin-bottom: 0;
  }
}

d-byline h3 {
  font-size: 0.6rem;
  font-weight: 400;
  color: rgba(0, 0, 0, 0.5);
  margin: 0;
  text-transform: uppercase;
}

d-byline p {
  margin: 0;
}

d-byline a,
d-article d-byline a {
  color: rgba(0, 0, 0, 0.8);
  text-decoration: none;
  border-bottom: none;
}

d-article d-byline a:hover {
  text-decoration: underline;
  border-bottom: none;
}

d-byline p.author {
  font-weight: 500;
}

d-byline .affiliations {

}
/*
 * Copyright 2018 The Distill Template Authors
 *
 * Licensed under the Apache License, Version 2.0 (the "License");
 * you may not use this file except in compliance with the License.
 * You may obtain a copy of the License at
 *
 *      http://www.apache.org/licenses/LICENSE-2.0
 *
 * Unless required by applicable law or agreed to in writing, software
 * distributed under the License is distributed on an "AS IS" BASIS,
 * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
 * See the License for the specific language governing permissions and
 * limitations under the License.
 */

d-article {
  contain: layout style;
  overflow-x: hidden;
  border-top: 1px solid rgba(0, 0, 0, 0.1);
  padding-top: 2rem;
  color: rgba(0, 0, 0, 0.8);
}

d-article > * {
  grid-column: text;
}

@media(min-width: 768px) {
  d-article {
    font-size: 16px;
  }
}

@media(min-width: 1024px) {
  d-article {
    font-size: 1.06rem;
    line-height: 1.7em;
  }
}


/* H2 */


d-article .marker {
  text-decoration: none;
  border: none;
  counter-reset: section;
  grid-column: kicker;
  line-height: 1.7em;
}

d-article .marker:hover {
  border: none;
}

d-article .marker span {
  padding: 0 3px 4px;
  border-bottom: 1px solid rgba(0, 0, 0, 0.2);
  position: relative;
  top: 4px;
}

d-article .marker:hover span {
  color: rgba(0, 0, 0, 0.7);
  border-bottom: 1px solid rgba(0, 0, 0, 0.7);
}

d-article h2 {
  font-weight: 600;
  font-size: 24px;
  line-height: 1.25em;
  margin: 2rem 0 1.5rem 0;
  border-bottom: 1px solid rgba(0, 0, 0, 0.1);
  padding-bottom: 1rem;
}

@media(min-width: 1024px) {
  d-article h2 {
    font-size: 36px;
  }
}

/* H3 */

d-article h3 {
  font-weight: 700;
  font-size: 18px;
  line-height: 1.4em;
  margin-bottom: 1em;
  margin-top: 2em;
}

@media(min-width: 1024px) {
  d-article h3 {
    font-size: 20px;
  }
}

/* H4 */

d-article h4 {
  font-weight: 600;
  text-transform: uppercase;
  font-size: 14px;
  line-height: 1.4em;
}

d-article a {
  color: inherit;
}

d-article p,
d-article ul,
d-article ol,
d-article blockquote {
  margin-top: 0;
  margin-bottom: 1em;
  margin-left: 0;
  margin-right: 0;
}

d-article blockquote {
  border-left: 2px solid rgba(0, 0, 0, 0.2);
  padding-left: 2em;
  font-style: italic;
  color: rgba(0, 0, 0, 0.6);
}

d-article a {
  border-bottom: 1px solid rgba(0, 0, 0, 0.4);
  text-decoration: none;
}

d-article a:hover {
  border-bottom: 1px solid rgba(0, 0, 0, 0.8);
}

d-article .link {
  text-decoration: underline;
  cursor: pointer;
}

d-article ul,
d-article ol {
  padding-left: 24px;
}

d-article li {
  margin-bottom: 1em;
  margin-left: 0;
  padding-left: 0;
}

d-article li:last-child {
  margin-bottom: 0;
}

d-article pre {
  font-size: 14px;
  margin-bottom: 20px;
}

d-article hr {
  grid-column: screen;
  width: 100%;
  border: none;
  border-bottom: 1px solid rgba(0, 0, 0, 0.1);
  margin-top: 60px;
  margin-bottom: 60px;
}

d-article section {
  margin-top: 60px;
  margin-bottom: 60px;
}

d-article span.equation-mimic {
  font-family: georgia;
  font-size: 115%;
  font-style: italic;
}

d-article > d-code,
d-article section > d-code  {
  display: block;
}

d-article > d-math[block],
d-article section > d-math[block]  {
  display: block;
}

@media (max-width: 768px) {
  d-article > d-code,
  d-article section > d-code,
  d-article > d-math[block],
  d-article section > d-math[block] {
      overflow-x: scroll;
      -ms-overflow-style: none;  // IE 10+
      overflow: -moz-scrollbars-none;  // Firefox
  }

  d-article > d-code::-webkit-scrollbar,
  d-article section > d-code::-webkit-scrollbar,
  d-article > d-math[block]::-webkit-scrollbar,
  d-article section > d-math[block]::-webkit-scrollbar {
    display: none;  // Safari and Chrome
  }
}

d-article .citation {
  color: #668;
  cursor: pointer;
}

d-include {
  width: auto;
  display: block;
}

d-figure {
  contain: layout style;
}

/* KaTeX */

.katex, .katex-prerendered {
  contain: style;
  display: inline-block;
}

/* Tables */

d-article table {
  border-collapse: collapse;
  margin-bottom: 1.5rem;
  border-bottom: 1px solid rgba(0, 0, 0, 0.2);
}

d-article table th {
  border-bottom: 1px solid rgba(0, 0, 0, 0.2);
}

d-article table td {
  border-bottom: 1px solid rgba(0, 0, 0, 0.05);
}

d-article table tr:last-of-type td {
  border-bottom: none;
}

d-article table th,
d-article table td {
  font-size: 15px;
  padding: 2px 8px;
}

d-article table tbody :first-child td {
  padding-top: 2px;
}
/*
 * Copyright 2018 The Distill Template Authors
 *
 * Licensed under the Apache License, Version 2.0 (the "License");
 * you may not use this file except in compliance with the License.
 * You may obtain a copy of the License at
 *
 *      http://www.apache.org/licenses/LICENSE-2.0
 *
 * Unless required by applicable law or agreed to in writing, software
 * distributed under the License is distributed on an "AS IS" BASIS,
 * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
 * See the License for the specific language governing permissions and
 * limitations under the License.
 */

span.katex-display {
  text-align: left;
  padding: 8px 0 8px 0;
  margin: 0.5em 0 0.5em 1em;
}

span.katex {
  -webkit-font-smoothing: antialiased;
  color: rgba(0, 0, 0, 0.8);
  font-size: 1.18em;
}
/*
 * Copyright 2018 The Distill Template Authors
 *
 * Licensed under the Apache License, Version 2.0 (the "License");
 * you may not use this file except in compliance with the License.
 * You may obtain a copy of the License at
 *
 *      http://www.apache.org/licenses/LICENSE-2.0
 *
 * Unless required by applicable law or agreed to in writing, software
 * distributed under the License is distributed on an "AS IS" BASIS,
 * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
 * See the License for the specific language governing permissions and
 * limitations under the License.
 */

@media print {

  @page {
    size: 8in 11in;
    @bottom-right {
      content: counter(page) " of " counter(pages);
    }
  }

  html {
    /* no general margins -- CSS Grid takes care of those */
  }

  p, code {
    page-break-inside: avoid;
  }

  h2, h3 {
    page-break-after: avoid;
  }

  d-header {
    visibility: hidden;
  }

  d-footer {
    display: none!important;
  }

}
</style>
  <style>
    .subgrid {
  grid-column: screen;
  display: grid;
  grid-template-columns: inherit;
  grid-template-rows: inherit;
  grid-column-gap: inherit;
  grid-row-gap: inherit;
}

d-figure.base-grid {
  grid-column: screen;
  background: hsl(0, 0%, 97%);
  padding: 20px 0;
  border-top: 1px solid rgba(0, 0, 0, 0.1);
  border-bottom: 1px solid rgba(0, 0, 0, 0.1);
}

d-figure {
  margin-bottom: 1em;
  position: relative;
}

d-figure > figure {
  margin-top: 0;
  margin-bottom: 0;
}

.shaded-figure {
  background-color: hsl(0, 0%, 97%);
  border-top: 1px solid hsla(0, 0%, 0%, 0.1);
  border-bottom: 1px solid hsla(0, 0%, 0%, 0.1);
  padding: 30px 0;
}

.pointer {
  position: absolute;
  width: 26px;
  height: 26px;
  top: 26px;
  left: -48px;
}

.grayscale-light {
  filter: gray;
  -webkit-filter: grayscale(1) contrast(0.2) brightness(1.6);
  filter: grayscale(1) contrast(0.2) brightness(1.6);
}

.grayscale-dark {
  filter: gray;
  -webkit-filter: grayscale(1) brightness(0.75);
  filter: grayscale(1) brightness(0.75);
}

.striped {
  background: repeating-linear-gradient(135deg, lightgray 0px, whitesmoke 10px, lightgray 20px);
}

body d-title {
  overflow-x: auto;
}

body d-article {
  overflow: visible;
}

d-title h1, d-title p, d-title figure {
  grid-column: page;
}

b {
  font-weight: bold;
}

label, button {
  cursor: pointer;
}

button {
  border-radius: 0.25em;
}

#coinrun-objects {
  grid-column: page;
}

#coinrun-objects .coinrun-objects-row-images td {
  white-space: nowrap;
}

#coinrun-objects img {
  width: auto;
  border: 1px solid gray;
}

#coinrun-objects .coinrun-object-single {
  width: 100%;
}

#coinrun-objects .coinrun-object-double {
  width: 48%;
}

#coinrun-objects .coinrun-objects-row-text td {
  position: relative;
  height: 9em;
}

#coinrun-objects .coinrun-objects-row-text figcaption {
  position: absolute;
  top: 1em;
}

#coinrun-actions .coinrun-action {
  font-weight: bold;
  padding: 0.2em;
  border: 1px solid gray;
  border-radius: 0.25em;
}

.play-pause-button {
  height: 1.3em;
  width: 1.3em;
  font-size: 3em;
  line-height: 0em;
}

.interface-failure-step-button {
  font-size: 1em;
  margin: 0em 0.1em;
}

#interface-failure-position {
  margin: 0em 1em;
}

.matplotlib-svg text, .matplotlib-svg tspan {
  font-family: inherit !important;
}

#model-editing-levels label {
  white-space: nowrap;
}

#model-editing-levels video {
  width: 100%;
}

#feature-vis-traditional td {
  padding: 8px;
}

#feature-vis-traditional img {
  display: inline-block;
  width: 128px;
  border: 1px solid gray;
}

#feature-vis-dataset {
  margin: 0 auto;
  text-align: center;
  max-width: 640px;
}

.feature-vis-dataset-item {
  display: inline-block;
  vertical-align: top;
  width: 136px;
}

#feature-vis-dataset img {
  display: block;
  width: 128px;
  border: 1px solid gray;
}

.feature-vis-dataset-text {
  margin-top: 0.2em;
  margin-bottom: 0.5em;
  font-size: 0.75em;
  line-height: 1.5em;
  font-style: italic;
}

#feature-vis-spatial {
  margin: 0.5em auto;
  height: 512px;
  width: 512px;
}

#feature-vis-spatial img {
  border: 1px solid gray;
}

#hero th, #hero td {
  vertical-align: top;
}

#hero #hero-annotations {
  position: relative;
  margin-top: 0.5em;
  font-size: 0.8em;
  color: gray;
}

#hero #hero-annotations > div {
  position: absolute;
  text-align: center;
  line-height: 1.1em;
  width: 100px;
}

#hero .hero-annotation-dot {
  display: inline-block;
  height: 16px;
  width: 16px;
  border-radius: 20%;
  vertical-align: top;
}

#hero .hero-annotation-image {
  height: auto;
  width: auto;
  margin: 1px;
  border: 1px solid gray;
  border-radius: 50%;
  cursor: pointer;
}

#hero .hero-annotation-line-vertical-outer {
  position: absolute;
  top: -142px;
  left: 47px;
  width: 6px;
  height: 137px;
  background-color: white;
}

#hero .hero-annotation-line-vertical-inner {
  position: absolute;
  z-index: 2;
  left: 2px;
  height: 143px;
  width: 2px;
  background-color: black;
}

#hero .hero-annotation-line-horizontal-outer {
  position: absolute;
  top: -144px;
  height: 6px;
  background-color: white;
}

#hero .hero-annotation-line-horizontal-inner {
  position: absolute;
  z-index: 2;
  top: 2px;
  height: 2px;
  background-color: black;
}

#attribution-demo td {
  width: 33.33%;
}

#attribution-demo td {
  min-width: 256px;
  max-width: 288px;
}

#hero td {
  min-width: 256px;
  max-width: 320px;
  padding: 2px 16px 2px 0px;
}

#attribution-demo .attribution-outer, #hero .hero-outer {
  position: relative;
  border: 1px solid gray;
  height: 0px;
  padding-bottom: 100%;
  width: 100%;
}

#attribution-demo .attribution-inner, #hero .hero-inner {
  position: absolute;
  height: 0px;
  padding-bottom: 100%;
  width: 100%;
}

#attribution-demo .attribution-image, #hero .hero-image {
  height: 0px;
  padding-bottom: 100%;
  width: 100%;
  background-size: 100% 100%;
}

#attribution-demo .attribution-legend-item {
  display: inline-block;
  padding: 0.5em;
  background: white;
  border: 1px solid white;
  border-radius: 0.25em;
  cursor: pointer;
  overflow: hidden;
}

#attribution-demo .attribution-legend-item:hover {
  background: whitesmoke;
  border: 1px solid gray;
}

#attribution-demo .attribution-legend-outer {
  position: relative;
  height: 192px;
  width: 128px;
}

#attribution-demo .attribution-legend-dot {
  position: absolute;
  top: 140.8px;
  left: 0px;
  height: 32px;
  width: 32px;
  border-radius: 20%;
}

#attribution-demo .attribution-legend-inner {
  position: absolute;
  height: 128px;
  width: 128px;
  top: 0px;
  left: 0px;
  border: 1px solid gray;
}

#attribution-demo .attribution-legend-label {
  position: absolute;
  height: 64px;
  width: 96px;
  top: 140.8px;
  left: 42.67px;
  font-size: 0.9em;
  line-height: 1.2em;
  font-style: italic;
  z-index: 1;
  text-align: left;
}

#coinrun-objects img, #feature-vis-traditional img, #feature-vis-dataset img, #feature-vis-spatial img, #attribution-demo .attribution-image, #hero .hero-image {
   image-rendering: optimizeSpeed;
   image-rendering: -moz-crisp-edges;
   image-rendering: -o-crisp-edges;
   image-rendering: -webkit-optimize-contrast;
   image-rendering: optimize-contrast;
   image-rendering: crisp-edges;
   image-rendering: pixelated;
   -ms-interpolation-mode: nearest-neighbor;
}

.architecture-list {
  margin-top: 0em;
}

.architecture-list li {
  margin-bottom: 0em;
}

/* table of contents */

@media (max-width: 1000px) {
  d-contents {
    justify-self: start;
    align-self: start;
    grid-column-start: 2;
    grid-column-end: 6;
    padding-bottom: 0.5em;
    margin-bottom: 1em;
    padding-left: 0.25em;
    border-bottom: 1px solid rgba(0, 0, 0, 0.1);
    border-bottom-width: 1px;
    border-bottom-style: solid;
    border-bottom-color: rgba(0, 0, 0, 0.1);
  }
}

@media (min-width: 1000px) {
  d-contents {
    align-self: start;
    grid-column-start: 1;
    grid-column-end: 4;
    justify-self: end;
    padding-right: 3em;
    padding-left: 2em;
    border-right: 1px solid rgba(0, 0, 0, 0.1);
    border-right-width: 1px;
    border-right-style: solid;
    border-right-color: rgba(0, 0, 0, 0.1);
  }
}

@media (min-width: 1180px) {
  d-contents {
    grid-column-start: 1;
    grid-column-end: 4;
    justify-self: end;
    padding-right: 3em;
    padding-left: 2em;
    border-right: 1px solid rgba(0, 0, 0, 0.1);
    border-right-width: 1px;
    border-right-style: solid;
    border-right-color: rgba(0, 0, 0, 0.1);
  }
}

d-contents nav h3 {
  margin-top: 0;
  margin-bottom: 1em;
}

d-contents nav a {
  color: rgba(0, 0, 0, 0.8);
  border-bottom: none;
  text-decoration: none;
}

d-contents li {
  list-style-type: none;
}

d-contents ul {
  padding-left: 1em;
}

d-contents nav ul li {
  margin-bottom: 0.25em;
}

d-contents nav a:hover {
  text-decoration: underline solid rgba(0, 0, 0, 0.6);
}

d-contents nav ul {
  margin-top: 0;
  margin-bottom: 6px;
}

d-contents nav > div {
  display: block;
  outline: none;
  margin-bottom: 0.5em;
}

d-contents nav > div > a {
  font-size: 13px;
  font-weight: 600;
}

d-contents nav > div > a:hover, d-contents nav > ul > li > a:hover {
  text-decoration: none;
}

  </style>
<link rel="stylesheet" href="https://distill.pub/third-party/katex/katex.min.css" crossorigin="anonymous">

    <link rel="icon" type="image/png" href="data:image/png;base64,iVBORw0KGgoAAAANSUhEUgAAAEAAAABACAYAAACqaXHeAAAAGXRFWHRTb2Z0d2FyZQBBZG9iZSBJbWFnZVJlYWR5ccllPAAAA99JREFUeNrsG4t1ozDMzQSM4A2ODUonKBucN2hugtIJ6E1AboLcBiQTkJsANiAb9OCd/OpzMWBJBl5TvaeXPiiyJetry0J8wW3D3QpjRh3GjneXDq+fSQA9s2mH9x3KDhN4foJfCb8N/Jrv+2fnDn8vLRQOplWHVYdvHZYdZsBcZP1vBmh/n8DzEmhUQDPaOuP9pFuY+JwJHwHnCLQE2tnWBGEyXozY9xCUgHMhhjE2I4heVWtgIkZ83wL6Qgxj1obfWBxymPwe+b00BCCRNPbwfb60yleAkkBHGT5AEehIYz7eJrFDMF9CvH4wwhcGHiHMneFvLDQwlwvMLQq58trRcYBWfYn0A0OgHWQUSu25mE+BnoYKnnEJoeIWAifzOv7vLWd2ZKRfWAIme3tOiUaQ3UnLkb0xj1FxRIeEGKaGIHOs9nEgLaaA9i0JRYo1Ic67wJW86KSKE/ZAM8KuVMk8ITVhmxUxJ3Cl2xlm9Vtkeju1+mpCQNxaEGNCY8bs9X2YqwNoQeGjBWut/ma0QAWy/TqAsHx9wSya3I5IRxOfTC+leG+kA/4vSeEcGBtNUN6byhu3+keEZCQJUNh8MAO7HL6H8pQLnsW/Hd4T4lv93TPjfM7A46iEEqbB5EDOvwYNW6tGNZzT/o+CZ6sqZ6wUtR/wf7mi/VL8iNciT6rHih48Y55b4nKCHJCCzb4y0nwFmin3ZEMIoLfZF8F7nncFmvnWBaBj7CGAYA/WGJsUwHdYqVDwAmNsUgAx4CGgAA7GOOxADYOFWOaIKifuVYzmOpREqA21Mo7aPsgiY1PhOMAmxtR+AUbYH3Id2wc0SAFIQTsn9IUGWR8k9jx3vtXSiAacFxTAGakBk9UudkNECd6jLe+6HrshshvIuC6IlLMRy7er+JpcKma24SlE4cFZSZJDGVVrsNvitQhQrDhW0jfiOLfFd47C42eHT56D/BK0To+58Ahj+cAT8HT1UWlfLZCCd/uKawzU0Rh2EyIX/Icqth3niG8ybNroezwe6khdCNxRN+l4XGdOLVLlOOt2hTRJlr1ETIuMAltVTMz70mJrkdGAaZLSmnBEqmAE32JCMmuTlCnRgsBENtOUpHhvvsYIL0ibnBkaC6QvKcR7738GKp0AKnim7xgUSNv1bpS8QwhBt8r+EP47v/oyRK/S34yJ9nT+AN0Tkm4OdB9E4BsmXM3SnMlRFUrtp6IDpV2eKzdYvF3etm3KhQksbOLChGkSmcBdmcEwvqkrMy5BzL00NZeu3qPYJOOuCc+5NjcWKXQxFvTa3NoXJ4d8in7fiAUuTt781dkvuHX4K8AA2Usy7yNKLy0AAAAASUVORK5CYII=
">
    <link href="/rss.xml" rel="alternate" type="application/rss+xml" title="Articles from Distill">

    <title>Understanding RL Vision</title>

    <link rel="canonical" href="https://distill.pub/2020/understanding-rl-vision">

    <!--  https://schema.org/Article -->
    <meta property="description" itemprop="description" content="With diverse environments, we can analyze, diagnose and edit deep reinforcement learning models using attribution.">
    <meta property="article:published" itemprop="datePublished" content="2020-11-17">
    <meta property="article:created" itemprop="dateCreated" content="2020-11-17">

    <meta property="article:modified" itemprop="dateModified" content="2021-02-12T02:05:35.000Z">

    <meta property="article:author" content="Jacob Hilton">
    <meta property="article:author" content="Nick Cammarata">
    <meta property="article:author" content="Shan Carter">
    <meta property="article:author" content="Gabriel Goh">
    <meta property="article:author" content="Chris Olah">
    <!--  https://developers.facebook.com/docs/sharing/webmasters#markup -->
    <meta property="og:type" content="article">
    <meta property="og:title" content="Understanding RL Vision">
    <meta property="og:description" content="With diverse environments, we can analyze, diagnose and edit deep reinforcement learning models using attribution.">
    <meta property="og:url" content="https://distill.pub/2020/understanding-rl-vision">
    <meta property="og:image" content="https://distill.pub/2020/understanding-rl-vision/thumbnail.jpg">
    <meta property="og:locale" content="en_US">
    <meta property="og:site_name" content="Distill">

    <!--  https://dev.twitter.com/cards/types/summary -->
    <meta name="twitter:card" content="summary_large_image">
    <meta name="twitter:title" content="Understanding RL Vision">
    <meta name="twitter:description" content="With diverse environments, we can analyze, diagnose and edit deep reinforcement learning models using attribution.">
    <meta name="twitter:url" content="https://distill.pub/2020/understanding-rl-vision">
    <meta name="twitter:image" content="https://distill.pub/2020/understanding-rl-vision/thumbnail.jpg">
    <meta name="twitter:image:width" content="560">
    <meta name="twitter:image:height" content="295">

      <!--  https://scholar.google.com/intl/en/scholar/inclusion.html#indexing -->
    <meta name="citation_title" content="Understanding RL Vision">
    <meta name="citation_fulltext_html_url" content="https://distill.pub/2020/understanding-rl-vision">
    <meta name="citation_volume" content="5">
    <meta name="citation_issue" content="11">
    <meta name="citation_firstpage" content="e29">
    <meta name="citation_doi" content="10.23915/distill.00029">
    <meta name="citation_journal_title" content="Distill">
    <meta name="citation_journal_abbrev" content="Distill">
    <meta name="citation_issn" content="2476-0757">
    <meta name="citation_fulltext_world_readable" content="">
    <meta name="citation_online_date" content="2020/11/17">
    <meta name="citation_publication_date" content="2020/11/17">
    <meta name="citation_author" content="Hilton, Jacob">
    <meta name="citation_author_institution" content="OpenAI">
    <meta name="citation_author" content="Cammarata, Nick">
    <meta name="citation_author_institution" content="OpenAI">
    <meta name="citation_author" content="Carter, Shan">
    <meta name="citation_author_institution" content="Observable">
    <meta name="citation_author" content="Goh, Gabriel">
    <meta name="citation_author_institution" content="OpenAI">
    <meta name="citation_author" content="Olah, Chris">
    <meta name="citation_author_institution" content="OpenAI">
    <meta name="citation_reference" content="citation_title=Quantifying generalization in reinforcement learning;citation_author=Karl Cobbe;citation_author=Oleg Klimov;citation_author=Chris Hesse;citation_author=Taehoon Kim;citation_author=John Schulman;citation_publication_date=2018;citation_arxiv_id=1812.02341;">
    <meta name="citation_reference" content="citation_title=Deep inside convolutional networks: Visualising image classification models and saliency maps;citation_author=Karen Simonyan;citation_author=Andrea Vedaldi;citation_author=Andrew Zisserman;citation_publication_date=2013;citation_arxiv_id=1312.6034;">
    <meta name="citation_reference" content="citation_title=Visualizing and understanding convolutional networks;citation_author=Matthew D Zeiler;citation_author=Rob Fergus;citation_publication_date=2014;citation_arxiv_id=1311.2901;">
    <meta name="citation_reference" content="citation_title=Striving for simplicity: The all convolutional net;citation_author=Jost Tobias Springenberg;citation_author=Alexey Dosovitskiy;citation_author=Thomas Brox;citation_author=Martin Riedmiller;citation_publication_date=2014;citation_arxiv_id=1412.6806;">
    <meta name="citation_reference" content="citation_title=Grad-CAM: Visual explanations from deep networks via gradient-based localization;citation_author=Ramprasaath R Selvaraju;citation_author=Michael Cogswell;citation_author=Abhishek Das;citation_author=Ramakrishna Vedantam;citation_author=Devi Parikh;citation_author=Dhruv Batra;citation_publication_date=2017;">
    <meta name="citation_reference" content="citation_title=Interpretable explanations of black boxes by meaningful perturbation;citation_author=Ruth C Fong;citation_author=Andrea Vedaldi;citation_publication_date=2017;">
    <meta name="citation_reference" content="citation_title=PatternNet and PatternLRP--Improving the interpretability of neural networks;citation_author=Pieter-Jan Kindermans;citation_author=Kristof T Schutt;citation_author=Maximilian Alber;citation_author=Klaus-Robert Muller;citation_author=Sven Dahne;citation_publication_date=2017;citation_arxiv_id=1705.05598;">
    <meta name="citation_reference" content="citation_title=The (un)reliability of saliency methods;citation_author=Pieter-Jan Kindermans;citation_author=Sara Hooker;citation_author=Julius Adebayo;citation_author=Maximilian Alber;citation_author=Kristof T Schutt;citation_author=Sven Dahne;citation_author=Dumitru Erhan;citation_author=Been Kim;citation_publication_date=2019;citation_arxiv_id=1711.00867;">
    <meta name="citation_reference" content="citation_title=Axiomatic attribution for deep networks;citation_author=Mukund Sundararajan;citation_author=Ankur Taly;citation_author=Qiqi Yan;citation_publication_date=2017;citation_arxiv_id=1703.01365;">
    <meta name="citation_reference" content="citation_title=The Building Blocks of Interpretability;citation_author=Chris Olah;citation_author=Arvind Satyanarayan;citation_author=Ian Johnson;citation_author=Shan Carter;citation_author=Ludwig Schubert;citation_author=Katherine Ye;citation_author=Alexander Mordvintsev;citation_publication_date=2018;citation_journal_title=Distill;">
    <meta name="citation_reference" content="citation_title=Leveraging Procedural Generation to Benchmark Reinforcement Learning;citation_author=Karl Cobbe;citation_author=Christopher Hesse;citation_author=Jacob Hilton;citation_author=John Schulman;citation_publication_date=2019;">
    <meta name="citation_reference" content="citation_title=Proximal policy optimization algorithms;citation_author=John Schulman;citation_author=Filip Wolski;citation_author=Prafulla Dhariwal;citation_author=Alec Radford;citation_author=Oleg Klimov;citation_publication_date=2017;citation_arxiv_id=1707.06347;">
    <meta name="citation_reference" content="citation_title=High-dimensional continuous control using generalized advantage estimation;citation_author=John Schulman;citation_author=Philipp Moritz;citation_author=Sergey Levine;citation_author=Michael Jordan;citation_author=Pieter Abbeel;citation_publication_date=2015;citation_arxiv_id=1506.02438;">
    <meta name="citation_reference" content="citation_title=Thread: Circuits;citation_author=Nick Cammarata;citation_author=Shan Carter;citation_author=Gabriel Goh;citation_author=Chris Olah;citation_author=Michael Petrov;citation_author=Ludwig Schubert;citation_publication_date=2020;citation_journal_title=Distill;">
    <meta name="citation_reference" content="citation_title=General Video Game AI: A multi-track framework for evaluating agents, games and content generation algorithms;citation_author=Diego Perez-Liebana;citation_author=Jialin Liu;citation_author=Ahmed Khalifa;citation_author=Raluca D Gaina;citation_author=Julian Togelius;citation_author=Simon M Lucas;citation_publication_date=2018;citation_arxiv_id=1802.10363;">
    <meta name="citation_reference" content="citation_title=Obstacle Tower: A Generalization Challenge in Vision, Control, and Planning;citation_author=Arthur Juliani;citation_author=Ahmed Khalifa;citation_author=Vincent-Pierre Berges;citation_author=Jonathan Harper;citation_author=Hunter Henry;citation_author=Adam Crespi;citation_author=Julian Togelius;citation_author=Danny Lange;citation_publication_date=2019;citation_arxiv_id=1902.01378;">
    <meta name="citation_reference" content="citation_title=Observational Overfitting in Reinforcement Learning;citation_author=Xingyou Song;citation_author=Yiding Jiang;citation_author=Yilun Du;citation_author=Behnam Neyshabur;citation_publication_date=2019;citation_arxiv_id=1912.02975;">
    <meta name="citation_reference" content="citation_title=Feature Visualization;citation_author=Chris Olah;citation_author=Alexander Mordvintsev;citation_author=Ludwig Schubert;citation_publication_date=2017;citation_journal_title=Distill;">
    <meta name="citation_reference" content="citation_title=Visualizing higher-layer features of a deep network;citation_author=Dumitru Erhan;citation_author=Yoshua Bengio;citation_author=Aaron Courville;citation_author=Pascal Vincent;citation_publication_date=2009;citation_journal_title=University of Montreal;citation_volume=1341;citation_number=3;">
    <meta name="citation_reference" content="citation_title=Deep neural networks are easily fooled: High confidence predictions for unrecognizable images;citation_author=Anh Nguyen;citation_author=Jason Yosinski;citation_author=Jeff Clune;citation_publication_date=2015;">
    <meta name="citation_reference" content="citation_title=Inceptionism: Going deeper into neural networks;citation_author=Alexander Mordvintsev;citation_author=Christopher Olah;citation_author=Mike Tyka;citation_publication_date=2015;citation_journal_title=Google Research Blog;">
    <meta name="citation_reference" content="citation_title=Plug &amp; play generative networks: Conditional iterative generation of images in latent space;citation_author=Anh Nguyen;citation_author=Jeff Clune;citation_author=Yoshua Bengio;citation_author=Alexey Dosovitskiy;citation_author=Jason Yosinski;citation_publication_date=2017;">
    <meta name="citation_reference" content="citation_title=Imagenet: A large-scale hierarchical image database;citation_author=Jia Deng;citation_author=Wei Dong;citation_author=Richard Socher;citation_author=Li-Jia Li;citation_author=Kai Li;citation_author=Li Fei-Fei;citation_publication_date=2009;">
    <meta name="citation_reference" content="citation_title=Going deeper with convolutions;citation_author=Christian Szegedy;citation_author=Wei Liu;citation_author=Yangqing Jia;citation_author=Pierre Sermanet;citation_author=Scott Reed;citation_author=Dragomir Anguelov;citation_author=Dumitru Erhan;citation_author=Vincent Vanhoucke;citation_author=Andrew Rabinovich;citation_publication_date=2015;citation_arxiv_id=1409.4842;">
    <meta name="citation_reference" content="citation_title=An Atari model zoo for analyzing, visualizing, and comparing deep reinforcement learning agents;citation_author=Felipe Petroski Such;citation_author=Vashisht Madhavan;citation_author=Rosanne Liu;citation_author=Rui Wang;citation_author=Pablo Samuel Castro;citation_author=Yulun Li;citation_author=Ludwig Schubert;citation_author=Marc Bellemare;citation_author=Jeff Clune;citation_author=Joel Lehman;citation_publication_date=2018;citation_arxiv_id=1812.07069;">
    <meta name="citation_reference" content="citation_title=Finding and Visualizing Weaknesses of Deep Reinforcement Learning Agents;citation_author=Christian Rupprecht;citation_author=Cyril Ibrahim;citation_author=Christopher J Pal;citation_publication_date=2019;citation_arxiv_id=1904.01318;">
    <meta name="citation_reference" content="citation_title=Caricatures;citation_author=Nick Cammerata;citation_author=Chris Olah;citation_author=Arvind Satyanarayan;citation_publication_date=unpublished;citation_journal_title=Distill draft;">
    <meta name="citation_reference" content="citation_title=Towards deep learning models resistant to adversarial attacks;citation_author=Aleksander Madry;citation_author=Aleksandar Makelov;citation_author=Ludwig Schmidt;citation_author=Dimitris Tsipras;citation_author=Adrian Vladu;citation_publication_date=2017;citation_arxiv_id=1706.06083;">
    <meta name="citation_reference" content="citation_title=Intriguing properties of neural networks;citation_author=Christian Szegedy;citation_author=Wojciech Zaremba;citation_author=Ilya Sutskever;citation_author=Joan Bruna;citation_author=Dumitru Erhan;citation_author=Ian Goodfellow;citation_author=Rob Fergus;citation_publication_date=2013;citation_arxiv_id=1312.6199;">
    <meta name="citation_reference" content="citation_title=Visualizing and understanding Atari agents;citation_author=Sam Greydanus;citation_author=Anurag Koul;citation_author=Jonathan Dodge;citation_author=Alan Fern;citation_publication_date=2017;citation_arxiv_id=1711.00138;">
    <meta name="citation_reference" content="citation_title=Explain Your Move: Understanding Agent Actions Using Specific and Relevant Feature Attribution;citation_author=Nikaash Puri;citation_author=Sukriti Verma;citation_author=Piyush Gupta;citation_author=Dhruv Kayastha;citation_author=Shripad Deshmukh;citation_author=Balaji Krishnamurthy;citation_author=Sameer Singh;citation_publication_date=2019;">
    <meta name="citation_reference" content="citation_title=Video Interface: Assuming Multiple Perspectives on a Video Exposes Hidden Structure;citation_author=Robert M Ochshorn;citation_publication_date=2017;">
    <meta name="citation_reference" content="citation_title=Reconciling modern machine-learning practice and the classical bias--variance trade-off;citation_author=Mikhail Belkin;citation_author=Daniel Hsu;citation_author=Siyuan Ma;citation_author=Soumik Mandal;citation_publication_date=2019;citation_journal_title=Proceedings of the National Academy of Sciences;citation_volume=116;citation_number=32;">
    <meta name="citation_reference" content="citation_title=Adversarial examples are not bugs, they are features;citation_author=Andrew Ilyas;citation_author=Shibani Santurkar;citation_author=Dimitris Tsipras;citation_author=Logan Engstrom;citation_author=Brandon Tran;citation_author=Aleksander Madry;citation_publication_date=2019;citation_arxiv_id=1905.02175;">
    <meta name="citation_reference" content="citation_title=A Discussion of 'Adversarial Examples Are Not Bugs, They Are Features';citation_author=Logan Engstrom;citation_author=Justin Gilmer;citation_author=Gabriel Goh;citation_author=Dan Hendrycks;citation_author=Andrew Ilyas;citation_author=Aleksander Madry;citation_author=Reiichiro Nakano;citation_author=Preetum Nakkiran;citation_author=Shibani Santurkar;citation_author=Brandon Tran;citation_author=Dimitris Tsipras;citation_author=Eric Wallace;citation_publication_date=2019;citation_journal_title=Distill;">
    <meta name="citation_reference" content="citation_title=Human-level performance in 3D multiplayer games with population-based reinforcement learning;citation_author=Max Jaderberg;citation_author=Wojciech M Czarnecki;citation_author=Iain Dunning;citation_author=Luke Marris;citation_author=Guy Lever;citation_author=Antonio Garcia Castaneda;citation_author=Charles Beattie;citation_author=Neil C Rabinowitz;citation_author=Ari S Morcos;citation_author=Avraham Ruderman;citation_author= others;citation_publication_date=2019;citation_journal_title=Science;citation_volume=364;citation_number=6443;">
    <meta name="citation_reference" content="citation_title=Solving Rubik's Cube with a Robot Hand;citation_author=Ilge Akkaya;citation_author=Marcin Andrychowicz;citation_author=Maciek Chociej;citation_author=Mateusz Litwin;citation_author=Bob McGrew;citation_author=Arthur Petron;citation_author=Alex Paino;citation_author=Matthias Plappert;citation_author=Glenn Powell;citation_author=Raphael Ribas;citation_author= others;citation_publication_date=2019;citation_arxiv_id=1910.07113;">
    <meta name="citation_reference" content="citation_title=Dota 2 with Large Scale Deep Reinforcement Learning;citation_author=Christopher Berner;citation_author=Greg Brockman;citation_author=Brooke Chan;citation_author=Vicki Cheung;citation_author=Przemyslaw Dębiak;citation_author=Christy Dennison;citation_author=David Farhi;citation_author=Quirin Fischer;citation_author=Shariq Hashme;citation_author=Chris Hesse;citation_author= others;citation_publication_date=2019;citation_arxiv_id=1912.06680;">
    <meta name="citation_reference" content="citation_title=Does Attribution Make Sense?;citation_author=Chris Olah;citation_author=Arvind Satyanarayan;citation_publication_date=unpublished;citation_journal_title=Distill draft;">
    <meta name="citation_reference" content="citation_title=IMPALA: Scalable distributed deep-RL with importance weighted actor-learner architectures;citation_author=Lasse Espeholt;citation_author=Hubert Soyer;citation_author=Remi Munos;citation_author=Karen Simonyan;citation_author=Volodymir Mnih;citation_author=Tom Ward;citation_author=Yotam Doron;citation_author=Vlad Firoiu;citation_author=Tim Harley;citation_author=Iain Dunning;citation_author= others;citation_publication_date=2018;citation_arxiv_id=1802.01561;">
</head>

<body distill-prerendered=""><distill-header distill-prerendered="">
<style>
distill-header {
  position: relative;
  height: 60px;
  background-color: hsl(200, 60%, 15%);
  width: 100%;
  box-sizing: border-box;
  z-index: 2;
  color: rgba(0, 0, 0, 0.8);
  border-bottom: 1px solid rgba(0, 0, 0, 0.08);
  box-shadow: 0 1px 6px rgba(0, 0, 0, 0.05);
}
distill-header .content {
  height: 70px;
  grid-column: page;
}
distill-header a {
  font-size: 16px;
  height: 60px;
  line-height: 60px;
  text-decoration: none;
  color: rgba(255, 255, 255, 0.8);
  padding: 22px 0;
}
distill-header a:hover {
  color: rgba(255, 255, 255, 1);
}
distill-header svg {
  width: 24px;
  position: relative;
  top: 4px;
  margin-right: 2px;
}
@media(min-width: 1080px) {
  distill-header {
    height: 70px;
  }
  distill-header a {
    height: 70px;
    line-height: 70px;
    padding: 28px 0;
  }
  distill-header .logo {
  }
}
distill-header svg path {
  fill: none;
  stroke: rgba(255, 255, 255, 0.8);
  stroke-width: 3px;
}
distill-header .logo {
  font-size: 17px;
  font-weight: 200;
}
distill-header .nav {
  float: right;
  font-weight: 300;
}
distill-header .nav a {
  font-size: 12px;
  margin-left: 24px;
  text-transform: uppercase;
}
</style>
<div class="content">
  <a href="/" class="logo">
    <svg viewBox="-607 419 64 64">
  <path d="M-573.4,478.9c-8,0-14.6-6.4-14.6-14.5s14.6-25.9,14.6-40.8c0,14.9,14.6,32.8,14.6,40.8S-565.4,478.9-573.4,478.9z"></path>
</svg>

    Distill
  </a>
  <nav class="nav">
    <a href="/about/">About</a>
    <a href="/prize/">Prize</a>
    <a href="/journal/">Submit</a>
  </nav>
</div>
</distill-header>

  <d-front-matter>
    <script type="text/json">{
      "title": "Understanding RL Vision",
      "description": "With diverse environments, we can analyze, diagnose and edit deep reinforcement learning models using attribution.",
      "authors": [
        {
          "author": "Jacob Hilton",
          "authorURL": "https://www.jacobh.co.uk/",
          "affiliation": "OpenAI",
          "affiliationURL": "https://openai.com"
        },
        {
          "author": "Nick Cammarata",
          "authorURL": "http://nickcammarata.com/",
          "affiliation": "OpenAI",
          "affiliationURL": "https://openai.com"
        },
        {
          "author": "Shan Carter",
          "authorURL": "http://shancarter.com/",
          "affiliation": "Observable",
          "affiliationURL": "http://observablehq.com/"
        },
        {
          "author": "Gabriel Goh",
          "authorURL": "http://gabgoh.github.io/",
          "affiliation": "OpenAI",
          "affiliationURL": "https://openai.com"
        },
        {
          "author": "Chris Olah",
          "authorURL": "https://colah.github.io/",
          "affiliation": "OpenAI",
          "affiliationURL": "https://openai.com"
        }
      ],
      "katex": {
        "delimiters": [
          {
            "left": "$",
            "right": "$",
            "display": false
          },
          {
            "left": "$$",
            "right": "$$",
            "display": true
          }
        ]
      }
    }</script>
  </d-front-matter>

  <d-title>
    <h1>Understanding RL Vision</h1>
    <p>With diverse environments, we can analyze, diagnose and edit deep reinforcement learning models using attribution.</p>
  </d-title>

  <d-byline>
  <div class="byline grid">
    <div class="authors-affiliations grid">
      <h3>Authors</h3>
      <h3>Affiliations</h3>

        <p class="author">

            <a class="name" href="https://www.jacobh.co.uk/">Ahmed sabir </a>
        </p>
        <p class="affiliation">
        <a class="affiliation" href="https://openai.com">OpenAI</a>
        </p>

        <p class="author">

            <a class="name" href="http://nickcammarata.com/">Nick Cammarata</a>
        </p>
        <p class="affiliation">
        <a class="affiliation" href="https://openai.com">OpenAI</a>
        </p>

        <p class="author">

            <a class="name" href="http://shancarter.com/">Shan Carter</a>
        </p>
        <p class="affiliation">
        <a class="affiliation" href="http://observablehq.com/">Observable</a>
        </p>

        <p class="author">

            <a class="name" href="http://gabgoh.github.io/">Gabriel Goh</a>
        </p>
        <p class="affiliation">
        <a class="affiliation" href="https://openai.com">OpenAI</a>
        </p>

        <p class="author">

            <a class="name" href="https://colah.github.io/">Chris Olah</a>
        </p>
        <p class="affiliation">
        <a class="affiliation" href="https://openai.com">OpenAI</a>
        </p>

    </div>
    <div>
      <h3>Published</h3>

        <p>Nov. 17, 2020</p>
    </div>
    <div>
      <h3>DOI</h3>

        <p><a href="https://doi.org/10.23915/distill.00029">10.23915/distill.00029</a></p>
    </div>
  </div>
</d-byline><d-article>

    <d-contents>
      <nav class="l-text figcaption">
        <h3>Contents</h3>
        <div><a href="#introduction">Introduction</a></div>
        <div><a href="#coinrun">Our CoinRun model</a></div>
        <div><a href="#analysis">Model analysis</a></div>
        <ul>
          <li><a href="#dissecting-failure">Dissecting failure</a></li>
          <li><a href="#hallucinations">Hallucinations</a></li>
          <li><a href="#model-editing">Model editing</a></li>
        </ul>
        <div><a href="#diversity-hypothesis">The diversity hypothesis</a></div>
        <div><a href="#feature-visualization">Feature visualization</a></div>
        <div><a href="#attribution">Attribution</a></div>
        <div><a href="#questions">Questions for further research</a></div>
      </nav>
    </d-contents>

    <div>
      <p id="introduction">







    <d-footnote-list></d-footnote-list>
    <d-citation-list distill-prerendered="true"><style>
d-citation-list {
  contain: style;
}

d-citation-list .references {
  grid-column: text;
}

d-citation-list .references .title {
  font-weight: 500;
}
</style><h3 id="references">References</h3><ol id="references-list" class="references"><li id="coinrunpaper"><span class="title">Quantifying generalization in reinforcement learning</span>   <a href="https://openai.com/blog/quantifying-generalization-in-reinforcement-learning/">[link]</a><br>Cobbe, K., Klimov, O., Hesse, C., Kim, T. and Schulman, J., 2018. arXiv preprint arXiv:1812.02341. </li><li id="attribution1"><span class="title">Deep inside convolutional networks: Visualising image classification models and saliency maps</span>   <a href="https://arxiv.org/pdf/1312.6034.pdf">[PDF]</a><br>Simonyan, K., Vedaldi, A. and Zisserman, A., 2013. arXiv preprint arXiv:1312.6034. </li><li id="attribution2"><span class="title">Visualizing and understanding convolutional networks</span>   <a href="https://arxiv.org/pdf/1311.2901.pdf">[PDF]</a><br>Zeiler, M.D. and Fergus, R., 2014. European conference on computer vision, pp. 818--833. </li><li id="attribution3"><span class="title">Striving for simplicity: The all convolutional net</span>   <a href="https://arxiv.org/pdf/1412.6806.pdf">[PDF]</a><br>Springenberg, J.T., Dosovitskiy, A., Brox, T. and Riedmiller, M., 2014. arXiv preprint arXiv:1412.6806. </li><li id="gradcam"><span class="title">Grad-CAM: Visual explanations from deep networks via gradient-based localization</span>   <a href="http://openaccess.thecvf.com/content_ICCV_2017/papers/Selvaraju_Grad-CAM_Visual_Explanations_ICCV_2017_paper.pdf">[PDF]</a><br>Selvaraju, R.R., Cogswell, M., Das, A., Vedantam, R., Parikh, D. and Batra, D., 2017. Proceedings of the IEEE International Conference on Computer Vision, pp. 618--626. </li><li id="attribution4"><span class="title">Interpretable explanations of black boxes by meaningful perturbation</span>   <a href="http://openaccess.thecvf.com/content_ICCV_2017/papers/Fong_Interpretable_Explanations_of_ICCV_2017_paper.pdf">[PDF]</a><br>Fong, R.C. and Vedaldi, A., 2017. Proceedings of the IEEE International Conference on Computer Vision, pp. 3429--3437. </li><li id="attribution5"><span class="title">PatternNet and PatternLRP--Improving the interpretability of neural networks</span>   <a href="https://arxiv.org/pdf/1705.05598.pdf">[PDF]</a><br>Kindermans, P., Schutt, K.T., Alber, M., Muller, K. and Dahne, S., 2017. stat, Vol 1050, pp. 16. </li><li id="attribution6"><span class="title">The (un)reliability of saliency methods</span>   <a href="https://arxiv.org/pdf/1711.00867.pdf">[PDF]</a><br>Kindermans, P., Hooker, S., Adebayo, J., Alber, M., Schutt, K.T., Dahne, S., Erhan, D. and Kim, B., 2019. Explainable AI: Interpreting, Explaining and Visualizing Deep Learning, pp. 267--280. Springer.</li><li id="integratedgradients"><span class="title">Axiomatic attribution for deep networks</span>   <a href="https://arxiv.org/pdf/1703.01365.pdf">[PDF]</a><br>Sundararajan, M., Taly, A. and Yan, Q., 2017. Proceedings of the 34th International Conference on Machine Learning-Volume 70, pp. 3319--3328. </li><li id="buildingblocks"><span class="title">The Building Blocks of Interpretability</span> <br>Olah, C., Satyanarayan, A., Johnson, I., Carter, S., Schubert, L., Ye, K. and Mordvintsev, A., 2018. Distill.  <a href="https://doi.org/10.23915/distill.00010" style="text-decoration:inherit;">DOI: 10.23915/distill.00010</a></li><li id="procgen"><span class="title">Leveraging Procedural Generation to Benchmark Reinforcement Learning</span>   <a href="https://openai.com/blog/procgen-benchmark/">[link]</a><br>Cobbe, K., Hesse, C., Hilton, J. and Schulman, J., 2019. </li><li id="ppo"><span class="title">Proximal policy optimization algorithms</span>   <a href="https://openai.com/blog/openai-baselines-ppo/">[link]</a><br>Schulman, J., Wolski, F., Dhariwal, P., Radford, A. and Klimov, O., 2017. arXiv preprint arXiv:1707.06347. </li><li id="gae"><span class="title">High-dimensional continuous control using generalized advantage estimation</span>   <a href="https://arxiv.org/pdf/1506.02438.pdf">[PDF]</a><br>Schulman, J., Moritz, P., Levine, S., Jordan, M. and Abbeel, P., 2015. arXiv preprint arXiv:1506.02438. </li><li id="circuits"><span class="title">Thread: Circuits</span> <br>Cammarata, N., Carter, S., Goh, G., Olah, C., Petrov, M. and Schubert, L., 2020. Distill.  <a href="https://doi.org/10.23915/distill.00024" style="text-decoration:inherit;">DOI: 10.23915/distill.00024</a></li><li id="gvgai"><span class="title">General Video Game AI: A multi-track framework for evaluating agents, games and content generation algorithms</span>   <a href="https://arxiv.org/pdf/1802.10363">[link]</a><br>Perez-Liebana, D., Liu, J., Khalifa, A., Gaina, R.D., Togelius, J. and Lucas, S.M., 2018. arXiv preprint arXiv:1802.10363. </li><li id="obstacletower"><span class="title">Obstacle Tower: A Generalization Challenge in Vision, Control, and Planning</span>   <a href="https://arxiv.org/pdf/1902.01378.pdf">[PDF]</a><br>Juliani, A., Khalifa, A., Berges, V., Harper, J., Henry, H., Crespi, A., Togelius, J. and Lange, D., 2019. arXiv preprint arXiv:1902.01378. </li><li id="sonicsaliency"><span class="title">Observational Overfitting in Reinforcement Learning</span>   <a href="https://arxiv.org/pdf/1912.02975.pdf">[PDF]</a><br>Song, X., Jiang, Y., Du, Y. and Neyshabur, B., 2019. arXiv preprint arXiv:1912.02975. </li><li id="featurevis"><span class="title">Feature Visualization</span> <br>Olah, C., Mordvintsev, A. and Schubert, L., 2017. Distill.  <a href="https://doi.org/10.23915/distill.00007" style="text-decoration:inherit;">DOI: 10.23915/distill.00007</a></li><li id="featurevis1"><span class="title">Visualizing higher-layer features of a deep network</span>   <a href="https://www.researchgate.net/profile/Aaron_Courville/publication/265022827_Visualizing_Higher-Layer_Features_of_a_Deep_Network/links/53ff82b00cf24c81027da530.pdf">[PDF]</a><br>Erhan, D., Bengio, Y., Courville, A. and Vincent, P., 2009. University of Montreal, Vol 1341(3), pp. 1. </li><li id="featurevis2"><span class="title">Deep neural networks are easily fooled: High confidence predictions for unrecognizable images</span>   <a href="https://www.cv-foundation.org/openaccess/content_cvpr_2015/papers/Nguyen_Deep_Neural_Networks_2015_CVPR_paper.pdf">[PDF]</a><br>Nguyen, A., Yosinski, J. and Clune, J., 2015. Proceedings of the IEEE conference on computer vision and pattern recognition, pp. 427--436. </li><li id="featurevis3"><span class="title">Inceptionism: Going deeper into neural networks</span>   <a href="https://research.googleblog.com/2015/06/inceptionism-going-deeper-into-neural.html">[HTML]</a><br>Mordvintsev, A., Olah, C. and Tyka, M., 2015. Google Research Blog. </li><li id="featurevis4"><span class="title">Plug &amp; play generative networks: Conditional iterative generation of images in latent space</span>   <a href="http://openaccess.thecvf.com/content_cvpr_2017/papers/Nguyen_Plug__Play_CVPR_2017_paper.pdf">[PDF]</a><br>Nguyen, A., Clune, J., Bengio, Y., Dosovitskiy, A. and Yosinski, J., 2017. Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition, pp. 4467--4477. </li><li id="imagenet"><span class="title">Imagenet: A large-scale hierarchical image database</span>   <a href="http://www.image-net.org/papers/imagenet_cvpr09.pdf">[PDF]</a><br>Deng, J., Dong, W., Socher, R., Li, L., Li, K. and Fei-Fei, L., 2009. Computer Vision and Pattern Recognition, 2009. CVPR 2009. IEEE Conference on, pp. 248--255.  <a href="https://doi.org/10.1109/cvprw.2009.5206848" style="text-decoration:inherit;">DOI: 10.1109/cvprw.2009.5206848</a></li><li id="googlenet"><span class="title">Going deeper with convolutions</span>   <a href="https://arxiv.org/pdf/1409.4842.pdf">[PDF]</a><br>Szegedy, C., Liu, W., Jia, Y., Sermanet, P., Reed, S., Anguelov, D., Erhan, D., Vanhoucke, V. and Rabinovich, A., 2015. Proceedings of the IEEE conference on computer vision and pattern recognition, pp. 1--9. </li><li id="atarimodelzoo"><span class="title">An Atari model zoo for analyzing, visualizing, and comparing deep reinforcement learning agents</span>   <a href="https://arxiv.org/pdf/1812.07069.pdf">[PDF]</a><br>Such, F.P., Madhavan, V., Liu, R., Wang, R., Castro, P.S., Li, Y., Schubert, L., Bellemare, M., Clune, J. and Lehman, J., 2018. arXiv preprint arXiv:1812.07069. </li><li id="atarifeaturevis"><span class="title">Finding and Visualizing Weaknesses of Deep Reinforcement Learning Agents</span>   <a href="https://arxiv.org/pdf/1904.01318.pdf">[PDF]</a><br>Rupprecht, C., Ibrahim, C. and Pal, C.J., 2019. arXiv preprint arXiv:1904.01318. </li><li id="caricatures"><span class="title">Caricatures</span> <br>Cammerata, N., Olah, C. and Satyanarayan, A., unpublished. Distill draft. Author list not yet finalized.</li><li id="adversarialtraining"><span class="title">Towards deep learning models resistant to adversarial attacks</span> <br>Madry, A., Makelov, A., Schmidt, L., Tsipras, D. and Vladu, A., 2017. arXiv preprint arXiv:1706.06083. </li><li id="featurevisadversarial"><span class="title">Intriguing properties of neural networks</span>   <a href="https://arxiv.org/pdf/1312.6199.pdf">[PDF]</a><br>Szegedy, C., Zaremba, W., Sutskever, I., Bruna, J., Erhan, D., Goodfellow, I. and Fergus, R., 2013. arXiv preprint arXiv:1312.6199. </li><li id="perturbationsaliency"><span class="title">Visualizing and understanding Atari agents</span>   <a href="https://arxiv.org/pdf/1711.00138.pdf">[PDF]</a><br>Greydanus, S., Koul, A., Dodge, J. and Fern, A., 2017. arXiv preprint arXiv:1711.00138. </li><li id="sarfa"><span class="title">Explain Your Move: Understanding Agent Actions Using Specific and Relevant Feature Attribution</span>   <a href="https://nikaashpuri.github.io/sarfa-saliency/">[link]</a><br>Puri, N., Verma, S., Gupta, P., Kayastha, D., Deshmukh, S., Krishnamurthy, B. and Singh, S., 2019. International Conference on Learning Representations. </li><li id="rmo"><span class="title">Video Interface: Assuming Multiple Perspectives on a Video Exposes Hidden Structure</span>   <a href="https://rmozone.com/snapshots/2017/10/rmo-at-google/#chew">[link]</a><br>Ochshorn, R.M., 2017. </li><li id="doubledescent"><span class="title">Reconciling modern machine-learning practice and the classical bias--variance trade-off</span>   <a href="http://www.cs.columbia.edu/~djhsu/papers/biasvariance-pnas.pdf">[PDF]</a><br>Belkin, M., Hsu, D., Ma, S. and Mandal, S., 2019. Proceedings of the National Academy of Sciences, Vol 116(32), pp. 15849--15854. National Acad Sciences.</li><li id="advexfeatures"><span class="title">Adversarial examples are not bugs, they are features</span>   <a href="https://gradientscience.org/adv/">[link]</a><br>Ilyas, A., Santurkar, S., Tsipras, D., Engstrom, L., Tran, B. and Madry, A., 2019. arXiv preprint arXiv:1905.02175. </li><li id="advexfeaturesdiscussion"><span class="title">A Discussion of 'Adversarial Examples Are Not Bugs, They Are Features'</span> <br>Engstrom, L., Gilmer, J., Goh, G., Hendrycks, D., Ilyas, A., Madry, A., Nakano, R., Nakkiran, P., Santurkar, S., Tran, B., Tsipras, D. and Wallace, E., 2019. Distill.  <a href="https://doi.org/10.23915/distill.00019" style="text-decoration:inherit;">DOI: 10.23915/distill.00019</a></li><li id="capturetheflag"><span class="title">Human-level performance in 3D multiplayer games with population-based reinforcement learning</span>   <a href="https://deepmind.com/blog/article/capture-the-flag-science">[link]</a><br>Jaderberg, M., Czarnecki, W.M., Dunning, I., Marris, L., Lever, G., Castaneda, A.G., Beattie, C., Rabinowitz, N.C., Morcos, A.S., Ruderman, A. and others,, 2019. Science, Vol 364(6443), pp. 859--865. American Association for the Advancement of Science.</li><li id="rubik"><span class="title">Solving Rubik's Cube with a Robot Hand</span>   <a href="https://openai.com/blog/solving-rubiks-cube/">[link]</a><br>Akkaya, I., Andrychowicz, M., Chociej, M., Litwin, M., McGrew, B., Petron, A., Paino, A., Plappert, M., Powell, G., Ribas, R. and others,, 2019. arXiv preprint arXiv:1910.07113. </li><li id="dota"><span class="title">Dota 2 with Large Scale Deep Reinforcement Learning</span>   <a href="https://openai.com/projects/five/">[link]</a><br>Berner, C., Brockman, G., Chan, B., Cheung, V., Dębiak, P., Dennison, C., Farhi, D., Fischer, Q., Hashme, S., Hesse, C. and others,, 2019. arXiv preprint arXiv:1912.06680. </li><li id="attributionpaths"><span class="title">Does Attribution Make Sense?</span> <br>Olah, C. and Satyanarayan, A., unpublished. Distill draft. Author list not yet finalized.</li><li id="impala"><span class="title">IMPALA: Scalable distributed deep-RL with importance weighted actor-learner architectures</span>   <a href="https://arxiv.org/pdf/1802.01561.pdf">[PDF]</a><br>Espeholt, L., Soyer, H., Munos, R., Simonyan, K., Mnih, V., Ward, T., Doron, Y., Firoiu, V., Harley, T., Dunning, I. and others,, 2018. arXiv preprint arXiv:1802.01561. </li></ol></d-citation-list>
  <distill-appendix>
<style>
  distill-appendix {
    contain: layout style;
  }

  distill-appendix .citation {
    font-size: 11px;
    line-height: 15px;
    border-left: 1px solid rgba(0, 0, 0, 0.1);
    padding-left: 18px;
    border: 1px solid rgba(0,0,0,0.1);
    background: rgba(0, 0, 0, 0.02);
    padding: 10px 18px;
    border-radius: 3px;
    color: rgba(150, 150, 150, 1);
    overflow: hidden;
    margin-top: -12px;
    white-space: pre-wrap;
    word-wrap: break-word;
  }

  distill-appendix > * {
    grid-column: text;
  }
</style>

    <h3 id="updates-and-corrections">Updates and Corrections</h3>
    <p>
    If you see mistakes or want to suggest changes, please <a href="https://github.com/distillpub/post--understanding-rl-vision/issues/new">create an issue on GitHub</a>. </p>

    <h3 id="reuse">Reuse</h3>
    <p>Diagrams and text are licensed under Creative Commons Attribution <a href="https://creativecommons.org/licenses/by/4.0/">CC-BY 4.0</a> with the <a class="github" href="https://github.com/distillpub/post--understanding-rl-vision">source available on GitHub</a>, unless noted otherwise. The figures that have been reused from other sources don’t fall under this license and can be recognized by a note in their caption: “Figure from …”.</p>

    <h3 id="citation">Citation</h3>
    <p>For attribution in academic contexts, please cite this work as</p>
    <pre class="citation short">Hilton, et al., "Understanding RL Vision", Distill, 2020.</pre>
    <p>BibTeX citation</p>
    <pre class="citation long">@article{hilton2020understanding,
  author = {Hilton, Jacob and Cammarata, Nick and Carter, Shan and Goh, Gabriel and Olah, Chris},
  title = {Understanding RL Vision},
  journal = {Distill},
  year = {2020},
  note = {https://distill.pub/2020/understanding-rl-vision},
  doi = {10.23915/distill.00029}
}</pre>
    </distill-appendix></d-appendix>

  <d-bibliography><script type="text/json">[["featurevis",{"author":"Olah, Chris and Mordvintsev, Alexander and Schubert, Ludwig","title":"Feature Visualization","journal":"Distill","year":"2017","note":"https://distill.pub/2017/feature-visualization","doi":"10.23915/distill.00007","type":"article"}],["buildingblocks",{"author":"Olah, Chris and Satyanarayan, Arvind and Johnson, Ian and Carter, Shan and Schubert, Ludwig and Ye, Katherine and Mordvintsev, Alexander","title":"The Building Blocks of Interpretability","journal":"Distill","year":"2018","note":"https://distill.pub/2018/building-blocks","doi":"10.23915/distill.00010","type":"article"}],["caricatures",{"author":"Cammerata, Nick and Olah, Chris and Satyanarayan, Arvind","title":"Caricatures","journal":"Distill draft","year":"unpublished","publisher":"Author list not yet finalized","type":"article"}],["attributionpaths",{"author":"Olah, Chris and Satyanarayan, Arvind","title":"Does Attribution Make Sense?","journal":"Distill draft","year":"unpublished","publisher":"Author list not yet finalized","type":"article"}],["circuits",{"author":"Cammarata, Nick and Carter, Shan and Goh, Gabriel and Olah, Chris and Petrov, Michael and Schubert, Ludwig","title":"Thread: Circuits","journal":"Distill","year":"2020","note":"https://distill.pub/2020/circuits","doi":"10.23915/distill.00024","type":"article"}],["integratedgradients",{"title":"Axiomatic attribution for deep networks","author":"Sundararajan, Mukund and Taly, Ankur and Yan, Qiqi","booktitle":"Proceedings of the 34th International Conference on Machine Learning-Volume 70","pages":"3319--3328","year":"2017","organization":"JMLR. org","url":"https://arxiv.org/pdf/1703.01365.pdf","type":"inproceedings"}],["gradcam",{"title":"Grad-CAM: Visual explanations from deep networks via gradient-based localization","author":"Selvaraju, Ramprasaath R and Cogswell, Michael and Das, Abhishek and Vedantam, Ramakrishna and Parikh, Devi and Batra, Dhruv","booktitle":"Proceedings of the IEEE International Conference on Computer Vision","pages":"618--626","year":"2017","url":"http://openaccess.thecvf.com/content_ICCV_2017/papers/Selvaraju_Grad-CAM_Visual_Explanations_ICCV_2017_paper.pdf","type":"inproceedings"}],["atarimodelzoo",{"title":"An Atari model zoo for analyzing, visualizing, and comparing deep reinforcement learning agents","author":"Such, Felipe Petroski and Madhavan, Vashisht and Liu, Rosanne and Wang, Rui and Castro, Pablo Samuel and Li, Yulun and Schubert, Ludwig and Bellemare, Marc and Clune, Jeff and Lehman, Joel","journal":"arXiv preprint arXiv:1812.07069","year":"2018","url":"https://arxiv.org/pdf/1812.07069.pdf","type":"article"}],["perturbationsaliency",{"title":"Visualizing and understanding Atari agents","author":"Greydanus, Sam and Koul, Anurag and Dodge, Jonathan and Fern, Alan","journal":"arXiv preprint arXiv:1711.00138","year":"2017","url":"https://arxiv.org/pdf/1711.00138.pdf","type":"article"}],["sarfa",{"title":"Explain Your Move: Understanding Agent Actions Using Specific and Relevant Feature Attribution","author":"Puri, Nikaash and Verma, Sukriti and Gupta, Piyush and Kayastha, Dhruv and Deshmukh, Shripad and Krishnamurthy, Balaji and Singh, Sameer","booktitle":"International Conference on Learning Representations","year":"2019","url":"https://nikaashpuri.github.io/sarfa-saliency/","type":"inproceedings"}],["atarifeaturevis",{"title":"Finding and Visualizing Weaknesses of Deep Reinforcement Learning Agents","author":"Rupprecht, Christian and Ibrahim, Cyril and Pal, Christopher J","journal":"arXiv preprint arXiv:1904.01318","year":"2019","url":"https://arxiv.org/pdf/1904.01318.pdf","type":"article"}],["sonicsaliency",{"title":"Observational Overfitting in Reinforcement Learning","author":"Song, Xingyou and Jiang, Yiding and Du, Yilun and Neyshabur, Behnam","journal":"arXiv preprint arXiv:1912.02975","year":"2019","url":"https://arxiv.org/pdf/1912.02975.pdf","type":"article"}],["gvgai",{"title":"General Video Game AI: A multi-track framework for evaluating agents, games and content generation algorithms","author":"Perez-Liebana, Diego and Liu, Jialin and Khalifa, Ahmed and Gaina, Raluca D and Togelius, Julian and Lucas, Simon M","journal":"arXiv preprint arXiv:1802.10363","year":"2018","url":"https://arxiv.org/pdf/1802.10363","type":"article"}],["obstacletower",{"title":"Obstacle Tower: A Generalization Challenge in Vision, Control, and Planning","author":"Juliani, Arthur and Khalifa, Ahmed and Berges, Vincent-Pierre and Harper, Jonathan and Henry, Hunter and Crespi, Adam and Togelius, Julian and Lange, Danny","journal":"arXiv preprint arXiv:1902.01378","year":"2019","url":"https://arxiv.org/pdf/1902.01378.pdf","type":"article"}],["coinrunpaper",{"title":"Quantifying generalization in reinforcement learning","author":"Cobbe, Karl and Klimov, Oleg and Hesse, Chris and Kim, Taehoon and Schulman, John","journal":"arXiv preprint arXiv:1812.02341","year":"2018","url":"https://openai.com/blog/quantifying-generalization-in-reinforcement-learning/","type":"article"}],["procgen",{"title":"Leveraging Procedural Generation to Benchmark Reinforcement Learning","author":"Cobbe, Karl and Hesse, Christopher and Hilton, Jacob and Schulman, John","year":"2019","url":"https://openai.com/blog/procgen-benchmark/","type":"article"}],["ppo",{"title":"Proximal policy optimization algorithms","author":"Schulman, John and Wolski, Filip and Dhariwal, Prafulla and Radford, Alec and Klimov, Oleg","journal":"arXiv preprint arXiv:1707.06347","year":"2017","url":"https://openai.com/blog/openai-baselines-ppo/","type":"article"}],["gae",{"title":"High-dimensional continuous control using generalized advantage estimation","author":"Schulman, John and Moritz, Philipp and Levine, Sergey and Jordan, Michael and Abbeel, Pieter","journal":"arXiv preprint arXiv:1506.02438","year":"2015","url":"https://arxiv.org/pdf/1506.02438.pdf","type":"article"}],["impala",{"title":"IMPALA: Scalable distributed deep-RL with importance weighted actor-learner architectures","author":"Espeholt, Lasse and Soyer, Hubert and Munos, Remi and Simonyan, Karen and Mnih, Volodymir and Ward, Tom and Doron, Yotam and Firoiu, Vlad and Harley, Tim and Dunning, Iain and others","journal":"arXiv preprint arXiv:1802.01561","year":"2018","url":"https://arxiv.org/pdf/1802.01561.pdf","type":"article"}],["imagenet",{"title":"Imagenet: A large-scale hierarchical image database","author":"Deng, Jia and Dong, Wei and Socher, Richard and Li, Li-Jia and Li, Kai and Fei-Fei, Li","booktitle":"Computer Vision and Pattern Recognition, 2009. CVPR 2009. IEEE Conference on","pages":"248--255","year":"2009","organization":"IEEE","doi":"10.1109/cvprw.2009.5206848","url":"http://www.image-net.org/papers/imagenet_cvpr09.pdf","type":"inproceedings"}],["googlenet",{"title":"Going deeper with convolutions","author":"Szegedy, Christian and Liu, Wei and Jia, Yangqing and Sermanet, Pierre and Reed, Scott and Anguelov, Dragomir and Erhan, Dumitru and Vanhoucke, Vincent and Rabinovich, Andrew","booktitle":"Proceedings of the IEEE conference on computer vision and pattern recognition","pages":"1--9","year":"2015","url":"https://arxiv.org/pdf/1409.4842.pdf","type":"inproceedings"}],["attribution1",{"title":"Deep inside convolutional networks: Visualising image classification models and saliency maps","author":"Simonyan, Karen and Vedaldi, Andrea and Zisserman, Andrew","journal":"arXiv preprint arXiv:1312.6034","year":"2013","url":"https://arxiv.org/pdf/1312.6034.pdf","type":"article"}],["attribution2",{"title":"Visualizing and understanding convolutional networks","author":"Zeiler, Matthew D and Fergus, Rob","booktitle":"European conference on computer vision","pages":"818--833","year":"2014","organization":"Springer","url":"https://arxiv.org/pdf/1311.2901.pdf","type":"inproceedings"}],["attribution3",{"title":"Striving for simplicity: The all convolutional net","author":"Springenberg, Jost Tobias and Dosovitskiy, Alexey and Brox, Thomas and Riedmiller, Martin","journal":"arXiv preprint arXiv:1412.6806","year":"2014","url":"https://arxiv.org/pdf/1412.6806.pdf","type":"article"}],["attribution4",{"title":"Interpretable explanations of black boxes by meaningful perturbation","author":"Fong, Ruth C and Vedaldi, Andrea","booktitle":"Proceedings of the IEEE International Conference on Computer Vision","pages":"3429--3437","year":"2017","url":"http://openaccess.thecvf.com/content_ICCV_2017/papers/Fong_Interpretable_Explanations_of_ICCV_2017_paper.pdf","type":"inproceedings"}],["attribution5",{"title":"PatternNet and PatternLRP--Improving the interpretability of neural networks","author":"Kindermans, Pieter-Jan and Schutt, Kristof T and Alber, Maximilian and Muller, Klaus-Robert and Dahne, Sven","journal":"stat","volume":"1050","pages":"16","year":"2017","url":"https://arxiv.org/pdf/1705.05598.pdf","type":"article"}],["attribution6",{"title":"The (un)reliability of saliency methods","author":"Kindermans, Pieter-Jan and Hooker, Sara and Adebayo, Julius and Alber, Maximilian and Schutt, Kristof T and Dahne, Sven and Erhan, Dumitru and Kim, Been","booktitle":"Explainable AI: Interpreting, Explaining and Visualizing Deep Learning","pages":"267--280","year":"2019","publisher":"Springer","url":"https://arxiv.org/pdf/1711.00867.pdf","type":"incollection"}],["featurevis1",{"title":"Visualizing higher-layer features of a deep network","author":"Erhan, Dumitru and Bengio, Yoshua and Courville, Aaron and Vincent, Pascal","journal":"University of Montreal","volume":"1341","number":"3","pages":"1","year":"2009","url":"https://www.researchgate.net/profile/Aaron_Courville/publication/265022827_Visualizing_Higher-Layer_Features_of_a_Deep_Network/links/53ff82b00cf24c81027da530.pdf","type":"article"}],["featurevis2",{"title":"Deep neural networks are easily fooled: High confidence predictions for unrecognizable images","author":"Nguyen, Anh and Yosinski, Jason and Clune, Jeff","booktitle":"Proceedings of the IEEE conference on computer vision and pattern recognition","pages":"427--436","year":"2015","url":"https://www.cv-foundation.org/openaccess/content_cvpr_2015/papers/Nguyen_Deep_Neural_Networks_2015_CVPR_paper.pdf","type":"inproceedings"}],["featurevis3",{"title":"Inceptionism: Going deeper into neural networks","author":"Mordvintsev, Alexander and Olah, Christopher and Tyka, Mike","journal":"Google Research Blog","year":"2015","url":"https://research.googleblog.com/2015/06/inceptionism-going-deeper-into-neural.html","type":"article"}],["featurevis4",{"title":"Plug & play generative networks: Conditional iterative generation of images in latent space","author":"Nguyen, Anh and Clune, Jeff and Bengio, Yoshua and Dosovitskiy, Alexey and Yosinski, Jason","booktitle":"Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition","pages":"4467--4477","year":"2017","url":"http://openaccess.thecvf.com/content_cvpr_2017/papers/Nguyen_Plug__Play_CVPR_2017_paper.pdf","type":"inproceedings"}],["featurevisadversarial",{"title":"Intriguing properties of neural networks","author":"Szegedy, Christian and Zaremba, Wojciech and Sutskever, Ilya and Bruna, Joan and Erhan, Dumitru and Goodfellow, Ian and Fergus, Rob","journal":"arXiv preprint arXiv:1312.6199","year":"2013","url":"https://arxiv.org/pdf/1312.6199.pdf","type":"article"}],["adversarialtraining",{"title":"Towards deep learning models resistant to adversarial attacks","author":"Madry, Aleksander and Makelov, Aleksandar and Schmidt, Ludwig and Tsipras, Dimitris and Vladu, Adrian","journal":"arXiv preprint arXiv:1706.06083","year":"2017","type":"article"}],["advexfeatures",{"title":"Adversarial examples are not bugs, they are features","author":"Ilyas, Andrew and Santurkar, Shibani and Tsipras, Dimitris and Engstrom, Logan and Tran, Brandon and Madry, Aleksander","journal":"arXiv preprint arXiv:1905.02175","year":"2019","url":"https://gradientscience.org/adv/","type":"article"}],["advexfeaturesdiscussion",{"author":"Engstrom, Logan and Gilmer, Justin and Goh, Gabriel and Hendrycks, Dan and Ilyas, Andrew and Madry, Aleksander and Nakano, Reiichiro and Nakkiran, Preetum and Santurkar, Shibani and Tran, Brandon and Tsipras, Dimitris and Wallace, Eric","title":"A Discussion of 'Adversarial Examples Are Not Bugs, They Are Features'","journal":"Distill","year":"2019","note":"https://distill.pub/2019/advex-bugs-discussion","doi":"10.23915/distill.00019","type":"article"}],["doubledescent",{"title":"Reconciling modern machine-learning practice and the classical bias--variance trade-off","author":"Belkin, Mikhail and Hsu, Daniel and Ma, Siyuan and Mandal, Soumik","journal":"Proceedings of the National Academy of Sciences","volume":"116","number":"32","pages":"15849--15854","year":"2019","publisher":"National Acad Sciences","url":"http://www.cs.columbia.edu/~djhsu/papers/biasvariance-pnas.pdf","type":"article"}],["capturetheflag",{"title":"Human-level performance in 3D multiplayer games with population-based reinforcement learning","author":"Jaderberg, Max and Czarnecki, Wojciech M and Dunning, Iain and Marris, Luke and Lever, Guy and Castaneda, Antonio Garcia and Beattie, Charles and Rabinowitz, Neil C and Morcos, Ari S and Ruderman, Avraham and others","journal":"Science","volume":"364","number":"6443","pages":"859--865","year":"2019","publisher":"American Association for the Advancement of Science","url":"https://deepmind.com/blog/article/capture-the-flag-science","type":"article"}],["rubik",{"title":"Solving Rubik's Cube with a Robot Hand","author":"Akkaya, Ilge and Andrychowicz, Marcin and Chociej, Maciek and Litwin, Mateusz and McGrew, Bob and Petron, Arthur and Paino, Alex and Plappert, Matthias and Powell, Glenn and Ribas, Raphael and others","journal":"arXiv preprint arXiv:1910.07113","year":"2019","url":"https://openai.com/blog/solving-rubiks-cube/","type":"article"}],["dota",{"title":"Dota 2 with Large Scale Deep Reinforcement Learning","author":"Berner, Christopher and Brockman, Greg and Chan, Brooke and Cheung, Vicki and Dębiak, Przemyslaw and Dennison, Christy and Farhi, David and Fischer, Quirin and Hashme, Shariq and Hesse, Chris and others","journal":"arXiv preprint arXiv:1912.06680","year":"2019","url":"https://openai.com/projects/five/","type":"article"}],["rmo",{"title":"Video Interface: Assuming Multiple Perspectives on a Video Exposes Hidden Structure","author":"Ochshorn, Robert M","year":"2017","url":"https://rmozone.com/snapshots/2017/10/rmo-at-google/#chew","type":"article"}]]</script></d-bibliography>

<script type="text/javascript" src="index.bundle.js"></script>
<distill-footer>
<style>

:host {
  color: rgba(255, 255, 255, 0.5);
  font-weight: 300;
  padding: 2rem 0;
  border-top: 1px solid rgba(0, 0, 0, 0.1);
  background-color: hsl(180, 5%, 15%); /*hsl(200, 60%, 15%);*/
  text-align: left;
  contain: content;
}

.footer-container .logo svg {
  width: 24px;
  position: relative;
  top: 4px;
  margin-right: 2px;
}

.footer-container .logo svg path {
  fill: none;
  stroke: rgba(255, 255, 255, 0.8);
  stroke-width: 3px;
}

.footer-container .logo {
  font-size: 17px;
  font-weight: 200;
  color: rgba(255, 255, 255, 0.8);
  text-decoration: none;
  margin-right: 6px;
}

.footer-container {
  grid-column: text;
}

.footer-container .nav {
  font-size: 0.9em;
  margin-top: 1.5em;
}

.footer-container .nav a {
  color: rgba(255, 255, 255, 0.8);
  margin-right: 6px;
  text-decoration: none;
}

</style>

<div class="footer-container">

  <a href="/" class="logo">
    <svg viewBox="-607 419 64 64">
  <path d="M-573.4,478.9c-8,0-14.6-6.4-14.6-14.5s14.6-25.9,14.6-40.8c0,14.9,14.6,32.8,14.6,40.8S-565.4,478.9-573.4,478.9z"></path>
</svg>

    Distill
  </a> is dedicated to clear explanations of machine learning

  <div class="nav">
    <a href="https://distill.pub/about/">About</a>
    <a href="https://distill.pub/journal/">Submit</a>
    <a href="https://distill.pub/prize/">Prize</a>
    <a href="https://distill.pub/archive/">Archive</a>
    <a href="https://distill.pub/rss.xml">RSS</a>
    <a href="https://github.com/distillpub">GitHub</a>
    <a href="https://twitter.com/distillpub">Twitter</a>
    &nbsp;&nbsp;&nbsp;&nbsp; ISSN 2476-0757
  </div>

</div>

</distill-footer><script>
  (function(i,s,o,g,r,a,m){i['GoogleAnalyticsObject']=r;i[r]=i[r]||function(){
  (i[r].q=i[r].q||[]).push(arguments)},i[r].l=1*new Date();a=s.createElement(o),
  m=s.getElementsByTagName(o)[0];a.async=1;a.src=g;m.parentNode.insertBefore(a,m)
  })(window,document,'script','https://www.google-analytics.com/analytics.js','ga');
  ga('create', 'UA-83741880-1', 'auto');
  ga('send', 'pageview');
</script></body></html>