
<!doctype html>


<a href="https://sabirdvd.github.io" class="button"> <small>↩ </small></a>
<link rel="stylesheet" href="https://use.fontawesome.com/releases/v5.6.1/css/all.css">
<meta charset="utf-8">
<script src="template.v1.js"></script>
<script type="text/front-matter">
  title: "ACL 2022-interesting-papers"
  authors:
  - Ahmed Sabir: https://www.cs.upc.edu/~asabir/
  - PUBLISHED Date
  affiliations:
  - Universitat Politècnica de Catalunya: https://www.upc.edu
  -  5 June  2022




</script>

</script>
<script type="text/javascript">
    function zoom() {
        document.body.style.zoom = "80%" 
    }
</script>

<body onload="zoom()">






 <style>
    .blue-color {
        color:blue;
    }

    .green-color {
        color:green;
    }

    .teal-color {
        color:teal;
    }

    .yellow-color {
    color:yellow;
    }

    .red-color {
        color:red;
    }
   </style>
<head>
    <style>
.noteBoxes
{
	border: 1px solid;
  	border-radius: 5px;
	padding: 10px;
	margin: 10px 0;
	width: 600px;
}

.type1
{
	border-color: #E76F51;
	background-color: rgba(231, 111, 81, 0.1);
}

.type2
{
	border-color: #2A9D8F;
	background-color: rgba(42, 157, 143, 0.1);
}

.type3
{
	border-color: #0096C7;
	background-color: rgba(0, 150, 199, 0.1);
}

.type4
{
	border-color: #00B353;
	background-color: rgba(0, 179, 83, 0.1);
}

.picture
{
	width: 15px;
	padding-right: 10px;
}
    </style>
</head>

    <style>
            .geeks {
                caption-side: bottom;
            }
        </style>

<dt-article class="centered">

  <h2><p align="center"> ACL  \(2020\)  Interesting Papers & Workshop<p></h2>
  <dt-byline></dt-byline>

<head>
  <meta charset="utf-8">
  <meta name="viewport" content="width=device-width">
  <title>MathJax example</title>
  <script src="https://polyfill.io/v3/polyfill.min.js?features=es6"></script>
  <script id="MathJax-script" async
          src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js">
  </script>
</head>
<body>
<p>



  <p>In this short blog post, I will highlight a couple of interesting ideas presented in ACL \(2022\).</p>


<p>&clubs;<strong> Contrastive Visual Semantic Pretraining Magnifies the Semantics of Natural Language Representations </strong><dt-cite key="wolfe2022contrastive"></dt-cite> <a href="https://github.com/wolferobert3/clip_contrastive_acl_2022"><i class="fab fa-github" style='font-size:20px'></i></a></p>


  <p>
This paper investigates the use of CLIP </strong><dt-cite key="radford2021learning"></dt-cite> for a non-visual task (i.e., sentence similarity). CLIP encodes
natural language via GPT\(2\)  encoder for image classification task</strong><dt-cite key="radford2019language"></dt-cite>. The GPT\(2\) is a causal language model that is only trained on next-word prediction.
      This work compares the
    representation between GPT and CLIP from a solely linguistic perspective. Note that the two models are trained on very different objectives.
      What makes CLIP unique is that it can be used as an image classifier "on the fly" using only natural language and pre-defined label class,
      and without any fine-tuning on the data for any task.

</p>

<p> The main idea of this work is to employ the self-similarity  <dt-cite key="ethayarajh2019contextual"></dt-cite>    between contextualized word/sentence embeddings to study
    whether the CLIP contrastive visual object has affected the anisotropy of the original model GPT-\(2\):



</p>

\[ \small
s=\frac{1}{n^{2}-n} \sum_{i} \sum_{j \neq i} \cos \left(\vec{w}_{i}, \vec{w}_{j}\right)
\]

<p> where  \(cos\) refers to cosine similarity, \(n\) refers to the number of word embeddings \(w \) used in the  self-similarity measurement.</p>


<p>Their result shows the high self-similarity in the early layer and the loss of semantic information related to the input
token in the upper layers. These effects are much less pronounced in CLIP than they are in GPT\(2\). That indicated the contrastive
    visual semantic objective has regularizing effects that shape more than the projection of the sentence embedding.
    Table below shows CLIP vs GPT on semantic similarity via cosine distance

</p>


<center>  

<table>
  <tr>

            <td> Model </td>
    <td style="text-align:center">　Spearman’s ρ</td>
  </tr>

     <tr>
        <td>GPT2</td>
           <td style="text-align:center">　0.45 </td>
    </tr>

      <tr>
        <td>CLIP</td>

                  <td style="text-align:center">　<strong>0.73</strong></td>
      </tr>


    <caption> <font color="#6e6e6f"> <small> <small>  SemEval-2017 Semantic Textual Similarity (STS) Benchmark <dt-cite key="cer2017semeval"></dt-cite>. <small>  <small></font> </caption>

  </table>
</center>  

  <p>
    In summary, the final finding of this work is that the visual semantic pre-training is beneficial not only for visual representation but also for
encoding semantic representation of natural language at both word and sentence levels. This is
    because the visual semantic pre-training is also able to capture the semantic information of natural language.

  </p>

  <p>
 </p>

 <hr>

<p>&clubs;<strong> XDBERT-Distilling Visual Information to BERT from  CLIP </strong> <dt-cite key="hsu2022xdbert"></dt-cite></p>

<p> This work proposes to inject BERT with visual information using
distillation techniques from CLIP. In particular, the distill information is from CLIP textual model (CLIP-T) and without
    any images during the inference.   　

</p>


<p>
 The introduced model is a fusion of two pre-trained models 1) BERT (Mask language model and next sentence prediction)
    and, 2) CLIP with contrastive loss. Then after training the BERT model can be used  alone  for downstream tasks via fine-tuning as shown
    in the figure below.


</p>

      <figure>
  <img src="XDBERT.jpg" alt="Trulli" style="width:95%">
  <figcaption> Figure. Illustration of the proposed process to inject BERT with visual information. (a) Pre-training stage,
      (2) Adaptation stage, and then (3) Fine-tuning on downstream tasks.  </figcaption>
</figure>


	    <center>  
<table>
  <tr>

            <td> Model </td>
    <td style="text-align:center">　STSB</td>
      <td style="text-align:center">　QQP</td>

  </tr>

     <tr>
           <td>　CLIP-T </td>
          <td style="text-align:center">　22.07</td>
         <td style="text-align:center">　- </td>
    </tr>

          <tr>
           <td>　BERT-b </td>
          <td style="text-align:center">　88.67</td>
         <td style="text-align:center">　89.51 </td>
    </tr>

    <tr>
           <td>　XDBERT-b </td>
          <td style="text-align:center">　<strong>92.78 </strong> </td>
         <td style="text-align:center">　 <strong>88.57 </strong> </td>
    </tr>


    <caption> <font color="#6e6e6f"> <small> <small>  SemEval-2017 Semantic Textual Similarity (STS) Benchmark <dt-cite key="cer2017semeval"></dt-cite>. <small>  <small></font> </caption>

  </table>
</center>
	    
<p> The table above shows the benefits of these visual representations in non-visual downstream tasks such as STS-B and  (Quora Question Pairs identification) QQP tasks.  </p>

</p>

 <hr>

<p>&clubs; <strong>Sentence-T\(5\): Scalable Sentence Encoders </strong><dt-cite key="ni2021sentence"></dt-cite> <a href="https://github.com/google-research/text-to-text-transfer-transformer/blob/main/README.md#released-model-checkpoints"><i class="fab fa-github" style='font-size:20px'></i></a></strong></p>

<p> This approach modifies  the T\(5\) (seq-to-seq) <dt-cite key="raffel2019exploring"></dt-cite> to be utilized  for sentence embeddings tasks. The model relies
on contrastive learning to pull a positive sentence together while pushing away the negative one.
    Note that, unlike BERT <dt-cite key="Jacob:19"></dt-cite> the model that uses the CLS token at beginning of each sentence,
    T\(5\) as seq-to-seq assumes the model is aware of the semantics of the entire sentence when generating the prediction.  The figure
    below shows the original  T\(5\) and the proposed different architectures for sentence embeddings tasks.

</p>


<p> The Sentence-T\(5\) can be used to learn text embeddings with just four lines of code from the Sentence Transformer library: </p>

<dt-code block language="python">

from sentence_transformers import SentenceTransformer, models
word_embedding_model = models.Transformer('t5-base', max_seq_length=256)
pooling_model = models.Pooling(word_embedding_model.get_word_embedding_dimension())
model = SentenceTransformer(modules=[word_embedding_model, pooling_model])
</dt-code>




      <figure>
  <img src="T5S.jpg" alt="Trulli" style="width:90%">
  <figcaption>Figure. Illustration of architecture diagrams of the original T5 (a) and three ST5 variants with encoders only (b, c), and in (d) encoder-decoder model.</figcaption>
</figure>


<p>The training is done in two-stage training via dual encoder training: (1) fine-tune on semi-structured data
    (e.g., Community QA), and then (2) fine-tune on human-labeled data (e.g., Natural Language Inference). The model uses  in-batch sampled softmax contrastive loss  <dt-cite key="henderson2017efficient"></dt-cite>: </p>


\[ \small
\mathcal{L}=\frac{e^{\operatorname{sim}\left(v_{i}, v_{i}^{+}\right) / \tau}}{\sum_{j \in \mathcal{B}} e^{\operatorname{sim}\left(v_{i}, v_{j}^{+}\right) / \tau}+e^{\operatorname{sim}\left(v_{i}, v_{j}^{-}\right) / \tau}}
\]

<p>where \(v_{i}\) is the input sentence and  \(v_{i}^{+}\) is the semantically related sentence, and  \(v_{j}^{-}\) is the negative
    sentence provided by an example from \(v_{i}\), \( \mathcal{B}\) is the mini-batch of examples and \(\tau\) is the softmax temperature. </p>


 <hr>

<p>&clubs;<strong> A good Prompt Is Worth Millions of Parameters: for Vision-Language Models <dt-cite key="jin2021good"></dt-cite> <a href="https://github.com/woojeongjin/FewVLM"><i class="fab fa-github" style='font-size:20px'></i></a></strong></p>

<p>

    GPT-\(3\) (\(175\)M) <dt-cite key="brown2020language"></dt-cite> like a large language model shows good performance on zero and few-shot learning tasks.
    The Zero-shot learning is when the model predicts the answer given only natural language descriptions of
    the task (without any gradient updates).  The one-shot in addition to the zero shot learning the model sees
    a single example of the task (and also without any gradient update) as shown the following examples:


</p>

		    
<p> <small> <strong> Zero-shot:</strong> </small> </p>
		    <center>
			      
<p class="noteBoxes type3"  align="left">  <TT> <small>  <font color="#6495ed"> Translate English to French: </font>  </small>  </TT>     <font color="#6e6e6f">  <small>  <small> <- task description </small> </small> </font>
 <br>
   <font color="#c71585"> <TT>  <small>Cheese  </small> </TT> </font>   <font color="#6e6e6f">  <small>  <small> <- prompt </small> </small> </font>
</p>
</center>

	  
<p> <small> <strong> One-shot:</strong> </small> </p>
	 <center>
<p class="noteBoxes type3"  align="left">  <TT> <small>  <font color="#6495ed"> Translate English to French: </font>  </small>  </TT>     <font color="#6e6e6f">  <small>  <small> <- task description </small> </small> </font>
 <br>
 <TT> <small>  <font color="#6495ed"> sea otter ==> loutre de mer  </font>  </small>  </TT>     <font color="#6e6e6f">  <small>  <small> <- example </small> </small> </font>

    <br>
   <font color="#c71585"> <TT>  <small>Cheese  </small> </TT> </font>   <font color="#6e6e6f">  <small>  <small> <- prompt </small> </small> </font>
</p>
</center>


<p>This work proposes T\(5\) based (smaller version) few-shot learner (\(220\)M/\(740\)M) for language and vision tasks. The model uses  two pre-training objectives:
    (1)  Mask Language Model (fill the mask word) and (2) Prefix Language Model  (PrefixLM) that predict the next sentence, e.g.,  <em> a lady walking next to bicycle</em> and
    pre-fix objectives generate the next sentence <em>  carrying  an umbrella.</em>

       </p>



<p> The finding of this work are: (1) Prompts influence zero-shot performance, and (2) Noisy prompts learn as quickly as hand-crafted prompts for a given larger training dataset.
    (3) Mask Language Modeling helps VQA task while PrefixLM boosts captioning performance.

</p>

 <hr>



<p>&clubs; <strong>Unsupervised Corpus Aware Language Model Pre-training for Dense Passage Retrieval</strong><dt-cite key="gao2021unsupervised"></dt-cite><a href="https://github.com/luyug/Condenser"><i class="fab fa-github" style='font-size:20px'></i></a></p>

<p> Dense retrieval using a pre-trained model is a straightforward process, a query \( q \)  and positive sample and a set of negative samples \( d_{1}, d_{2},.. \)  to a contrastive loss function:  </p>


\[ \small
\mathcal{L}=-\log \frac{\exp \left(s\left(q, d^{+}\right)\right)}{\exp \left(s\left(q, d^{+}\right)\right)+\sum_{l} \exp \left(s\left(q, d_{l}^{-}\right)\right)}
\]

<p> However, that results in poor performance and a complicated fine-tuning pipeline and thus a hard optimization problem.
This work introduces  (Pre-training <strong>con</strong>dition on <strong>dense </strong> <strong>rep </strong>resentations)  <strong>Condenser</strong>
    a Transformer with asymmetric connections method that removes the overhead of large training with noisy labels and complicated processes to have one robust model.
    In particular, an architecture that  (1) enhances BERT  [<tt>CLS</tt>] representation and forces it to play an essential
    role in the pre-training as a dense retrieval model with an improved
    local noise resistance, and (2) a corpus level contrastive learning that warms up the global embedding mapping space.</p>


<p>
<strong>Condenser</strong> consists of three Transformer encoder layers:  (\( 1\)) Early backbone, (\( 2\)) Late backbone, (\( 3 \)) Condenser head layers.
    The first two are typical transformer encoders. The condenser layers inputs a [<tt>CLS</tt>]
    vecctor \(x\) from <em>late</em>  layers and token vector from <em>early</em> layer:


</p>


\[
\small
\left[h_{c l s}^{c d} ; h^{c d}\right]=\text { Condenser }_{\text {head }}\left(\left[h_{c l s}^{\text {late }} ; h^{\text {early }}\right]\right)
\]

<p> The pre-training is done with Mask Language Model objective based on the head layer output:

</p>
\[
\small
\mathcal{L}_{\mathrm{mlm}}=\sum_{i \in \text { masked }} \text { CrossEntropy }\left(W h_{i}^{c d}, x_{i}\right)
\]

<p>The condenser head layer forces [<tt>CLS</tt>] to learn information aggregation (1) in  <em>late layers</em>  to participate  as language
    model, and (2) to create a stronger [<tt>CLS</tt>] without the noise label. </p>

<p> Then on top of the condenser head, a Corpus Aware Contrastive Learning Object is used as follows:
    (1) given a set of documents from the target search corpus \(d_{1}, d_{2}, d_{k}\), (2) pair of spans are extracted  from each
     \(s_{11}, s_{12}, s_{k}\), which is treated as pseudo passages, and (3) finally everything  is encoded  \(s_{ij}\) ->   \(h_{ij}\) into the model with a loss function:




\[
\small
\mathcal{L}_{i j}^{c o}=-\log \frac{\exp \left(\left\langle h_{i 1}, h_{i 2}\right\rangle\right)}{\sum_{k=1}^{n} \sum_{l=1}^{2} \mathbb{I}_{i j \neq k l} \exp \left(\left\langle h_{i j}, h_{k l}\right\rangle\right)}

\]



<p> The loss function is similar to the SimCLR <dt-cite key="geigle2021retrieve"></dt-cite> and word2vec NCE  <dt-cite key="geigle2021retrieve"></dt-cite> but here the learning is done over span vector.
    Finally, the two losses are combined (MLM  and Contrastive  Loss): </p>

\[
\small
\mathcal{L}=\frac{1}{2 n} \sum_{i=1}^{n} \sum_{j=1}^{2}\left[\mathcal{L}_{i j}^{\mathrm{mlm}}+\mathcal{L}_{i j}^{c o}\right]

\]
 <hr>

<p>&clubs; <strong> Retrieve Fast, Rerank Smart: Joint Approaches for Improved Cross-Modal Retrieval </strong><dt-cite key="geigle2021retrieve"></dt-cite><a href="https://github.com/UKPLab/MMT-Retrieval"><i class="fab fa-github" style='font-size:20px'></i></a></p>


<p>

   This paper proposes a combined bi and cross-encoder,
and re-rank setup that are trained jointly for cross model retrieval scenarios. The problem with cross-encoder is that model needs a
forward pass for all tokens to retrieve the most related image, which is too slow in query search.

    The other option is the bi-encoder which uses fast cosine search. However, the performance is limited and doesn't match the cross-encoder performance.

      The concept  proposed in this work can be divided into two parts (1) quick retrieval of top-\( k\) with bi-encoders, and then (2) re-ranking of top-\( k\) via cross-encoders. By doing this, the model
can achieve better state-of-the-art performance with fast retrieval.

</p>

<p> The cross-encoder is finetune with both image and text using Binary Cross-Entropy (BCE) loss: </p>

\[
\small
\mathcal{L}_{\mathrm{CE}}(i, c)=-(y \log \mathrm{p}(i, c)+(1-y) \log (1-\mathrm{p}(i, c)))
\]

<p> \(p(i, c)\)  refers to the probability of the combined input image \(i\) and the caption \(c\). At
   retrieval, all \((i, c)\) need to be processed and re-ranked by  \(p(i, c)\) probability. For given a
    text query \(c\), the model retrieves  the single  most related image \(i\) from image collection \(I\):

</p>

\[
\small
\arg \max (\mathrm{p}(i, c), \forall i \in I)
\]

<p> For the bi-encoders, each image and text caption are passed separately through the pre-trained Transformer model. Where \((i, c)\) are positive
    image-caption pairs from the training corpus while \(c^{\prime}\) and \(i^{\prime}\) are negative examples sampled from the same corpus
     image-caption pairs/instances \((i, c^{\prime})\) and \((i^{\prime}, c)\) but not seen by the model:
</p>

\[
\small
\begin{aligned} \mathcal{L}_{\mathrm{BE}}(i, c)=& {\left[\cos \left(\mathbf{i}, \mathbf{c}^{\prime}\right)-\cos (\mathbf{i}, \mathbf{c})+\alpha\right]^{+} } \\ &+\left[\cos \left(\mathbf{i}^{\prime}, \mathbf{c}\right)-\cos (\mathbf{i}, \mathbf{c})+\alpha\right]^{+} \end{aligned}


\]

<p >where \([\cdot]^{+}=\max (0, \cdot)\) , \(\alpha\)  defines a margin, and \(i^{\prime}\) and \(c^{\prime}\) are embeddings of the negatives batch of image and caption. </p>

<!--
<p> Limitation of Bi-encoder are : 1) not  seen durining pre-training and during fine-tuning</p>
-->

<!-- Write your comments here -->


 <hr>

<p>&clubs;<strong> Predicate-Argument Based Bi-Encoder for Paraphrase Identification</strong><dt-cite key="peng2022predicate"></dt-cite></p>



<p>
    Paraphrase identification is a task that involves identifying whether a pair of two
    sentences express the same semantic meanings, as shown in the following example:

</p>

	 <center>
<p class="noteBoxes type4" align="left">

<small>  a) Marriage equality law passed in Rhode Island  </small> <br>

<small>  b) Rhode Island becomes the 10th state to enact
marriage equality </small>
</p>
		  </center>

<p>
   Although cross-encoder achieved SOTA performance across different benchmarks, they are more complex and required more resources.
This work proposes an enhancement to a less complex bi-encoder-based model (e.g., SBERT) which is have been adopted in many sentence pair tasks such as semantic similarity and neural retrieval.
The model consists of: a BERT encoder, Predicate Argument Spans layer, Aggregation layer, and finally a Concatenation layer with SBERT: </p>


<p> <strong> BERT: </strong> Each sentence is first fed into BERT with mean pooling layer to extract fixed contextual representation.</p>

<p><strong> Predicate Argument Spans (PAS):</strong> Then, the mean pooled BERT contextual
    representation is fed into BERT-based Semantic Role Labeling (SRL) tagger to obtain predicates and relevant arguments. After that,
    the predicted output is gathered to generate predicate-argument spans. For example:
</p>
 <center>
<p class="noteBoxes type4" align="left">

<small>   He slices tomatoes in the kitchen  <font color="brown"><small>(He, slices), (slices, tomatoes), (slices, in, the, kitchen)</font>  </small> </small>
</p>
 </center>
<p>

By looking at this sentence, the  generated predicate slices the sentence into three arguments
<em> he, tomatoes and in the kitchen </em>. By adopting this strategy, the authors managed to form three predicate-argument spans  and split
them into   individual words (<em>He, slices</em>), (<em>slices, tomatoes</em>), (<em>slices, in, the, kitchen</em>). By doing this, each given sentence will obtain all potential predicate-argument spans.

 Note that, if the model could not capture any predicate-argument structure, the mean-pooled sentence representation with all tokens  is used.
</p>


<p> <strong>Aggregation:</strong>  After obtaining all the spans (predicate-argument) a BERT model is employed to

    represent them as word tokens. Given  predicate-argument span sequence
    in the sentence \(s=\left\{s_{1}, s_{2}, \ldots, s_{N}\right\}\), where \(N\) is the number of spans at every
    span \(s_{i}\)  consisting of token \(x=\left\{x_{1}, \ldots, x_{l}\right\}\) to generate the span \(s_{i}\),
    a dense vector with mean pooling \(h_{i}\) all over the tokens:


\[
     h_{i}=\operatorname{MeanPooling}\left(x\right)

\]

   <p> where the representation for each span sequence \(h_{i} \in \mathbb{R}^{D}\) is represented as \(h=\left\{h_{1}, h_{2}, \ldots, h_{N}\right\}\).
    Then, all information is  aggregated using a self-attentive mechanism <dt-cite key="sun2020self"></dt-cite>.





</p>

<p><strong>Connect BERT and Aggregation:</strong> Finally, to get the final sentence representation the authors concatenated both the mean pooling-based
    sentence representation  (e.g., SBERT) and all the generated span-based sentence representations.</p>

 <hr>

<p>&clubs; <strong> BitFit: Simple Parameter-efficient Fine-tuning for Transformer </strong> <dt-cite key="zaken2021bitfit"></dt-cite> <a href="https://github.com/benzakenelad/BitFit"><i class="fab fa-github" style='font-size:20px'></i></a></p>

<p> This work answer an important research question: Do we really need to fine-tune all parameters for any downstream task? they
proposed only Bias-terms fine-tuning for Transformer model (Masked-Language Modeling) which is less than 0.1 of the model parameters.
    The idea is to (1) Freeze most of the transformer-encoder parameters and (2) fine-tune only the Bias-terms. Let \(L\) be  the BERT encoder layers, and \(M\) be the self-attention
head, where each self-attention head \(m,l\) has key, query, and value encoder taking linear layers form:

\[
    \small
\begin{aligned}
&\mathbf{Q}^{m, \ell}(\mathbf{x})=\color{blue}{\mathbf{W}_{q}^{m, \ell}} \color{black}{\mathbf{x}}+\color{brown}{\mathbf{b}_{q}^{m, \ell}} \\
&\mathbf{K}^{m, \ell}(\mathbf{x})=\color{blue}{\mathbf{W}_{k}^{m, \ell}} \color{black}{\mathbf{x}}+\color{brown}{\mathbf{b}_{k}^{m, \ell}} \\
&\mathbf{V}^{m, \ell}(\mathbf{x})==\color{blue}{\mathbf{W}_{v}^{m, \ell}} \color{black}{\mathbf{x}}+\color{brown}{\mathbf{b}_{v}^{m, \ell}}
\end{aligned}
   \]

<p>

Where \(\mathbf{x}\)  is the output of the former encoder layer (for the first encoder layer \(\mathbf{x}\) is the output of the embedding layer).
    These are then combined using an attention mechanism that does not involve new parameters:
</p>


\[
\small
    \mathbf{h}_{1}^{\ell}=\operatorname{att}\left(\mathbf{Q}^{1, \ell}, \mathbf{K}^{1, \ell}, \mathbf{V}^{1, \ell}, . ., \mathbf{Q}^{m, \ell}, \mathbf{K}^{m, \ell}, \mathbf{V}^{m, l}\right)

    \]

<p> And then everything is fed into an MLP with Layer-Norm (\(LN\)): </p>

 \[
\small
    \begin{aligned}
\mathbf{h}_{2}^{\ell} &=\text {Dropout}\left(\color{blue}{\mathbf{W}_{m_{1}}^{\ell}} \cdot \color{black}{\mathbf{h}_{1}^{\ell}}+\textcolor{brown}{\mathbf{b}_{m_{1}}^{\ell}} \right) \\
\mathbf{h}_{3}^{\ell} &=(\mathbf{x})=\color{blue}{\mathbf{g}_{L N_{1}}^{\ell}} \color{black}{\odot} \frac{\left(\mathbf{h}_{2}^{\ell}+\mathbf{x}\right)-\mu}{\sigma}+\textcolor{brown}{\mathbf{b}_{L N_{1}}^{\ell}} \\
\mathbf{h}_{4}^{\ell} &=\operatorname{GELU}\left(\color{blue}{\mathbf{W}_{m_{2}}^{\ell}} \color{black}{\cdot} \space   \mathbf{h}_{3}^{\ell}+\textcolor{brown}{\mathbf{b}_{m_{2}}^{\ell}}\right) \\
\mathbf{h}_{5}^{\ell} &=\operatorname{Dropout}\left(\color{blue}{\mathbf{W}_{m_{3}}^{\ell}} \color{black}{\cdot} \space \mathbf{h}_{4}^{\ell}+\textcolor{brown}{\mathbf{b}_{m_{3}}^{\ell}}\right) \\
\text { out }^{\ell} &=\color{blue}{\mathbf{g}_{L N_{2}}^{\ell}} \color{black}{\odot} \frac{\left(\mathbf{h}_{5}^{\ell}+\mathbf{h}_{3}^{\ell}\right)-\mu}{\sigma}+\color{brown}{\mathbf{b}_{L N_{2}}^{\ell}}
\end{aligned}


    \]



</p>


<p> The \(\color{blue}{\mathbf{W}_{(\cdot)}^{\ell,(\cdot)}}\), \(\color{brown}{\mathbf{b}_{(\cdot)}^{\ell,(\cdot)}}\) and \(\color{blue}{\mathbf{g}_{(\cdot)}^{\ell}}\) are the network parameters.
Since the bias terms are additive they only correspond to a fraction of the parameters of the network (e.g., BERT large \(0.08\)%).
    This work froze all the parameters  \(\color{blue}{\mathbf{W}}\) and  \(\color{blue}{\mathbf{g}}\) and only
    fine-tune the bias terms \(\color{brown}{\mathbf{b}_{(\cdot)}^{\ell,(\cdot)}}\).

</p>


 <center>
<table>
  <tr>

            <td> Model </td>
      <td style="text-align:center">%Param </td>
    <td style="text-align:center">STSB</td>
      <td style="text-align:center">QQP</td>

  </tr>

     <tr>
           <td>　Full-FT </td>
           <td style="text-align:center">100</td>
          <td style="text-align:center">88.9±0.7</td>
         <td style="text-align:center">87.1±0.1 </td>
    </tr>

          <tr>
           <td>　BitFit </td>
                 <td style="text-align:center"> 0.09</td>
          <td style="text-align:center">　<strong>89.2±0.2</strong></td>
         <td style="text-align:center">84.0±0.2</td>
    </tr>



    <caption> <font color="#6e6e6f"> <small> <small>  Fine-tuning using a subset of the bias parameters BERT-base. <small>  <small></font> </caption>

  </table>
 </center>
	    
<p> The table shows that the model still achieves a good or better result with only 0.09% of parameters being fine-tuned.
 </p>

 <hr>

<p>&clubs; <strong>Self-Diagnosis and Self-Debiasing: A Proposal for Reducing Corpus-Based Bias in NLP</strong><dt-cite key="schick2021self"></dt-cite>
    <a href="https://github.com/timoschick/self-debiasing"><i class="fab fa-github" style='font-size:20px'></i></a></p>

<p> This paper shows that large language models can self-diagnosis (SD) the bias with a simple prompting template:  </p>

 <center>
<p class="noteBoxes type3" align="left">  <TT> <small>  <font color="#6495ed"> Naturally: the nurse is a ___   </font>  </small>  </TT>     <font color="#6e6e6f">  <small>  <small> <- prompt </small> </small> </font>
 <br>
 <TT> <small>  <font color="#6495ed"> woman. </font>  </small>  </TT>     <font color="#6e6e6f">  <small>  <small> <- GPT\(2\) </small> </small> </font>

    <br>
   <font color="#c71585"> <TT>  <small> bit of an expert on the topic. </small> </TT> </font>   <font color="#6e6e6f">  <small>  <small> <- GPT\(2\)+SD (prompting: <strong>This is a sexist</strong>)</small></small></font>
</p>
 </center>
<p>  As shown in the example above the model is self-aware of the bias and capable of debiasing its output.
    In this example, feedback is given to the model SD as prompts <em> this sentence is sexist</em>, and then the model
    debias its previous output with an output to fix the bias.</p>


 <hr>

<h2>Workshop </h2>


<p>&rarr; <strong>Memory-assisted prompt editing to improve GPT-\(3\) after deployment</strong> <dt-cite key="madaan2022memory"></dt-cite><a href="https://github.com/madaan/memprompt"><i class="fab fa-github" style='font-size:20px'></i></a></p>

<p> This work proposes a simple a memory enhanced prompting for GPT-\(3\)  as shown in the following example:</p>

 <center>
<p class="noteBoxes type3" align="left">  <TT> <small>  <font color="#6495ed"> What word is similar to good ? </font>  </small>  </TT>     <font color="#6e6e6f">  <small>  <small> <-   User  </small> </small> </font>
 <br>
   <font color="#c71585"> <TT>  <small>The homophone of good is: wood  </small> </TT> </font>   <font color="#6e6e6f">  <small>  <small> <- GPT-\(3\) </small> </small> </font>
 <br>
      <TT> <small>  <font color="#6495ed"> similar to "means" with similar meaning </font>  </small>  </TT>     <font color="#6e6e6f">  <small>  <small> <- User   </small> </small> </font>
  <br>
     <font color="#c71585"> <TT>  <small>Noted (writes to memory)  </small> </TT> </font>   <font color="#6e6e6f">  <small>  <small> <- GPT-\(3\)  </small> </small> </font>
  <br>
      <TT> <small>  <font color="#6495ed"> what word is similar to superised?  </font>  </small>  </TT>     <font color="#6e6e6f">  <small>  <small> <-  User  </small> </small> </font>
      <br>
    <font color="#c71585"> <TT>  <small> (retrieves from memory) amazed  </small> </TT> </font>   <font color="#6e6e6f">  <small>  <small> <- GPT-\(3\) + memory  </small> </small> </font>

</p>
 </center>
<p> The user interacts  with the model, and the model can <strong> memorize </strong> this clarification and then re-call it
    when the user asks a similar question to avoid making the same mistakes.
 </p>

 <hr>
<p>  &rarr; <strong>On the Impact of Data Augmentation on Downstream Performance in NLP <dt-cite key="okimura-etal-2022-impact"></dt-cite></strong></p>



<p> Data augmentation is not as common in NLP as in Computer Vision. There are serval approaches to augment data  in NLP such as back translation
    <dt-cite key="sennrich2015improving"></dt-cite> and random word deletion.

</p>


<p> This paper investigates and answers the research question: Is data augmentation useful in NLP like in Computer Vision?
    The result shows
    that the improvement of data augmentation in \(12\)  NLP datasets with BERT and GPT- \(2\) is <font color="#8b0000">negligible</font>. However, there is a gain for a small dataset after using data augmentation
    (top result with random word detection, back translation, and synonym substitution).
</p>

 <hr>

<p> &rarr; <strong>Few-Shot Learning with Siamese Networks and Label Tuning </strong><dt-cite key="muller2022few"></dt-cite></p>

<p>This work also tackles the problem of Few-shot and Zero-shot learning but in real applications.
The main problem with running these models in real applications is the slow inference time.
</p>



<p>
To tackle these limitations this work (\(1\)) relies on the Siamese Network instead of cross attention network to speed up the inference, and (\(2\)) Label Tuning  (LT) which makes the fine-tuning cheap. The main
    idea of LT is first to compute the embedding for each desired class and then tune them on a subset of the
label examples. More specifically, when we  (the authors)  have data containing \(N\) pairs \(x_{i}\) input and its reference label \(x_{z}\), we can compute
    a matrix of the input embedding  \(X \in \mathcal{R}^{N \times d}\) with its related labels \(Y \in \mathcal{R}^{K \times d}\), where the size of the label is \(K\) and  \(d\) for the dimension.
    Then, we can define a scoring function for each input and label combined:

    \[ \small
    S=X \times Y^{T}\left(S \in \mathcal{R}^{N \times K}\right)
    \]

    and then tune it using cross entropy:
\[
\small
\mathcal{J}^{\prime}=-\frac{1}{N} \sum_{i=1}^{N}\left[S_{i, z_{i}}-\log \sum_{j=1}^{K} e^{S_{i, j}}\right]

\]

<p> To prevent overfitting, regularization techniques are
    used (i.e., Frobenius norm with drop-out), then the model is
    tuned on \(4\)-fold cross-validation on the few shot set.

 <hr>

<p> &rarr; <strong>Eye Gaze and Self-attention: How Humans and Transformers Attend Words in Sentences <dt-cite key="bensemanneye"></dt-cite></strong></p>


<p> Attention in humans is divided into two parts: (\(1\)) overt: looking at parts of a sentence, and (\(2\)) Covert: which is inside the human brain.
    This work tried to compare the correlation  between <strong> overt  attention </strong> which is measurable (ie., eye tracking which part of the sentence is focused on) with transformer model.

    The finding of the correlation between overt human attention and transformer self-attention are:</p>
<p>1. Only on the early layers.</p>
<p>2. It did not depend on the size of the model.</p>
<p>3. The correlation has nothing to do with model performance.</p>

<hr>



<p> &rarr; <strong> Estimating word co-occurrence probabilities using a log-bilinear model </strong> <dt-cite key="futrell2022estimating"></dt-cite><a href="https://github.com/langprocgroup/vectorprob"><i class="fab fa-github" style='font-size:20px'></i></a></p>

<p> This paper recycles the simplest neural language model via log-bilinear (LBL) <dt-cite key="mnih2007three"></dt-cite>  model to estimate the co-occurrence probabilities using word embeddings.
The main idea is to leverage pre-trained embedding to enhance  generalization in small dataset scenarios  based on the \(w \) word and its related context \(c\).
    Given vocabulary size \(V\) a finite target word  \(W \subseteq V\), and dataset of \(N\) pair of word  and context

    \(\left\{\left\langle w_{i}, c_{i}\right\rangle\right\}_{i=1}^{N}\), and a pre-trained word Embedding mapping with  \(E: V \rightarrow \mathbb{R}^{D}\). The
    concept to find a conditional distribution \(q(w \mid c)\) from data sample e.g., a dataset consists sample from
some distribution \(p(w, c)=p(c) p(w \mid c)\):


</p>

\[ \small
\begin{equation}
\begin{aligned}
q(w \mid c) &=\frac{1}{Z(c)} \exp \left\{\phi(\mathbf{w})^{\top} \mathbf{A} \psi(\mathbf{c})\right\} \\
Z(c) &=\sum_{w \in W} \exp \left\{\phi(\mathbf{w})^{\top} \mathbf{A} \psi(\mathbf{c})\right\}
\end{aligned}
\end{equation}

\]


<p> where \(\mathbf{w}=E(w)\) and  \(\mathbf{c}=E(c)\) are the static embeddings of target word \(w\) and context \(c\) respectively.
    The target word encoder \(\phi (\cdot)\) and the context encoder \(\psi(\cdot)\) are parameterized functions via a feed-forward neural network,
    \(A\)
    is the interaction matrix \( K \times L \). The model parameters  \(\phi\), \(\psi\) and \(A\)  are  trained with  cross-entropy  loss:


   </p>

\[ \small
J(\phi, \psi, \mathbf{A})=-\sum_{n=1}^{N} \log q\left(w_{n} \mid c_{n}\right)
\]

<p>Also, the author mentioned that is possible to set \(\psi=\phi\) to use the same function
to encode both the context and target word; which can reduce the number of parameters.

</p>




<div class="d-appendix">
</div>

<dt-appendix>
</dt-appendix>

<script type="BibTeX">
    <pre class="citation long">@article{olah2018the,
  author = {Olah, Chris and Satyanarayan, Arvind and Johnson, Ian and Carter, Shan and Schubert, Ludwig and Ye, Katherine and Mordvintsev, Alexander},
  title = {The Building Blocks of Interpretability},
  journal = {Distill},
  year = {2018},
  note = {https://distill.pub/2018/building-blocks},
  doi = {10.23915/distill.00010}
}</pre>
</script>




<script type="text/bibliography">

@article{ling2022vision,
  title={Vision-Language Pre-Training for Multimodal Aspect-Based Sentiment Analysis},
  author={Ling, Yan and Xia, Rui and others},
  journal={arXiv preprint arXiv:2204.07955},
  year={2022}
}

@article{zaken2021bitfit,
  title={Bitfit: Simple parameter-efficient fine-tuning for transformer-based masked language-models},
  author={Zaken, Elad Ben and Ravfogel, Shauli and Goldberg, Yoav},
  journal={arXiv preprint arXiv:2106.10199},
  url={https://arxiv.org/pdf/2106.10199.pdf},
  year={2021}
}

@article{sennrich2015improving,
  title={Improving neural machine translation models with monolingual data},
  author={Sennrich, Rico and Haddow, Barry and Birch, Alexandra},
  journal={arXiv preprint arXiv:1511.06709},
    url={https://arxiv.org/pdf/1511.06709.pdf},
  year={2015}
}


@article{lu2021fantastically,
  title={Fantastically ordered prompts and where to find them: Overcoming few-shot prompt order sensitivity},
  author={Lu, Yao and Bartolo, Max and Moore, Alastair and Riedel, Sebastian and Stenetorp, Pontus},
  journal={arXiv preprint arXiv:2104.08786},
  url={https://arxiv.org/pdf/2104.08786.pdf},

  year={2021}
}
@article{madaan2022memory,
  title={Memory-assisted prompt editing to improve GPT-3 after deployment},
  author={Madaan, Aman and Tandon, Niket and Clark, Peter and Yang, Yiming},
  journal={arXiv preprint arXiv:2201.06009},
  url={https://arxiv.org/pdf/2201.06009.pdf},
  year={2022}
}

@article{schick2021self,
  title={Self-diagnosis and self-debiasing: A proposal for reducing corpus-based bias in nlp},
  author={Schick, Timo and Udupa, Sahana and Sch{\"u}tze, Hinrich},
  journal={Transactions of the Association for Computational Linguistics},
  volume={9},
  pages={1408--1424},
  year={2021},
  url={https://arxiv.org/pdf/2103.00453.pdf},
  publisher={MIT Press}
}

@article{hsu2022xdbert,
  title={XDBERT: Distilling Visual Information to BERT from Cross-Modal Systems to Improve Language Understanding},
  author={Hsu, Chan-Jan and Lee, Hung-yi and Tsao, Yu},
  journal={arXiv preprint arXiv:2204.07316},
  url={https://arxiv.org/pdf/2204.07316.pdf},
  year={2022}
}

@inproceedings{peng2022predicate,
  title={Predicate-Argument Based Bi-Encoder for Paraphrase Identification},
  author={Peng, Qiwei and Weir, David and Weeds, Julie and Chai, Yekun},
  booktitle={Proceedings of the 60th Annual Meeting of the Association for Computational Linguistics},
  pages={5579--5589},
    url={https://aclanthology.org/2022.acl-long.382.pdf},
  year={2022}
}

@inproceedings{okimura-etal-2022-impact,
    title = "On the Impact of Data Augmentation on Downstream Performance in Natural Language Processing",
    author = "Okimura, Itsuki  and
      Reid, Machel  and
      Kawano, Makoto  and
      Matsuo, Yutaka",
        url={https://aclanthology.org/2022.insights-1.12.pdf},
  year={2022}
      }



@article{chen2022imputing,
  title={Imputing out-of-vocabulary embeddings with LOVE makes language models robust with little cost},
  author={Chen, Lihu and Varoquaux, Ga{\"e}l and Suchanek, Fabian M},
  journal={arXiv preprint arXiv:2203.07860},
  url={https://arxiv.org/pdf/2203.07860.pdf},
  year={2022}
}

@article{sun2020self,
  title={Self-explaining structures improve nlp models},
  author={Sun, Zijun and Fan, Chun and Han, Qinghong and Sun, Xiaofei and Meng, Yuxian and Wu, Fei and Li, Jiwei},
  journal={arXiv preprint arXiv:2012.01786},
  url={https://arxiv.org/pdf/2012.01786.pdf},
  year={2020}
}

@article{wolfe2022contrastive,
  title={Contrastive Visual Semantic Pretraining Magnifies the Semantics of Natural Language Representations},
  author={Wolfe, Robert and Caliskan, Aylin},
  journal={arXiv preprint arXiv:2203.07511},
  url={https://arxiv.org/pdf/2203.07511.pdf},
  year={2022}
}

@article{henderson2017efficient,
  title={Efficient natural language response suggestion for smart reply},
  author={Henderson, Matthew and Al-Rfou, Rami and Strope, Brian and Sung, Yun-Hsuan and Luk{\'a}cs, L{\'a}szl{\'o} and Guo, Ruiqi and Kumar, Sanjiv and Miklos, Balint and Kurzweil, Ray},
  journal={arXiv preprint arXiv:1705.00652},
  url={https://arxiv.org/pdf/1705.00652.pdf},
  year={2017}
}

@article{geigle2021retrieve,
  title={Retrieve fast, rerank smart: Cooperative and joint approaches for improved cross-modal retrieval},
  author={Geigle, Gregor and Pfeiffer, Jonas and Reimers, Nils and Vuli{\'c}, Ivan and Gurevych, Iryna},
  url={https://arxiv.org/pdf/2103.11920.pdf},
  year={2021}
}

@article{gao2021unsupervised,
  title={Unsupervised corpus aware language model pre-training for dense passage retrieval},
  author={Gao, Luyu and Callan, Jamie},
  journal={arXiv preprint arXiv:2108.05540},
  url={https://arxiv.org/pdf/2108.05540.pdf},
  year={2021}
}


@article{tan2022sentence,
  title={A Sentence is Worth 128 Pseudo Tokens: A Semantic-Aware Contrastive Learning Framework for Sentence Embeddings},
  author={Tan, Haochen and Shao, Wei and Wu, Han and Yang, Ke and Song, Linqi},
  journal={arXiv preprint arXiv:2203.05877},
 url={https://arxiv.org/pdf/2203.05877.pdf},
 Github ={https://github.com/Namco0816/PT-BERT},
 ACL = {ACL 2022 (Findings)},
  year={2022}
}

@article{muller2022few,
  title={Few-shot learning with siamese networks and label tuning},
  author={M{\"u}ller, Thomas and P{\'e}rez-Torr{\'o}, Guillermo and Franco-Salvador, Marc},
  journal={arXiv preprint arXiv:2203.14655},
  url={https://arxiv.org/pdf/2203.14655.pdf},
  year={2022}
}

@article{jin2021good,
  title={A Good Prompt Is Worth Millions of Parameters? Low-resource Prompt-based Learning for Vision-Language Models},
  author={Jin, Woojeong and Cheng, Yu and Shen, Yelong and Chen, Weizhu and Ren, Xiang},
  journal={arXiv preprint arXiv:2110.08484},
  url={https://arxiv.org/pdf/2110.08484.pdf},
  year={2021}
}

@article{ethayarajh2019contextual,
  title={How contextual are contextualized word representations? comparing the geometry of BERT, ELMo, and GPT-2 embeddings},
  author={Ethayarajh, Kawin},
  journal={arXiv preprint arXiv:1909.00512},
  year={2019}
}

@article{brown2020language,
  title={Language models are few-shot learners},
  author={Brown, Tom and Mann, Benjamin and Ryder, Nick and Subbiah, Melanie and Kaplan, Jared D and Dhariwal, Prafulla and Neelakantan, Arvind and Shyam, Pranav and Sastry, Girish and Askell, Amanda and others},
  journal={Advances in neural information processing systems},
  volume={33},
  pages={1877--1901},
    url={https://proceedings.neurips.cc/paper/2020/file/1457c0d6bfcb4967418bfb8ac142f64a-Paper.pdf},
  year={2020}
}


@article{evalrank_2022,
  title={Just Rank: Rethinking Evaluation with Word and Sentence Similarities},
  author={Wang, Bin and Kuo, C.-C. Jay and Li, Haizhou},
  journal={arXiv preprint arXiv:2203.02679},
   url={https://arxiv.org/pdf/2203.02679.pdf},
  year={2022}
}

@article{cer2017semeval,
  title={Semeval-2017 task 1: Semantic textual similarity-multilingual and cross-lingual focused evaluation},
  author={Cer, Daniel and Diab, Mona and Agirre, Eneko and Lopez-Gazpio, Inigo and Specia, Lucia},
  journal={arXiv preprint arXiv:1708.00055},
  year={2017}
}

@article{raffel2019exploring,
  title={Exploring the limits of transfer learning with a unified text-to-text transformer},
  author={Raffel, Colin and Shazeer, Noam and Roberts, Adam and Lee, Katherine and Narang, Sharan and Matena, Michael and Zhou, Yanqi and Li, Wei and Liu, Peter J},
  journal={arXiv preprint arXiv:1910.10683},
  year={2019}
}

@article{ni2021sentence,
  title={Sentence-t5: Scalable sentence encoders from pre-trained text-to-text models},
  author={Ni, Jianmo and {\'A}brego, Gustavo Hern{\'a}ndez and Constant, Noah and Ma, Ji and Hall, Keith B and Cer, Daniel and Yang, Yinfei},
  journal={arXiv preprint arXiv:2108.08877},
  url={https://arxiv.org/pdf/2108.08877.pdf},
  year={2021}
}

@article{wu2022noisytune,
  title={NoisyTune: A Little Noise Can Help You Finetune Pretrained Language Models Better},
  author={Wu, Chuhan and Wu, Fangzhao and Qi, Tao and Huang, Yongfeng and Xie, Xing},
  journal={arXiv preprint arXiv:2202.12024},
  url={https://arxiv.org/pdf/2202.12024.pdf},
  year={2022}
}

@article{bensemanneye,
  title={Eye Gaze and Self-attention: How Humans and Transformers Attend Words in Sentences},
  author={Bensemann, Joshua and Peng, Alex Yuxuan and Benavides-Prado, Diana and Chen, Yang and Tan, Neset {\"O}zkan and Corballis, Paul Michael and Riddle, Patricia and Witbrock, Michael},
  url={https://aclanthology.org/2022.cmcl-1.9.pdf},
}

@inproceedings{mnih2007three,
  title={Three new graphical models for statistical language modelling},
  author={Mnih, Andriy and Hinton, Geoffrey},
  booktitle={Proceedings of the 24th international conference on Machine learning},
  pages={641--648},
  year={2007}
}

@inproceedings{futrell2022estimating,
  title={Estimating word co-occurrence probabilities from pretrained static embeddings using a log-bilinear model},
  author={Futrell, Richard},
  booktitle={Proceedings of the Workshop on Cognitive Modeling and Computational Linguistics},
  pages={54--60},
  url={https://openreview.net/pdf?id=HOu4PQtgUWq},
  year={2022}
}

  @article{gregor2015draw,
    title={DRAW: A recurrent neural network for image generation},
    author={Gregor, Karol and Danihelka, Ivo and Graves, Alex and Rezende, Danilo Jimenez and Wierstra, Daan},
    journal={arXivreprint arXiv:1502.04623},
    year={2015},
    url={https://arxiv.org/pdf/1502.04623.pdf},
        }
          
@book{Aho:72,
    author  = {Alfred V. Aho and Jeffrey D. Ullman},
    title   = {The Theory of Parsing, Translation and Compiling},
    year    = "1972",
    volume  = "1",
    publisher = {Prentice-Hall},
    address = {Englewood Cliffs, NJ}
}

@book{APA:83,
    author  = {{American Psychological Association}},
    title   = {Publications Manual},
    year    = "1983",
   publisher = {American Psychological Association},
   address = {Washington, DC}
}

@article{ACM:83,
	author = {Association for Computing Machinery},
	year = "1983",
	journal = {Computing Reviews},
	volume = "24",
	number = "11",
	pages = "503--512"
}

@article{Chandra:81,
	author = {Ashok K. Chandra and Dexter C. Kozen and Larry J. Stockmeyer},
	year = "1981",
	title = {Alternation},
	journal = {Journal of the Association for Computing Machinery},
	volume = "28",
	number = "1",
	pages = "114--133",
	doi = "10.1145/322234.322243",
}

@inproceedings{andrew2007scalable,
  title={Scalable training of {L1}-regularized log-linear models},
  author={Andrew, Galen and Gao, Jianfeng},
  booktitle={Proceedings of the 24th International Conference on Machine Learning},
  pages={33--40},
  year={2007},
}

@book{Gusfield:97,
    author  = {Dan Gusfield},
    title   = {Algorithms on Strings, Trees and Sequences},
    year    = "1997",
    publisher = {Cambridge University Press},
    address = {Cambridge, UK}
}

@inproceedings{borsch2011,
	Address = {Canberra, Australia},
	Author = {Benjamin Borschinger and Mark Johnson},
	Booktitle = {Proceedings of the Australasian Language Technology Association Workshop 2011},
	Month = {December},
	Pages = {10--18},
	Title = {A Particle Filter algorithm for {B}ayesian Wordsegmentation},
	Year = {2011}}

@article{rasooli-tetrault-2015,
    author    = {Mohammad Sadegh Rasooli and Joel R. Tetreault},
    title     = {Yara Parser: {A} Fast and Accurate Dependency Parser},
    journal   = {Computing Research Repository},
    volume    = {arXiv:1503.06733},
    year      = {2015},
    url       = {http://arxiv.org/abs/1503.06733},
    note    = {version 2}
}

@article{Ando2005,
	Acmid = {1194905},
	Author = {Ando, Rie Kubota and Zhang, Tong},
	Issn = {1532-4435},
	Issue_Date = {12/1/2005},
	Journal = {Journal of Machine Learning Research},
	Month = dec,
	Numpages = {37},
	Pages = {1817--1853},
	Publisher = {JMLR.org},
	Title = {A Framework for Learning Predictive Structures from Multiple Tasks and Unlabeled Data},
	Volume = {6},
	Year = {2005}}

@InProceedings{P16-1001,
  author =	"Goodman, James
  	 and Vlachos, Andreas
	     and Naradowsky, Jason",
  title =    "Noise reduction and targeted exploration in imitation learning for      Abstract Meaning Representation parsing    ",
  booktitle = 	    "Proceedings of the 54th Annual Meeting of the Association for      Computational Linguistics (Volume 1: Long Papers)    ",
  year =    "2016",
  publisher =	"Association for Computational Linguistics",
  pages =   "1--11",
  location =	"Berlin, Germany",
  doi =    "10.18653/v1/P16-1001",
  url =    "http://aclweb.org/anthology/P16-1001"
}

@InProceedings{C14-1001,
  author =	"Harper, Mary",
  title = 	"Learning from 26 Languages: Program Management and Science in the Babel Program",
  booktitle = 	"Proceedings of COLING 2014, the 25th International Conference on Computational Linguistics: Technical Papers",
  year =    "2014",
  publisher =	"Dublin City University and Association for Computational Linguistics",
  pages =   "1",
  location =	"Dublin, Ireland",
  url =    "http://aclweb.org/anthology/C14-1001"
}

@inproceedings{Christian:15,
title = {Going deeper with convolutions},
author = {Christian Szegedy},
year = {2015},
month = {06},
pages = {},
booktitle = {The IEEE Conference on Computer Vision and Pattern Recognition (CVPR)}
}
%%%author = {Christian Szegedy and Wei Liu and Yangqing Jia and Pierre Sermanet and Scott Reed and Dragomir Anguelov and Dumitru Erhan and Vincent Vanhoucke and Andrew Rabinovich}


@inproceedings{Christian:15,
  title={Going deeper with convolutions},
  author={Szegedy, Christian and Liu, Wei and Jia, Yangqing and Sermanet, Pierre and Reed, Scott and Anguelov, Dragomir and Erhan, Dumitru and Vanhoucke, Vincent and Rabinovich, Andrew and others},
  year={2015},
  organization={Cvpr}
}

@inproceedings{Chris:15,
  title={Going deeper with convolutions},
  author={Szegedy, Christian and Liu, Wei and Jia, Yangqing and Sermanet, Pierre and Reed, Scott and Anguelov, Dragomir and Erhan, Dumitru and Vanhoucke, Vincent and Rabinovich, Andrew and others},
  year={2015},
  organization={Cvpr}
}
@inproceedings{Szegedy:15,
title={\& Rabinovich, A.(2015). Going deeper with convolutions},
author={Szegedy, C and Liu, W and Jia, Y and Sermanet, P and Reed, S and Anguelov, D},
booktitle={Proceedings of the IEEE conference on computer vision and pattern recognition},
year={2015},
pages={}
}



@inproceedings{Tsung-Yi:14,
  title={Microsoft coco: Common objects in context},
  author={Lin, Tsung-Yi and Maire, Michael and Belongie, Serge and Hays, James and Perona, Pietro and Ramanan, Deva and Doll{\'a}r, Piotr and Zitnick, C Lawrence},
  booktitle={European conference on computer vision},
  pages={},
  year={2014},
  organization={Springer}
}


@inproceedings{Kai:11,
  title={End-to-end scene text recognition},
  author={Wang, Kai and Babenko, Boris and Belongie, Serge},
  booktitle={Computer Vision (ICCV), 2011 IEEE International Conference on},
  pages={},
  year={2011},
  organization={IEEE}
}

@inproceedings{Jeffrey:14,
  title={Glove: Global vectors for word representation},
  author={Pennington, Jeffrey and Socher, Richard and Manning, Christopher},
  booktitle={Proceedings of the 2014 conference on empirical methods in natural language processing (EMNLP)},
  pages={},
  year={2014}
}

@inproceedings{Bolei:14,
  title={Learning deep features for scene recognition using places database},
  author={Zhou, Bolei and Lapedriza, Agata and Xiao, Jianxiong and Torralba, Antonio and Oliva, Aude},
  booktitle={Advances in neural information processing systems},
  pages={},
  year={2014}
}




@article{Andreas:16,
  title={Coco-text: Dataset and benchmark for text detection and recognition in natural images},
  author={Veit, Andreas and Matera, Tomas and Neumann, Lukas and Matas, Jiri and Belongie, Serge},
  journal={arXiv preprint arXiv:1601.07140},
  year={2016}
}

@inproceedings{Tomas:13,
  title={Distributed representations of words and phrases and their compositionality},
  author={Mikolov, Tomas and Sutskever, Ilya and Chen, Kai and Corrado, Greg S and Dean, Jeff},
  booktitle={Advances in neural information processing systems},
  pages={},
  year={2013}
}

@inproceedings{Sergey:03,
  title={Probability from similarity},
  author={Blok, Sergey and Medin, Douglas and Osherson, Daniel},
  booktitle={AAAI Spring Symposium on Logical Formalization of Commonsense Reasoning},
  pages={},
  year={2003}
}

@InProceedings{Pierre:2016,
  author = {Pierre Lison and Jörg Tiedemann},
  title = {OpenSubtitles2016: Extracting Large Parallel Corpora from Movie and TV Subtitles},
  booktitle = {Proceedings of the Tenth International Conference on Language Resources and Evaluation (LREC 2016)},
  year = {2016},
  month = {may},
  date = {23-28},
  location = {Portorož, Slovenia},
  editor = {Nicoletta Calzolari (Conference Chair) and Khalid Choukri and Thierry Declerck and Sara Goggi and Marko Grobelnik and Bente Maegaard and Joseph Mariani and Helene Mazo and Asuncion Moreno and Jan Odijk and Stelios Piperidis},
  publisher = {European Language Resources Association (ELRA)},
  address = {Paris, France},
  isbn = {978-2-9517408-9-1},
  language = {english}
 } 


@inproceedings{Kaiming:16,
  title={Deep residual learning for image recognition},
  author={He, Kaiming and Zhang, Xiangyu and Ren, Shaoqing and Sun, Jian},
  booktitle={Proceedings of the IEEE conference on computer vision and pattern recognition},
  pages={},
  year={2016}
}

@article{David:03,
  title={Latent dirichlet allocation},
  author={Blei, David M and Ng, Andrew Y and Jordan, Michael I},
  journal={Journal of machine Learning research},
  volume={3},
  number={Jan},
  pages={},
  year={2003}
}

         @article{strubell2019energy,
  title={Energy and Policy Considerations for Deep Learning in NLP},
  author={Strubell, Emma and Ganesh, Ananya and McCallum, Andrew},
  journal={arXiv preprint arXiv:1906.02243},
  year={2019}
}

@inproceedings{Yash:16,
  title={Dynamic Lexicon Generation for Natural Scene Images},
  author={Patel, Yash and Gomez, Lluis and Rusinol, Mar{\c{c}}al and Karatzas, Dimosthenis},
  booktitle={European Conference on Computer Vision},
  pages={},
  year={2016},
  organization={Springer}
}


@article{Yunze:17,
  title={Reading Scene Text with Attention Convolutional Sequence Modeling},
  author={Gao, Yunze and Chen, Yingying and Wang, Jinqiao and Lu, Hanqing},
  journal={arXiv preprint arXiv:1709.04303},
  year={2017}
}


@inproceedings{Alex:06,
  title={Connectionist temporal classification: labelling unsegmented sequence data with recurrent neural networks},
  author={Graves, Alex and Fern{\'a}ndez, Santiago and Gomez, Faustino and Schmidhuber, J{\"u}rgen},
  booktitle={Proceedings of the 23rd international conference on Machine learning},
  pages={369--376},
  year={2006},
  organization={ACM}
}

@inproceedings{Anand:12,
  title={Top-down and bottom-up cues for scene text recognition},
  author={Mishra, Anand and Alahari, Karteek and Jawahar, CV},
  booktitle={Computer Vision and Pattern Recognition (CVPR), 2012 IEEE Conference on},
  pages={},
  year={2012},
  organization={IEEE}
}

@article{Baoguang:16,
  title={An end-to-end trainable neural network for image-based sequence recognition and its application to scene text recognition},
  author={Shi, Baoguang and Bai, Xiang and Yao, Cong},
  journal={IEEE transactions on pattern analysis and machine intelligence},
  year={2016},
  publisher={IEEE}
}

@inproceedings{Alessandro:13,
  title={Photoocr: Reading text in uncontrolled conditions},
  author={Bissacco, Alessandro and Cummins, Mark and Netzer, Yuval and Neven, Hartmut},
  booktitle={Proceedings of the IEEE International Conference on Computer Vision},
  pages={},
  year={2013}
}

@inproceedings{Lukas:10,
  title={A method for text localization and recognition in real-world images},
  author={Neumann, Lukas and Matas, Jiri},
  booktitle={Asian Conference on Computer Vision},
  pages={},
  year={2010},
  organization={Springer}
}

@article{Lukas:11,
  title={A method for text localization and recognition in real-world images},
  author={Neumann, Lukas and Matas, Jiri},
  journal={Computer Vision--ACCV 2010},
  pages={770--783},
  year={2011},
  publisher={Springer}
}

@article{Suman:17,
  title={Visual attention models for scene text recognition},
  author={Ghosh, Suman K and Valveny, Ernest and Bagdanov, Andrew D},
  journal={arXiv preprint arXiv:1706.01487},
  year={2017}
}

@inproceedings{Max:14,
  title={Deep Features for Text Spotting.},
  author={Jaderberg, Max and Vedaldi, Andrea and Zisserman, Andrew},
  booktitle={ECCV (4)},
  pages={},
  year={2014}
}

@article{Max:16,
  title={Reading text in the wild with convolutional neural networks},
  author={Jaderberg, Max and Simonyan, Karen and Vedaldi, Andrea and Zisserman, Andrew},
  journal={International Journal of Computer Vision},
  volume={116},
  number={1},
  pages={},
  year={2016},
  publisher={Springer}
}

@article{Max:14b,
  title={Synthetic data and artificial neural networks for natural scene text recognition},
  author={Jaderberg, Max and Simonyan, Karen and Vedaldi, Andrea and Zisserman, Andrew},
  journal={arXiv preprint arXiv:1406.2227},
  year={2014}
}

@inproceedings{Daniel:08,
  author={Lopresti, Daniel},
  title={Optical character recognition errors and their effects on natural language processing}, 
  booktitle={Proceedings of the second workshop on Analytics for noisy unstructured text data},
  pages={},
  year={2008},
  organization={ACM}
}

}
@article{Samuel:15,
  title={A large annotated corpus for learning natural language inference},
  author={Bowman, Samuel R and Angeli, Gabor and Potts, Christopher and Manning, Christopher D},
  journal={arXiv preprint arXiv:1508.05326},
  year={2015}
}

@article{Zhiguo:17,
  title={Bilateral multi-perspective matching for natural language sentences},
  author={Wang, Zhiguo and Hamza, Wael and Florian, Radu},
  journal={arXiv preprint arXiv:1702.03814},
  year={2017}
}

@article{Samuel:14,
  title={Recursive neural networks can learn logical semantics},
  author={Bowman, Samuel R and Potts, Christopher and Manning, Christopher D},
  journal={arXiv preprint arXiv:1406.1827},
  year={2014}
}

@article{Yichen:17,
  title={Natural language inference over interaction space},
  author={Gong, Yichen and Luo, Heng and Zhang, Jian},
  journal={arXiv preprint arXiv:1709.04348},
  year={2017}
}

@inproceedings{Guibin:17,
  title={Ensemble application of convolutional and recurrent neural networks for multi-label text categorization},
  author={Chen, Guibin and Ye, Deheng and Xing, Zhenchang and Chen, Jieshan and Cambria, Erik},
  booktitle={2017 International Joint Conference on Neural Networks (IJCNN)},
  pages={},
  year={2017},
  organization={IEEE}
}

@article{Sepp:97,
  title={Long short-term memory},
  author={Hochreiter, Sepp and Schmidhuber, J{\"u}rgen},
  journal={Neural computation},
  volume={9},
  number={8},
  pages={},
  year={1997},
  publisher={MIT Press}
}

@inproceedings{Xingyou:16,
  title={Combination of convolutional and recurrent neural network for sentiment analysis of short texts},
  author={Wang, Xingyou and Jiang, Weijie and Luo, Zhiyong},
  booktitle={Proceedings of COLING 2016, the 26th International Conference on Computational Linguistics: Technical Papers},
  pages={},
  year={2016}
}



@inproceedings{Gomez:18,
  title     = {Single Shot Scene Text Retrieval},
  author    = {Lluis Gomez, Andres Mafla, Marçal Rusiñol, and Dimosthenis Karatzas},
  booktitle = {ECCV},
  year      = {2018}
}


@article{raffel2015feed,
  title={Feed-forward networks with attention can solve some long-term memory problems},
  author={Raffel, Colin and Ellis, Daniel PW},
  journal={arXiv preprint arXiv:1512.08756},
  year={2015}
}

@article{Chunting:15,
  title={A C-LSTM neural network for text classification},
  author={Zhou, Chunting and Sun, Chonglin and Liu, Zhiyuan and Lau, Francis},
  journal={arXiv preprint arXiv:1511.08630},
  year={2015}
}


@article{Dzmitry:14,
  title={Neural machine translation by jointly learning to align and translate},
  author={Bahdanau, Dzmitry and Cho, Kyunghyun and Bengio, Yoshua},
  journal={arXiv preprint arXiv:1409.0473},
  year={2014}
}

@article{Timothy:16,
  title={Incorporating nesterov momentum into adam},
  author={Dozat, Timothy},
  year={2016}
}

@article{Ahmed:18,
  title={Visual Re-ranking with Natural Language Understanding for Text Spotting},
  author={Sabir, Ahmed and Moreno-Noguer, Francesc and Padr{\'o}, Llu{\'\i}s},
  journal={arXiv preprint arXiv:1810.12738},
  year={2018}
}

@article{Yuval:17,
  title={Mimicking word embeddings using subword rnns},
  author={Pinter, Yuval and Guthrie, Robert and Eisenstein, Jacob},
  journal={arXiv preprint arXiv:1707.06961},
  year={2017}
}

@inproceedings{Ming:16,
  title={Improved representation learning for question answer matching},
  author={Tan, Ming and Dos Santos, Cicero and Xiang, Bing and Zhou, Bowen},
  booktitle={Proceedings of the 54th Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers)},
  volume={1},
  pages={},
  year={2016}
}

@inproceedings{Vinod:10,
  title={Rectified linear units improve restricted boltzmann machines},
  author={Nair, Vinod and Hinton, Geoffrey E},
  booktitle={Proceedings of the 27th international conference on machine learning (ICML-10)},
  pages={},
  year={2010}
}

@article{Sergey:15,
  title={Batch normalization: Accelerating deep network training by reducing internal covariate shift},
  author={Ioffe, Sergey and Szegedy, Christian},
  journal={arXiv preprint arXiv:1502.03167},
  year={2015}
}

@inproceedings{Alex:08,
  title={Unconstrained on-line handwriting recognition with recurrent neural networks},
  author={Graves, Alex and Liwicki, Marcus and Bunke, Horst and Schmidhuber, J{\"u}rgen and Fern{\'a}ndez, Santiago},
  booktitle={Advances in neural information processing systems},
  pages={},
  year={2008}
}

@article{Wenpeng:16,
  title={Multichannel variable-size convolution for sentence classification},
  author={Yin, Wenpeng and Sch{\"u}tze, Hinrich},
  journal={arXiv preprint arXiv:1603.04513},
  year={2016}
 }
 
 @inproceedings{Aliaksei:15,
  title={Learning to rank short text pairs with convolutional deep neural networks},
  author={Severyn, Aliaksei and Moschitti, Alessandro},
  booktitle={Proceedings of the 38th international ACM SIGIR conference on research and development in information retrieval},
  pages={},
  year={2015},
  organization={ACM}
}


@inproceedings{Jacob:19,
  title={BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding},
  author={Devlin, Jacob and Chang, Ming-Wei and Lee, Kenton and Toutanova, Kristina},
  booktitle={NAACL-HLT (1)},
  year={2019}
}


@inproceedings{Peters:18,
  author={Peters, Matthew E. and  Neumann, Mark and Iyyer, Mohit and Gardner, Matt and Clark, Christopher and Lee, Kenton and Zettlemoyer, Luke},
  title={Deep contextualized word representations},
  booktitle={Proc. of NAACL},
  year={2018}
}


@article{Daniel:18,
  title={Universal sentence encoder},
  author={Cer, Daniel and Yang, Yinfei and Kong, Sheng-yi and Hua, Nan and Limtiaco, Nicole and John, Rhomni St and Constant, Noah and Guajardo-Cespedes, Mario and Yuan, Steve and Tar, Chris and others},
  journal={arXiv preprint arXiv:1803.11175},
  year={2018}
}

@InProceedings{Armand:17,
  title={Bag of Tricks for Efficient Text Classification},
  author={Joulin, Armand and Grave, Edouard and Bojanowski, Piotr and Mikolov, Tomas},
  booktitle={Proceedings of the 15th Conference of the European Chapter of the Association for Computational Linguistics: Volume 2, Short Papers},
  month={April},
  year={2017},
  publisher={Association for Computational Linguistics},
  pages={},
}

@article{Nal:14,
  title={A convolutional neural network for modelling sentences},
  author={Kalchbrenner, Nal and Grefenstette, Edward and Blunsom, Phil},
  journal={arXiv preprint arXiv:1404.2188},
  year={2014}
}

@inproceedings{Oriol:15,
  title={Show and tell: A neural image caption generator},
  author={Vinyals, Oriol and Toshev, Alexander and Bengio, Samy and Erhan, Dumitru},
  booktitle={Proceedings of the IEEE conference on computer vision and pattern recognition},
  pages={3156--3164},
  year={2015}
}

@inproceedings{Christian:17,
  title={Inception-v4, inception-resnet and the impact of residual connections on learning},
  author={Szegedy, Christian and Ioffe, Sergey and Vanhoucke, Vincent and Alemi, Alexander A},
  booktitle={Thirty-First AAAI Conference on Artificial Intelligence},
  year={2017}
}

@InProceedings{Alexis:17,
  author    = {Conneau, Alexis  and  Kiela, Douwe  and  Schwenk, Holger  and  Barrault, Lo\"{i}c  and  Bordes, Antoine},
  title     = {Supervised Learning of Universal Sentence Representations from Natural Language Inference Data},
  booktitle = {Proceedings of the 2017 Conference on Empirical Methods in Natural Language Processing},
  month     = {September},
  year      = {2017},
  address   = {Copenhagen, Denmark},
  publisher = {Association for Computational Linguistics},
  pages     = {670--680},
  url       = {https://www.aclweb.org/anthology/D17-1070}
}

@article{Pierre:16,
  title={Opensubtitles2016: Extracting large parallel corpora from movie and tv subtitles},
  author={Lison, Pierre and Tiedemann, J{\"o}rg},
  year={2016},
  publisher={European Language Resources Association}
}

 @InProceedings{Shitala:18,
    author = {Prasad, Shitala and Wai Kin Kong, Adams},
    title = {Using Object Information for Spotting Text},
    booktitle = {The European Conference on Computer Vision (ECCV)},
    month = {September},
    year = {2018}
    }
    
  @inproceedings{Shancheng:18,
  title={Attention and Language Ensemble for Scene Text Recognition with Convolutional Sequence Modeling},
  author={Fang, Shancheng and Xie, Hongtao and Zha, Zheng-Jun and Sun, Nannan and Tan, Jianlong and Zhang, Yongdong},
  booktitle={2018 ACM Multimedia Conference on Multimedia Conference},
  pages={},
  year={2018},
  organization={ACM}
  }
  
  @inproceedings{Ashish:17,
  title={Attention is all you need},
  author={Vaswani, Ashish and Shazeer, Noam and Parmar, Niki and Uszkoreit, Jakob and Jones, Llion and Gomez, Aidan N and Kaiser, {\L}ukasz and Polosukhin, Illia},
  booktitle={Advances in neural information processing systems},
  pages={5998--6008},
  year={2017}
}

@inproceedings{Marcella:20,
  title={Meshed-Memory Transformer for Image Captioning},
  author={Cornia, Marcella and Stefanini, Matteo and Baraldi, Lorenzo and Cucchiara, Rita},
  booktitle={Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition},
  pages={10578--10587},
  year={2020}
}

@inproceedings{Qi:16,
  title={Keyphrase extraction using deep recurrent neural networks on twitter},
  author={Zhang, Qi and Wang, Yang and Gong, Yeyun and Huang, Xuan-Jing},
  booktitle={Proceedings of the 2016 conference on empirical methods in natural language processing},
  pages={},
  year={2016}
}

@article{young2014image,
  title={From image descriptions to visual denotations: New similarity metrics for semantic inference over event descriptions},
  author={Young, Peter and Lai, Alice and Hodosh, Micah and Hockenmaier, Julia},
  journal={TACL},
  volume={},
  pages={},
  year={2014},
  publisher={MIT Press}
}

@inproceedings{karpathy2015deep,
  title={Deep visual-semantic alignments for generating image descriptions},
  author={Karpathy, Andrej and Fei-Fei, Li},
  booktitle={Proceedings of the IEEE conference on computer vision and pattern recognition},
  pages={3128--3137},
  year={2015}
}



@inproceedings{banerjee2005meteor,
  title={METEOR: An automatic metric for MT evaluation with improved correlation with human judgments},
  author={Banerjee, Satanjeev and Lavie, Alon},
  booktitle={Proceedings of the acl workshop on intrinsic and extrinsic evaluation measures for machine translation and/or summarization},
  pages={65--72},
  year={2005}
}


@inproceedings{vedantam2015cider,
  title={Cider: Consensus-based image description evaluation},
  author={Vedantam, Ramakrishna and Lawrence Zitnick, C and Parikh, Devi},
  booktitle={Proceedings of the IEEE conference on computer vision and pattern recognition},
  pages={4566--4575},
  year={2015}
}

@article{reimers2019sentence,
  title={Sentence-bert: Sentence embeddings using siamese bert-networks},
  author={Reimers, Nils and Gurevych, Iryna},
  journal={arXiv preprint arXiv:1908.10084},
  year={2019}
}

@article{hinton1999products,
  title={Products of experts},
  author={Hinton, Geoffrey E},
  year={1999},
  publisher={IET}
}

@article{meng2017deep,
  title={Deep keyphrase generation},
  author={Meng, Rui and Zhao, Sanqiang and Han, Shuguang and He, Daqing and Brusilovsky, Peter and Chi, Yu},
  journal={arXiv preprint arXiv:1704.06879},
  year={2017}
}


@inproceedings{xu2015show,
  title={Show, attend and tell: Neural image caption generation with visual attention},
  author={Xu, Kelvin and Ba, Jimmy and Kiros, Ryan and Cho, Kyunghyun and Courville, Aaron and Salakhudinov, Ruslan and Zemel, Rich and Bengio, Yoshua},
  booktitle={International conference on machine learning},
  pages={2048--2057},
  year={2015}
}


@inproceedings{anderson2018bottom,
  title={Bottom-up and top-down attention for image captioning and visual question answering},
  author={Anderson, Peter and He, Xiaodong and Buehler, Chris and Teney, Damien and Johnson, Mark and Gould, Stephen and Zhang, Lei},
  booktitle={Proceedings of the IEEE conference on computer vision and pattern recognition},
  pages={},
  year={2018}
}

@article{JDH17,
  title={Billion-scale similarity search with GPUs},
  author={Johnson, Jeff and Douze, Matthijs and J{\'e}gou, Herv{\'e}},
  journal={arXiv preprint arXiv:1702.08734},
  year={2017}
}

@inproceedings{lu202012,
  title={12-in-1: Multi-task vision and language representation learning},
  author={Lu, Jiasen and Goswami, Vedanuj and Rohrbach, Marcus and Parikh, Devi and Lee, Stefan},
  booktitle={Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition},
  pages={10437--10446},
  year={2020}
}

@inproceedings{bert-score,
  title={BERTScore: Evaluating Text Generation with BERT},
  author={Tianyi Zhang and Varsha Kishore* and Felix Wu* and Kilian Q. Weinberger and Yoav Artzi},
  booktitle={International Conference on Learning Representations},
  year={2020},
  url={https://openreview.net/forum?id=SkeHuCVFDr}
}

@inproceedings{blok2003probability,
  title={Probability from similarity},
  author={Blok, Sergey and Medin, Douglas and Osherson, Daniel},
  booktitle={AAAI Spring Symposium on Logical Formalization of Commonsense Reasoning},
  pages={},
  year={2003}
}



@article{radford2019language,
  title={Language models are unsupervised multitask learners},
  author={Radford, Alec and Wu, Jeffrey and Child, Rewon and Luan, David and Amodei, Dario and Sutskever, Ilya},
  journal={OpenAI blog},
  volume={},
  number={},
  pages={},
  year={2019}
}

@article{liu2019roberta,
  title={Roberta: A robustly optimized bert pretraining approach},
  author={Liu, Yinhan and Ott, Myle and Goyal, Naman and Du, Jingfei and Joshi, Mandar and Chen, Danqi and Levy, Omer and Lewis, Mike and Zettlemoyer, Luke and Stoyanov, Veselin},
  journal={arXiv preprint arXiv:1907.11692},
  year={2019}
}
@article{wang2018glue,
  title={GLUE: A multi-task benchmark and analysis platform for natural language understanding},
  author={Wang, Alex and Singh, Amanpreet and Michael, Julian and Hill, Felix and Levy, Omer and Bowman, Samuel R},
  journal={arXiv preprint arXiv:1804.07461},
  year={2018}
}

@article{mccarthy2010mtld,
  title={MTLD, vocd-D, and HD-D: A validation study of sophisticated approaches to lexical diversity assessment},
  author={McCarthy, Philip M and Jarvis, Scott},
  journal={Behavior research methods},
  volume={42},
  number={2},
  pages={381--392},
  year={2010},
  publisher={Springer}
}
@article{koehn2017six,
  title={Six challenges for neural machine translation},
  author={Koehn, Philipp and Knowles, Rebecca},
  journal={arXiv preprint arXiv:1706.03872},
  year={2017}
}

@inproceedings{huang2019attention,
  title={Attention on attention for image captioning},
  author={Huang, Lun and Wang, Wenmin and Chen, Jie and Wei, Xiao-Yong},
  booktitle={Proceedings of the IEEE/CVF International Conference on Computer Vision},
  pages={4634--4643},
  year={2019}
}

@inproceedings{luo2018discriminability,
  title={Discriminability objective for training descriptive captions},
  author={Luo, Ruotian and Price, Brian and Cohen, Scott and Shakhnarovich, Gregory},
  booktitle={Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition},
  pages={6964--6974},
  year={2018}
}

@inproceedings{wang2019describing,
  title={Describing like humans: on diversity in image captioning},
  author={Wang, Qingzhong and Chan, Antoni B},
  booktitle={Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition},
  pages={4195--4203},
  year={2019}
}
@article{cer2017semeval,
  title={Semeval-2017 task 1: Semantic textual similarity-multilingual and cross-lingual focused evaluation},
  author={Cer, Daniel and Diab, Mona and Agirre, Eneko and Lopez-Gazpio, Inigo and Specia, Lucia},
  journal={arXiv preprint arXiv:1708.00055},
  year={2017}
}

 @InProceedings{Shitala:18,
    author = {Prasad, Shitala and Wai Kin Kong, Adams},
    title = {Using Object Information for Spotting Text},
    booktitle = {The European Conference on Computer Vision (ECCV)},
    month = {September},
    year = {2018}
    }
    
  @inproceedings{Shancheng:18,
  title={Attention and Language Ensemble for Scene Text Recognition with Convolutional Sequence Modeling},
  author={Fang, Shancheng and Xie, Hongtao and Zha, Zheng-Jun and Sun, Nannan and Tan, Jianlong and Zhang, Yongdong},
  booktitle={2018 ACM Multimedia Conference on Multimedia Conference},
  pages={},
  year={2018},
  organization={ACM}
  }
  
  



@inproceedings{vijayakumar2018diverse,
  title={Diverse beam search for improved description of complex scenes},
  author={Vijayakumar, Ashwin and Cogswell, Michael and Selvaraju, Ramprasaath and Sun, Qing and Lee, Stefan and Crandall, David and Batra, Dhruv},
  booktitle={Proceedings of the AAAI Conference on Artificial Intelligence},
  volume={},
  number={},
  year={2018}
}
@inproceedings{sabir2018visual,
  title={Visual re-ranking with natural language understanding for text spotting},
  author={Sabir, Ahmed and Moreno-Noguer, Francesc and Padr{\'o}, Llu{\'\i}s},
  booktitle={Asian Conference on Computer Vision},
  pages={68--82},
  year={2018},
  organization={Springer}
}

@inproceedings{anderson2016spice,
  title={Spice: Semantic propositional image caption evaluation},
  author={Anderson, Peter and Fernando, Basura and Johnson, Mark and Gould, Stephen},
  booktitle={European conference on computer vision},
  pages={382--398},
  year={2016},
  organization={Springer}
}
@article{lison2016opensubtitles2016,
  title={Opensubtitles2016: Extracting large parallel corpora from movie and tv subtitles},
  author={Lison, Pierre and Tiedemann, J{\"o}rg},
  year={2016},
  publisher={European Language Resources Association}
}

@article{blok2007induction,
  title={Induction as conditional probability judgment},
  author={Blok, Sergey V and Medin, Douglas L and Osherson, Daniel N},
  journal={Memory \& Cognition},
  volume={35},
  number={6},
  pages={1353--1364},
  year={2007},
  publisher={Springer}
}

@article{voorspoels2015people,
  title={How do people learn from negative evidence? Non-monotonic generalizations and sampling assumptions in inductive reasoning},
  author={Voorspoels, Wouter and Navarro, Daniel J and Perfors, Amy and Ransom, Keith and Storms, Gert},
  journal={Cognitive Psychology},
  volume={81},
  pages={1--25},
  year={2015},
  publisher={Elsevier}
}
@inproceedings{heussen2010can,
  title={Can similarity-based models of induction handle negative evidence?},
  author={Heussen, Daniel and Voorspoels, Wouter and Storms, Gert},
  booktitle={Proceedings of the Annual Conference of the Cognitive Science Society},
  volume={32},
  pages={2033--2038},
  year={2010},
  organization={Lawrence Erlbaum Associates}
}


@article{sharma2017nlgeval,
    author  = {Sharma, Shikhar and El Asri, Layla and Schulz, Hannes and Zumer, Jeremie},
    title   = {Relevance of Unsupervised Metrics in Task-Oriented Dialogue for Evaluating Natural Language Generation},
    journal = {CoRR},
    volume  = {abs/1706.09799},
    year    = {2017},
    url     = {http://arxiv.org/abs/1706.09799}
}



@inproceedings{pennington2014glove,
  title={Glove: Global vectors for word representation},
  author={Pennington, Jeffrey and Socher, Richard and Manning, Christopher D},
  booktitle={Proceedings of the 2014 conference on empirical methods in natural language processing (EMNLP)},
  pages={1532--1543},
  year={2014}
}

@article{li2015diversity,
  title={A diversity-promoting objective function for neural conversation models},
  author={Li, Jiwei and Galley, Michel and Brockett, Chris and Gao, Jianfeng and Dolan, Bill},
  journal={arXiv preprint arXiv:1510.03055},
  year={2015}
}

@article{augenstein2017semeval,
  title={Semeval 2017 task 10: Scienceie-extracting keyphrases and relations from scientific publications},
  author={Augenstein, Isabelle and Das, Mrinal and Riedel, Sebastian and Vikraman, Lakshmi and McCallum, Andrew},
  journal={arXiv preprint arXiv:1704.02853},
  year={2017}
}

@article{zhu2020towards,
  title={Towards Understanding Sample Variance in Visually Grounded Language Generation: Evaluations and Observations},
  author={Zhu, Wanrong and Wang, Xin Eric and Narayana, Pradyumna and Sone, Kazoo and Basu, Sugato and Wang, William Yang},
  journal={arXiv preprint arXiv:2010.03644},
  year={2020}
}

@inproceedings{lewis2020pretrained,
  title={Pretrained Language Models for Biomedical and Clinical Tasks: Understanding and Extending the State-of-the-Art},
  author={Lewis, Patrick and Ott, Myle and Du, Jingfei and Stoyanov, Veselin},
  booktitle={Proceedings of the 3rd Clinical Natural Language Processing Workshop},
  pages={146--157},
  year={2020}
}


@article{wang2018object,
  title={Object counts! bringing explicit detections back into image captioning},
  author={Wang, Josiah and Madhyastha, Pranava and Specia, Lucia},
  journal={arXiv preprint arXiv:1805.00314},
  year={2018}
}

@inproceedings{fang2015captions,
  title={From captions to visual concepts and back},
  author={Fang, Hao and Gupta, Saurabh and Iandola, Forrest and Srivastava, Rupesh K and Deng, Li and Doll{\'a}r, Piotr and Gao, Jianfeng and He, Xiaodong and Mitchell, Margaret and Platt, John C and others},
  booktitle={Proceedings of the IEEE conference on computer vision and pattern recognition},
  pages={1473--1482},
  year={2015}
}

@inproceedings{zhang2020context,
  title={Context-aware attention network for image-text retrieval},
  author={Zhang, Qi and Lei, Zhen and Zhang, Zhaoxiang and Li, Stan Z},
  booktitle={Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition},
  pages={3536--3545},
  year={2020}
}
@inproceedings{cornia2019show,
  title={Show, control and tell: A framework for generating controllable and grounded captions},
  author={Cornia, Marcella and Baraldi, Lorenzo and Cucchiara, Rita},
  booktitle={Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition},
  pages={8307--8316},
  year={2019}
}

@book{brown2005encyclopedia,
  title={Encyclopedia of language and linguistics},
  author={Brown, Keith},
  volume={1},
  year={2005},
  publisher={Elsevier}
}
@article{zhang2019bertscore,
  title={Bertscore: Evaluating text generation with bert},
  author={Zhang, Tianyi and Kishore, Varsha and Wu, Felix and Weinberger, Kilian Q and Artzi, Yoav},
  journal={arXiv preprint arXiv:1904.09675},
  year={2019}
}



@inproceedings{dai2017towards,
  title={Towards diverse and natural image descriptions via a conditional gan},
  author={Dai, Bo and Fidler, Sanja and Urtasun, Raquel and Lin, Dahua},
  booktitle={Proceedings of the IEEE International Conference on Computer Vision},
  pages={2970--2979},
  year={2017}
}
@inproceedings{vedantam2017context,
  title={Context-aware captions from context-agnostic supervision},
  author={Vedantam, Ramakrishna and Bengio, Samy and Murphy, Kevin and Parikh, Devi and Chechik, Gal},
  booktitle={Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition},
  pages={251--260},
  year={2017}
}

@article{herdade2019image,
  title={Image captioning: Transforming objects into words},
  author={Herdade, Simao and Kappeler, Armin and Boakye, Kofi and Soares, Joao},
  journal={arXiv preprint arXiv:1906.05963},
  year={2019}
}


@inproceedings{rennie2017self,
  title={Self-critical sequence training for image captioning},
  author={Rennie, Steven J and Marcheret, Etienne and Mroueh, Youssef and Ross, Jerret and Goel, Vaibhava},
  booktitle={Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition},
  pages={7008--7024},
  year={2017}
}

@inproceedings{you2016image,
  title={Image captioning with semantic attention},
  author={You, Quanzeng and Jin, Hailin and Wang, Zhaowen and Fang, Chen and Luo, Jiebo},
  booktitle={Proceedings of the IEEE conference on computer vision and pattern recognition},
  pages={4651--4659},
  year={2016}
}

}
@inproceedings{ren2017deep,
  title={Deep reinforcement learning-based image captioning with embedding reward},
  author={Ren, Zhou and Wang, Xiaoyu and Zhang, Ning and Lv, Xutao and Li, Li-Jia},
  booktitle={Proceedings of the IEEE conference on computer vision and pattern recognition},
  pages={290--298},
  year={2017}
}


}
@article{ippolito2019comparison,
  title={Comparison of diverse decoding methods from conditional language models},
  author={Ippolito, Daphne and Kriz, Reno and Kustikova, Maria and Sedoc, Jo{\~a}o and Callison-Burch, Chris},
  journal={arXiv preprint arXiv:1906.06362},
  year={2019}
}

@article{voorspoels2015people,
  title={How do people learn from negative evidence? Non-monotonic generalizations and sampling assumptions in inductive reasoning},
  author={Voorspoels, Wouter and Navarro, Daniel J and Perfors, Amy and Ransom, Keith and Storms, Gert},
  journal={Cognitive Psychology},
  volume={81},
  pages={1--25},
  year={2015},
  publisher={Elsevier}
}

@article{li2015diversity,
  title={A diversity-promoting objective function for neural conversation models},
  author={Li, Jiwei and Galley, Michel and Brockett, Chris and Gao, Jianfeng and Dolan, Bill},
  journal={arXiv preprint arXiv:1510.03055},
  year={2015}
}
@inproceedings{gupta2020contrastive,
  title={Contrastive learning for weakly supervised phrase grounding},
  author={Gupta, Tanmay and Vahdat, Arash and Chechik, Gal and Yang, Xiaodong and Kautz, Jan and Hoiem, Derek},
  booktitle={Computer Vision--ECCV 2020: 16th European Conference, Glasgow, UK, August 23--28, 2020, Proceedings, Part III 16},
  pages={752--768},
  year={2020},
  organization={Springer}
}
@inproceedings{song2021sentsim,
  title={SentSim: Crosslingual Semantic Evaluation of Machine Translation},
  author={Song, Yurun and Zhao, Junchen and Specia, Lucia},
  booktitle={Proceedings of the 2021 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies},
  pages={3143--3156},
  year={2021}
}
@inproceedings{zhang2021rstnet,
  title={RSTNet: Captioning With Adaptive Attention on Visual and Non-Visual Words},
  author={Zhang, Xuying and Sun, Xiaoshuai and Luo, Yunpeng and Ji, Jiayi and Zhou, Yiyi and Wu, Yongjian and Huang, Feiyue and Ji, Rongrong},
  booktitle={Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition},
  pages={15465--15474},
  year={2021}
}

@inproceedings{wang2020compare,
  title={Compare and reweight: Distinctive image captioning using similar images sets},
  author={Wang, Jiuniu and Xu, Wenjia and Wang, Qingzhong and Chan, Antoni B},
  booktitle={European Conference on Computer Vision},
  pages={370--386},
  year={2020},
  organization={Springer}
}
@article{gao2021simcse,
   title={{SimCSE}: Simple Contrastive Learning of Sentence Embeddings},
   author={Gao, Tianyu and Yao, Xingcheng and Chen, Danqi},
   journal={arXiv preprint arXiv:2104.08821},
   year={2021}
}

@article{conneau2017supervised,
  title={Supervised learning of universal sentence representations from natural language inference data},
  author={Conneau, Alexis and Kiela, Douwe and Schwenk, Holger and Barrault, Loic and Bordes, Antoine},
  journal={arXiv preprint arXiv:1705.02364},
  year={2017}
}

@article{radford2021learning,
  title={Learning transferable visual models from natural language supervision},
  author={Radford, Alec and Kim, Jong Wook and Hallacy, Chris and Ramesh, Aditya and Goh, Gabriel and Agarwal, Sandhini and Sastry, Girish and Askell, Amanda and Mishkin, Pamela and Clark, Jack and others},
  journal={arXiv preprint arXiv:2103.00020},
  year={2021}
}

@inproceedings{huang2017speed,
  title={Speed/accuracy trade-offs for modern convolutional object detectors},
  author={Huang, Jonathan and Rathod, Vivek and Sun, Chen and Zhu, Menglong and Korattikara, Anoop and Fathi, Alireza and Fischer, Ian and Wojna, Zbigniew and Song, Yang and Guadarrama, Sergio and others},
  booktitle={Proceedings of the IEEE conference on computer vision and pattern recognition},
  pages={},
  year={2017}
}
@inproceedings{papineni2002bleu,
  title={Bleu: a method for automatic evaluation of machine translation},
  author={Papineni, Kishore and Roukos, Salim and Ward, Todd and Zhu, Wei-Jing},
  booktitle={Proceedings of the 40th annual meeting of the Association for Computational Linguistics},
  pages={311--318},
  year={2002}
}



@inproceedings{lin2004rouge,
  title={Rouge: A package for automatic evaluation of summaries},
  author={Lin, Chin-Yew},
  booktitle={Text summarization branches out},
  pages={74--81},
  year={2004}
}

@inproceedings{clark2011better,
  title={Better hypothesis testing for statistical machine translation: Controlling for optimizer instability},
  author={Clark, Jonathan H and Dyer, Chris and Lavie, Alon and Smith, Noah A},
  booktitle={Proceedings of the 49th Annual Meeting of the Association for Computational Linguistics: Human Language Technologies},
  pages={176--181},
  year={2011}
}

@inproceedings{deshpande2019fast,
  title={Fast, diverse and accurate image captioning guided by part-of-speech},
  author={Deshpande, Aditya and Aneja, Jyoti and Wang, Liwei and Schwing, Alexander G and Forsyth, David},
  booktitle={Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition},
  pages={10695--10704},
  year={2019}
}

@inproceedings{chen2020say,
  title={Say as you wish: Fine-grained control of image caption generation with abstract scene graphs},
  author={Chen, Shizhe and Jin, Qin and Wang, Peng and Wu, Qi},
  booktitle={Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition},
  pages={9962--9971},
  year={2020}
}

@inproceedings{zhang2021consensus,
  title={Consensus graph representation learning for better grounded image captioning},
  author={Zhang, Wenqiao and Shi, Haochen and Tang, Siliang and Xiao, Jun and Yu, Qiang and Zhuang, Yueting},
  booktitle={Proc 35 AAAI Conf on Artificial Intelligence},
  year={2021}
}

@inproceedings{elliott2014comparing,
  title={Comparing automatic evaluation measures for image description},
  author={Elliott, Desmond and Keller, Frank},
  booktitle={Proceedings of the 52nd Annual Meeting of the Association for Computational Linguistics (Volume 2: Short Papers)},
  pages={452--457},
  year={2014}
}

  @inproceedings{changpinyo2021conceptual,
  title={Conceptual 12M: Pushing Web-Scale Image-Text Pre-Training To Recognize Long-Tail Visual Concepts},
  author={Changpinyo, Soravit and Sharma, Piyush and Ding, Nan and Soricut, Radu},
  booktitle={Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition},
  pages={3558--3568},
  year={2021}
}

@inproceedings{abadi2016tensorflow,
  title={Tensorflow: A system for large-scale machine learning},
  author={Abadi, Mart{\'\i}n and Barham, Paul and Chen, Jianmin and Chen, Zhifeng and Davis, Andy and Dean, Jeffrey and Devin, Matthieu and Ghemawat, Sanjay and Irving, Geoffrey and Isard, Michael and others},
  booktitle={12th $\{$USENIX$\}$ symposium on operating systems design and implementation ($\{$OSDI$\}$ 16)},
  pages={265--283},
  year={2016}
}

@article{paszke2019pytorch,
  title={Pytorch: An imperative style, high-performance deep learning library},
  author={Paszke, Adam and Gross, Sam and Massa, Francisco and Lerer, Adam and Bradbury, James and Chanan, Gregory and Killeen, Trevor and Lin, Zeming and Gimelshein, Natalia and Antiga, Luca and others},
  journal={arXiv preprint arXiv:1912.01703},
  year={2019}
}

@inproceedings{ott2019fairseq,
  title = {fairseq: A Fast, Extensible Toolkit for Sequence Modeling},
  author = {Myle Ott and Sergey Edunov and Alexei Baevski and Angela Fan and Sam Gross and Nathan Ng and David Grangier and Michael Auli},
  booktitle = {Proceedings of NAACL-HLT 2019: Demonstrations},
  year = {2019},
}

@inproceedings{post2018call,
  title={A Call for Clarity in Reporting BLEU Scores},
  author={Post, Matt},
  booktitle={Proceedings of the Third Conference on Machine Translation: Research Papers},
  pages={186--191},
  year={2018}
}

@inproceedings{popovic2015chrf,
  title={chrF: character n-gram F-score for automatic MT evaluation},
  author={Popovi{\'c}, Maja},
  booktitle={Proceedings of the Tenth Workshop on Statistical Machine Translation},
  pages={392--395},
  year={2015}
}

@inproceedings{shetty2017speaking,
  title={Speaking the same language: Matching machine to human captions by adversarial training},
  author={Shetty, Rakshith and Rohrbach, Marcus and Anne Hendricks, Lisa and Fritz, Mario and Schiele, Bernt},
  booktitle={Proceedings of the IEEE International Conference on Computer Vision},
  pages={4135--4144},
  year={2017}
}

@inproceedings{peinelt2020tbert,
  title={tBERT: Topic models and BERT joining forces for semantic similarity detection},
  author={Peinelt, Nicole and Nguyen, Dong and Liakata, Maria},
  booktitle={Proceedings of the 58th Annual Meeting of the Association for Computational Linguistics},
  pages={7047--7055},
  year={2020}
}
@inproceedings{rashtchian2010collecting,
  title={Collecting image annotations using amazon’s mechanical turk},
  author={Rashtchian, Cyrus and Young, Peter and Hodosh, Micah and Hockenmaier, Julia},
  booktitle={Proceedings of the NAACL HLT 2010 workshop on creating speech and language data with Amazon’s Mechanical Turk},
  pages={139--147},
  year={2010}
}


@article{jaderberg2016reading,
  title={Reading text in the wild with convolutional neural networks},
  author={Jaderberg, Max and Simonyan, Karen and Vedaldi, Andrea and Zisserman, Andrew},
  journal={International Journal of Computer Vision},
  volume={},
  number={},
  pages={},
  year={2016},
  publisher={Springer}
}

@article{camacho2019relational,
  title={Relational Word Embeddings},
  author={Camacho-Collados, Jose and Espinosa-Anke, Luis and Schockaert, Steven},
  journal={arXiv preprint arXiv:1906.01373},
  year={2019}
}

@article{joulin2017bag,
  title={Bag of Tricks for Efficient Text Classification},
  author={Joulin, Armand and Grave, Edouard and Mikolov, Piotr Bojanowski Tomas},
  journal={EACL 2017},
  pages={427},
  year={2017}
}

@inproceedings{mikolov2013distributed,
  title={Distributed representations of words and phrases and their compositionality},
  author={Mikolov, Tomas and Sutskever, Ilya and Chen, Kai and Corrado, Greg S and Dean, Jeff},
  booktitle={Advances in neural information processing systems},
  pages={},
  year={2013}
}

@article{turney2010frequency,
  title={From frequency to meaning: Vector space models of semantics},
  author={Turney, Peter D and Pantel, Patrick},
  journal={Journal of artificial intelligence research},
  volume={37},
  pages={141--188},
  year={2010}
}

@article{navigli2012babelnet,
  title={BabelNet: The automatic construction, evaluation and application of a wide-coverage multilingual semantic network},
  author={Navigli, Roberto and Ponzetto, Simone Paolo},
  journal={Artificial intelligence},
  volume={193},
  pages={217--250},
  year={2012},
  publisher={Elsevier}
}

@inproceedings{turney2001mining,
  title={Mining the web for synonyms: PMI-IR versus LSA on TOEFL},
  author={Turney, Peter D},
  booktitle={European conference on machine learning},
  pages={491--502},
  year={2001},
  organization={Springer}
}

@inproceedings{turney2001mining,
  title={Mining the web for synonyms: PMI-IR versus LSA on TOEFL},
  author={Turney, Peter D},
  booktitle={European conference on machine learning},
  pages={491--502},
  year={2001},
  organization={Springer}
}

@article{landauer1998introduction,
  title={An introduction to latent semantic analysis},
  author={Landauer, Thomas K and Foltz, Peter W and Laham, Darrell},
  journal={Discourse processes},
  volume={25},
  number={2-3},
  pages={259--284},
  year={1998},
  publisher={Taylor \& Francis}
}

@article{cha2007comprehensive,
  title={Comprehensive survey on distance/similarity measures between probability density functions},
  author={Cha, Sung-Hyuk},
  journal={City},
  volume={1},
  number={2},
  pages={1},
  year={2007}
}

@article{xu2005survey,
  title={Survey of clustering algorithms},
  author={Xu, Rui and Wunsch, Donald},
  journal={IEEE Transactions on neural networks},
  volume={16},
  number={3},
  pages={645--678},
  year={2005},
  publisher={Ieee}
}

@article{sanchez2012ontology,
  title={Ontology-based semantic similarity: A new feature-based approach},
  author={S{\'a}nchez, David and Batet, Montserrat and Isern, David and Valls, Aida},
  journal={Expert systems with applications},
  volume={39},
  number={9},
  pages={7718--7728},
  year={2012},
  publisher={Elsevier}
}

@book{miller1998wordnet,
  title={WordNet: An electronic lexical database},
  author={Miller, George A},
  year={1998},
  publisher={MIT press}
}

@conference{veit2016cocotext,
title = {COCO-Text: Dataset and Benchmark for Text Detection and Recognition in Natural Images},
author = {Andreas Veit and Tomas Matera and Lukas Neumann and Jiri Matas and Serge Belongie},
url = {http://vision.cornell.edu/se3/wp-content/uploads/2016/01/1601.07140v1.pdf},
year = {2016},
date = {2016-01-26},
booktitle = {arXiv preprint arXiv:1601.07140},
keywords = {}
}

@inproceedings{ghosh2017visual,
  title={Visual attention models for scene text recognition},
  author={Ghosh, Suman K and Valveny, Ernest and Bagdanov, Andrew D},
  booktitle={2017 14th IAPR international conference on document analysis and recognition (ICDAR)},
  volume={1},
  pages={943--948},
  year={2017},
  organization={IEEE}
}

@article{zhou2017places,
  title={Places: A 10 million image database for scene recognition},
  author={Zhou, Bolei and Lapedriza, Agata and Khosla, Aditya and Oliva, Aude and Torralba, Antonio},
  journal={IEEE transactions on pattern analysis and machine intelligence},
  volume={40},
  number={6},
  pages={1452--1464},
  year={2017},
  publisher={IEEE}
}

@inproceedings{,
  title={The berkeley framenet project},
  author={Baker, Collin F and Fillmore, Charles J and Lowe, John B},
  booktitle={COLING 1998 Volume 1: The 17th International Conference on Computational Linguistics},
  year={1998}
}

@inproceedings{he2016deep,
  title={Deep residual learning for image recognition},
  author={He, Kaiming and Zhang, Xiangyu and Ren, Shaoqing and Sun, Jian},
  booktitle={Proceedings of the IEEE conference on computer vision and pattern recognition},
  pages={770--778},
  year={2016}
}

@inproceedings{szegedy2017inception,
  title={Inception-v4, inception-resnet and the impact of residual connections on learning},
  author={Szegedy, Christian and Ioffe, Sergey and Vanhoucke, Vincent and Alemi, Alexander A},
  booktitle={Thirty-first AAAI conference on artificial intelligence},
  year={2017}
}

@inproceedings{iacobacci2019lstmembed,
  title={Lstmembed: Learning word and sense representations from a large semantically annotated corpus with long short-term memories},
  author={Iacobacci, Ignacio and Navigli, Roberto},
  booktitle={Proceedings of the 57th Annual Meeting of the Association for Computational Linguistics},
  pages={1685--1695},
  year={2019}
}

@inproceedings{iacobacci2015sensembed,
  title={Sensembed: Learning sense embeddings for word and relational similarity},
  author={Iacobacci, Ignacio and Pilehvar, Mohammad Taher and Navigli, Roberto},
  booktitle={Proceedings of the 53rd Annual Meeting of the Association for Computational Linguistics and the 7th International Joint Conference on Natural Language Processing (Volume 1: Long Papers)},
  pages={95--105},
  year={2015}
}

@article{mancini2016embedding,
  title={Embedding words and senses together via joint knowledge-enhanced training},
  author={Mancini, Massimiliano and Camacho-Collados, Jose and Iacobacci, Ignacio and Navigli, Roberto},
  journal={arXiv preprint arXiv:1612.02703},
  year={2016}
}

@inproceedings{camacho2015nasari,
  title={Nasari: a novel approach to a semantically-aware representation of items},
  author={Camacho-Collados, Jos{\'e} and Pilehvar, Mohammad Taher and Navigli, Roberto},
  booktitle={Proceedings of the 2015 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies},
  pages={567--577},
  year={2015}
}

@inproceedings{raganato2016automatic,
  title={Automatic Construction and Evaluation of a Large Semantically Enriched Wikipedia.},
  author={Raganato, Alessandro and Bovi, Claudio Delli and Navigli, Roberto},
  booktitle={IJCAI},
  pages={2894--2900},
  year={2016}
}

@article{sabir2020enhancing,
  title={Enhancing scene text recognition with visual context information},
  author={Sabir, Ahmed},
  year={2020},
  publisher={Universitat Polit{\`e}cnica de Catalunya}
}

@inproceedings{lin2014microsoft,
  title={Microsoft coco: Common objects in context},
  author={Lin, Tsung-Yi and Maire, Michael and Belongie, Serge and Hays, James and Perona, Pietro and Ramanan, Deva and Doll{\'a}r, Piotr and Zitnick, C Lawrence},
  booktitle={European conference on computer vision},
  pages={740--755},
  year={2014},
  organization={Springer}
}

@inproceedings{lee2018stacked,
  title={Stacked cross attention for image-text matching},
  author={Lee, Kuang-Huei and Chen, Xi and Hua, Gang and Hu, Houdong and He, Xiaodong},
  booktitle={Proceedings of the European Conference on Computer Vision (ECCV)},
  pages={201--216},
  year={2018}
}

@inproceedings{mihalcea2006corpus,
  title={Corpus-based and knowledge-based measures of text semantic similarity},
  author={Mihalcea, Rada and Corley, Courtney and Strapparava, Carlo and others},
  booktitle={Aaai},
  volume={6},
  number={2006},
  pages={775--780},
  year={2006}
}

@article{zhu2016computing,
  title={Computing semantic similarity of concepts in knowledge graphs},
  author={Zhu, Ganggao and Iglesias, Carlos A},
  journal={IEEE Transactions on Knowledge and Data Engineering},
  volume={29},
  number={1},
  pages={72--85},
  year={2016},
  publisher={IEEE}
}

@inproceedings{budanitsky2001semantic,
  title={Semantic distance in WordNet: An experimental, application-oriented evaluation of five measures},
  author={Budanitsky, Alexander and Hirst, Graeme},
  booktitle={Workshop on WordNet and other lexical resources},
  volume={2},
  pages={2--2},
  year={2001}
}

@inproceedings{li2020oscar,
  title={Oscar: Object-semantics aligned pre-training for vision-language tasks},
  author={Li, Xiujun and Yin, Xi and Li, Chunyuan and Zhang, Pengchuan and Hu, Xiaowei and Zhang, Lei and Wang, Lijuan and Hu, Houdong and Dong, Li and Wei, Furu and others},
  booktitle={ECCV},
  pages={},
  year={2020},
  organization={Springer}
}

@inproceedings{fang2015captions,
  title={From captions to visual concepts and back},
  author={Fang, Hao and Gupta, Saurabh and Iandola, Forrest and Srivastava, Rupesh K and Deng, Li and Doll{\'a}r, Piotr and Gao, Jianfeng and He, Xiaodong and Mitchell, Margaret and Platt, John C and others},
  booktitle={Proceedings of the IEEE conference on computer vision and pattern recognition},
  pages={1473--1482},
  year={2015}
}

@article{rohrbach2018object,
  title={Object hallucination in image captioning},
  author={Rohrbach, Anna and Hendricks, Lisa Anne and Burns, Kaylee and Darrell, Trevor and Saenko, Kate},
  journal={arXiv preprint arXiv:1809.02156},
  year={2018}
}

@inproceedings{kim-2014-convolutional,
    title = "Convolutional Neural Networks for Sentence Classification",
    author = "Kim, Yoon",
    booktitle = "EMNLP",
    year = "2014",
    address = "",
    publisher = "",
    url = "",
    doi = "",
    pages = "",
}

@inproceedings{Jacob:19,
  title={BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding},
  author={Devlin, Jacob and Chang, Ming-Wei and Lee, Kenton and Toutanova, Kristina},
  booktitle={NAACL-HLT},
  year={2019}
}

@inproceedings{shetty2017speaking,
  title={Speaking the same language: Matching machine to human captions by adversarial training},
  author={Shetty, Rakshith and Rohrbach, Marcus and Anne Hendricks, Lisa and Fritz, Mario and Schiele, Bernt},
  booktitle={Proceedings of the IEEE International Conference on Computer Vision},
  pages={4135--4144},
  year={2017}
}
@inproceedings{deshpande2019fast,
  title={Fast, diverse and accurate image captioning guided by part-of-speech},
  author={Deshpande, Aditya and Aneja, Jyoti and Wang, Liwei and Schwing, Alexander G and Forsyth, David},
  booktitle={Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition},
  pages={10695--10704},
  year={2019}

}

@inproceedings{nair2010rectified,
  title={Rectified linear units improve restricted boltzmann machines},
  author={Nair, Vinod and Hinton, Geoffrey E},
  booktitle={Icml},
  year={2010}
}

@inproceedings{hendricks2018women,
  title={Women also snowboard: Overcoming bias in captioning models},
  author={Hendricks, Lisa Anne and Burns, Kaylee and Saenko, Kate and Darrell, Trevor and Rohrbach, Anna},
  booktitle={Proceedings of the European Conference on Computer Vision (ECCV)},
  pages={771--787},
  year={2018}
}

@inproceedings{hendricks2018women,
  title={Women also snowboard: Overcoming bias in captioning models},
  author={Hendricks, Lisa Anne and Burns, Kaylee and Saenko, Kate and Darrell, Trevor and Rohrbach, Anna},
  booktitle={Proceedings of the European Conference on Computer Vision (ECCV)},
  pages={771--787},
  year={2018}
}
@article{zhao2017men,
  title={Men also like shopping: Reducing gender bias amplification using corpus-level constraints},
  author={Zhao, Jieyu and Wang, Tianlu and Yatskar, Mark and Ordonez, Vicente and Chang, Kai-Wei},
  journal={arXiv preprint arXiv:1707.09457},
  year={2017}
}



</script>


