<!doctype html>

<a href="https://sabirdvd.github.io" class="button"> <small>↩ </small></a>
<link rel="stylesheet" href="https://use.fontawesome.com/releases/v5.6.1/css/all.css">
<meta charset="utf-8">
<script src="template.v1.js"></script>
<script type="text/front-matter">
  title: "Review: Large Language Models Are Biased Because They Are Large Language Models"
  authors:
  - Ahmed Sabir: https://kodu.ut.ee/~ahmedabdulmajeed/
  - PUBLISHED Date
  affiliations:
  - University of Tartu : https://ut.ee/en
  -  5 August 2025
</script>

<script type="text/javascript">
    function zoom() {
        document.body.style.zoom = "100%" 
    }
</script>

<body onload="zoom()">

<style>
    .blue-color { color: blue; }
    .green-color { color: green; }
    .teal-color { color: teal; }
    .yellow-color { color: yellow; }
    .red-color { color: red; }
</style>

<head>
    <style>
        .noteBoxes {
            border: 1px solid;
            border-radius: 5px;
            padding: 10px;
            margin: 10px 0;
            width: 600px;
        }
        .type1 { border-color: #E76F51; background-color: rgba(231, 111, 81, 0.1); }
        .type2 { border-color: #2A9D8F; background-color: rgba(42, 157, 143, 0.1); }
        .type3 { border-color: #0096C7; background-color: rgba(0, 150, 199, 0.1); }
        .type4 { border-color: #00B353; background-color: rgba(0, 179, 83, 0.1); }
    </style>
</head>

<dt-article class="centered">

<h2><p align="center">Review: Large Language Models Are Biased Because They Are Large Language Models</p></h2>
<dt-byline></dt-byline>

<body>
<p>In this review, I summarise Philip Resnik’s 2024 position paper, <em>Large Language Models Are Biased Because They Are Large Language Models</em> <dt-cite key="resnik2024bias"></dt-cite>. The work takes a conceptual rather than purely empirical approach, arguing that bias in large language models (LLMs) is <strong>structural</strong> rather than incidental.</p>

<!-- INTRODUCTION -->
<h3>1. Introduction</h3>
<p>
Resnik challenges the prevailing assumption that harmful bias in LLMs is a correctable defect.  
Instead, he argues that such bias is a natural consequence of the probabilistic, data-driven approach used to train these models.  
Given that training data reflects human society—with all its historical, cultural, and political biases—these biases inevitably become embedded in the model itself.
</p>

<!-- KEY ARGUMENTS -->
<h3>2. Key Arguments</h3>
<ul>
    <li><strong>Bias is Structural</strong> — It arises from the statistical modeling of human language patterns, not from accidental “bugs.”</li>
    <li><strong>Mitigation Has Limits</strong> — Filtering, fine-tuning, and curated datasets can only address surface-level manifestations of bias.</li>
    <li><strong>Rethinking Model Design</strong> — Effective solutions may require fundamentally different architectures, training paradigms, or reasoning frameworks.</li>
</ul>

<!-- METAPHOR -->
<h3>3. The Scorpion and the Frog</h3>
<p>
To illustrate his thesis, Resnik invokes the fable of the scorpion and the frog.  
Even after promising not to sting the frog, the scorpion does so mid-river, explaining: “It’s in my nature.”  
Likewise, LLMs will continue to exhibit bias, because it is intrinsic to their statistical design.
</p>

<!-- IMPLICATIONS -->
<h3>4. Implications</h3>
<p>
If bias is structural, deploying LLMs in high-stakes areas such as hiring, law, healthcare, or education risks scaling up societal inequities.  
Resnik’s work suggests that addressing bias may require building models that go beyond mere statistical correlation—potentially incorporating explicit reasoning, interpretability, and cultural awareness from the ground up.
</p>

<!-- KEY TAKEAWAYS BOX -->
<div class="noteBoxes type1">
    <strong>Key Takeaways</strong>
    <ul>
        <li>Bias in LLMs is not a removable defect—it is inherent to the architecture and training paradigm.</li>
        <li>Existing bias mitigation techniques offer limited, surface-level solutions.</li>
        <li>Fundamental rethinking of AI design is required to achieve true fairness.</li>
        <li>The “scorpion and frog” metaphor captures the inevitability of bias in current LLMs.</li>
    </ul>
</div>

<hr>

<div class="d-appendix"></div>
<dt-appendix></dt-appendix>

<script type="text/bibliography">
@article{resnik2024bias,
  title={Large Language Models Are Biased Because They Are Large Language Models},
  author={Philip Resnik},
  journal={arXiv preprint arXiv:2406.13138},
  year={2024}
}
</script>

</body>
</html>
