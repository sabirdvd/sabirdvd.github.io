<!doctype html>

<a href="https://sabirdvd.github.io" class="button"> <small>↩ </small></a>
<link rel="stylesheet" href="https://use.fontawesome.com/releases/v5.6.1/css/all.css">
<meta charset="utf-8">
<script src="template.v1.js"></script>
<script type="text/front-matter">
  title: "Review: Large Language Models Are Biased Because They Are Large Language Models"
  authors:
  - Ahmed Sabir: https://kodu.ut.ee/~ahmedabdulmajeed/
  - PUBLISHED Date
  affiliations:
  - University of Tartu : https://ut.ee/en
  -  5 August 2025
</script>

<script type="text/javascript">
    function zoom() {
        document.body.style.zoom = "100%" 
    }
</script>

<body onload="zoom()">

<style>
    .blue-color { color: blue; }
    .green-color { color: green; }
    .teal-color { color: teal; }
    .yellow-color { color: yellow; }
    .red-color { color: red; }
</style>

<head>
    <style>
        .noteBoxes {
            border: 1px solid;
            border-radius: 5px;
            padding: 10px;
            margin: 10px 0;
            width: 600px;
        }
        @media screen and (max-width: 480px){
            #wrap {
                max-width: auto;
            }
        }
        .type1 { border-color: #E76F51; background-color: rgba(231, 111, 81, 0.1); }
        .type2 { border-color: #2A9D8F; background-color: rgba(42, 157, 143, 0.1); }
        .type3 { border-color: #0096C7; background-color: rgba(0, 150, 199, 0.1); }
        .type4 { border-color: #00B353; background-color: rgba(0, 179, 83, 0.1); }
        .picture { width: 15px; padding-right: 10px; }
    </style>
</head>

<dt-article class="centered">

<h2><p align="center">Review: Large Language Models Are Biased Because They Are Large Language Models</p></h2>
<dt-byline></dt-byline>

<head>
  <meta charset="utf-8">
  <meta name="viewport" content="width=device-width">
  <title>MathJax example</title>
  <script src="https://polyfill.io/v3/polyfill.min.js?features=es6"></script>
  <script id="MathJax-script" async
          src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js">
  </script>
</head>

<body>
<p>In this post, I review Philip Resnik’s thought-provoking paper, <em>Large Language Models Are Biased Because They Are Large Language Models</em> </strong> <dt-cite key="resnik2024bias"></dt-cite>. The work challenges a common assumption in AI ethics: that harmful bias in language models is an accidental flaw that can be patched away.</p>

<p>&clubs; <strong>Main Idea</strong></p>
<p>
Resnik’s central claim is that bias in LLMs is not incidental — it is inherent to the very way these systems are built.  
Because LLMs are trained on massive datasets that reflect the social, cultural, and historical biases of human language, those biases become <em>structurally embedded</em> in the models.  
As such, even after extensive fine-tuning and filtering, bias will inevitably resurface in different forms.
</p>

<p>&clubs; <strong>Key Arguments</strong></p>
<ul>
    <li><strong>Bias is a structural property</strong>: It emerges naturally from probabilistic modeling of human language.</li>
    <li><strong>Mitigation techniques are limited</strong>: Post-processing, fine-tuning, or curated data may reduce surface-level bias but cannot remove it entirely.</li>
    <li><strong>Foundational change is needed</strong>: Addressing bias requires rethinking the architecture and training paradigm of LLMs, not just applying incremental fixes.</li>
</ul>

<p>&clubs; <strong>Metaphor: The Scorpion and the Frog</strong></p>
<p>
Resnik uses the famous fable to illustrate his point:  
A scorpion convinces a frog to carry it across a river, promising not to sting. Halfway across, it stings anyway. When the frog asks why, the scorpion replies, “Because it’s in my nature.”  
Likewise, current LLMs will continue to produce biased outputs because it is in their <em>nature</em> as language models trained on human data.
</p>

<p>&clubs; <strong>Why It Matters</strong></p>
<p>
If bias is built into the structure of current LLMs, then deploying them in sensitive domains — hiring, healthcare, education, law — risks automating and amplifying societal inequalities.  
This recognition should push the research community to explore fundamentally new AI design approaches, including hybrid systems that combine statistical learning with explicit reasoning and explainability.
</p>

<hr>

<div class="d-appendix"></div>
<dt-appendix></dt-appendix>

<script type="text/bibliography">
@article{resnik2024bias,
  title={Large Language Models Are Biased Because They Are Large Language Models},
  author={Philip Resnik},
  journal={arXiv preprint arXiv:2406.13138},
  year={2024}
}
</script>

</body>
</html>
