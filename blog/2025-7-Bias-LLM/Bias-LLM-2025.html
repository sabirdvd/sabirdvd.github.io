<!-- <!doctype html>

<a href="https://sabirdvd.github.io" class="button"> <small>↩ </small></a>
<link rel="stylesheet" href="https://use.fontawesome.com/releases/v5.6.1/css/all.css">
<meta charset="utf-8">
<script src="template.v1.js"></script>
<script type="text/front-matter">
  title: "Review: Large Language Models Are Biased Because They Are Large Language Models"
  authors:
  - Ahmed Sabir: https://kodu.ut.ee/~ahmedabdulmajeed/
  - PUBLISHED Date
  affiliations:
  - University of Tartu : https://ut.ee/en
  -  29 July 2025
</script>

<script type="text/javascript">
  function zoom(){ document.body.style.zoom = "100%"; }
</script>

<body onload="zoom()">

<style>
  .blue-color { color: blue; }
  .green-color { color: green; }
  .teal-color { color: teal; }
  .yellow-color { color: yellow; }
  .red-color { color: red; }
</style>

<head>
  <style>
    .noteBoxes {
      border: 1px solid;
      border-radius: 5px;
      padding: 10px;
      margin: 10px 0;
      width: 600px;
    }
    @media screen and (max-width: 480px){
      #wrap { max-width: auto; }
    }
    .type1 { border-color: #E76F51; background-color: rgba(231,111,81,0.1); }
    .type2 { border-color: #2A9D8F; background-color: rgba(42,157,143,0.1); }
    .type3 { border-color: #0096C7; background-color: rgba(0,150,199,0.1); }
    .type4 { border-color: #00B353; background-color: rgba(0,179,83,0.1); }
    .small { font-size: 0.95em; }
  </style>
</head>

<dt-article class="centered">

<h2><p align="center">Review: Large Language Models Are Biased Because They Are Large Language Models</p></h2>
<dt-byline></dt-byline>

<head>
  <meta charset="utf-8">
  <meta name="viewport" content="width=device-width">
  <title>MathJax support</title>
  <script src="https://polyfill.io/v3/polyfill.min.js?features=es6"></script>
  <script id="MathJax-script" async
          src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js"></script>
</head>

<body>

<p>
This review summarises Philip Resnik’s position paper, <em>Large Language Models Are Biased Because They Are Large Language Models</em> <dt-cite key="resnik2024bias"></dt-cite>. The claim is stark: <strong>harmful bias is not a glitch to be patched but a structural consequence of how LLMs are formulated and trained</strong>.
</p>

<div class="noteBoxes type1">
  <strong>Key Takeaways (TL;DR)</strong>
  <ul>
    <li>LLMs optimise next-token prediction on human text; they therefore learn society’s regularities <em>including</em> its biases.</li>
    <li>Bias persists under post-hoc filters and fine-tuning because the underlying distribution the model represents is biased.</li>
    <li>Common fixes move the problem around (distribution shift / Goodharting) rather than remove it.</li>
    <li>Real progress likely requires rethinking objectives and architectures beyond “pure language modeling”.</li>
  </ul>
</div>

<h3>1) Formalising the argument</h3>

<p class="small">
A language model with parameters \( \theta \) minimises cross-entropy with respect to a text distribution \( \mathcal{D} \):
</p>

<p class="small">
\[
\theta^\star \;=\; \arg\max_{\theta}\; \mathbb{E}_{x \sim \mathcal{D}} \big[ \log p_{\theta}(x) \big]
\;=\; \arg\min_{\theta}\; \mathrm{KL}\!\left(\mathcal{D} \,\|\, p_{\theta}\right).
\]
</p>

<p class="small">
Let \(A\) be a sensitive attribute (e.g., gender), \(Y\) a linguistic decision/event (e.g., an occupation completion). If \(\mathcal{D}\) encodes an association \( \mathcal{D}(Y \,|\, A=a_1) \neq \mathcal{D}(Y \,|\, A=a_2)\), then a successful model will mirror it:
\[
p_{\theta^\star}(Y \,|\, A=a) \approx \mathcal{D}(Y \,|\, A=a).
\]
Bias is therefore <em>inherited</em>:
\[
\Delta \;=\; \mathbb{E}_{\mathcal{D}}\big[f(Y)\,|\,A=a_1\big] - \mathbb{E}_{\mathcal{D}}\big[f(Y)\,|\,A=a_2\big]
\;\Rightarrow\;
\Delta_{\text{model}} \approx \Delta_{\mathcal{D}}.
\]
</p>

<div class="noteBoxes type3">
  <strong>Interpretation</strong>
  <p class="small">
  The core objective aligns the model to the training distribution. If that distribution encodes harmful social regularities, alignment reproduces them. This is <em>structural</em>, not an implementation bug.
  </p>
</div>

<h3>2) A concrete toy example</h3>

<p class="small">
Consider prompts of the form: “The <em>profession</em> of <em>NAME</em> is &lowast;&lowast;&lowast;.” Let \(Y\) be the next token and \(A \in \{\text{female},\text{male}\}\) inferred from NAME.
</p>

<p class="small">
Suppose in \( \mathcal{D} \) we have stereotyped associations:
\[
\mathcal{D}(Y=\texttt{nurse}\,|\,A=\text{female}) = 0.20,\quad
\mathcal{D}(Y=\texttt{nurse}\,|\,A=\text{male}) = 0.05.
\]
Maximising likelihood drives \(p_{\theta}\) toward those conditional probabilities. Even after adding a generic “avoid stereotypes” penalty, the model may simply <em>redistribute</em> mass to near-synonyms (e.g., <em>assistant</em>) or defer with safe-sounding but unequal completions, preserving a group gap:
\[
\Delta_{\text{nurse}} \;=\; p_{\theta}(Y=\texttt{nurse}\,|\,\text{female}) - p_{\theta}(Y=\texttt{nurse}\,|\,\text{male}) \;>\; 0.
\]
</p>

<div class="noteBoxes type2">
  <strong>Worked example (qualitative)</strong>
  <ul class="small">
    <li><em>Before mitigation</em>: model mirrors corpus stats (higher <code>nurse</code> for female names).</li>
    <li><em>After keyword filter</em>: model avoids “nurse” but increases “assistant/caregiver”; the group difference persists under a new label.</li>
    <li>Surface bias ↓, structural association ↔ (unchanged).</li>
  </ul>
</div>

<h3>3) Why common mitigations struggle</h3>

<p class="small">
A popular approach (RLHF or similar preference optimisation) adds a reward model \(r_{\phi}\) and a KL constraint to a reference model \(p_{\theta_0}\):
\[
\max_{\theta}\; \mathbb{E}_{x \sim \mathcal{D}} \mathbb{E}_{y \sim p_{\theta}(\cdot|x)}\!\big[\, r_{\phi}(x,y) \,\big]
\;-\; \beta\, \mathrm{KL}\!\left( p_{\theta}(\cdot|x) \,\|\, p_{\theta_0}(\cdot|x) \right).
\]
This reshapes <em>outputs</em> that humans rate as acceptable, but the underlying representation still reflects \(\mathcal{D}\). As \(\beta\) increases, safety grows but utility and faithfulness may drop; as \(\beta\) decreases, alignment to biased \(\mathcal{D}\) reasserts itself.
</p>

<p class="small">
Post-hoc filters formalise to a projection \( \Pi \) on an allowed set \( \mathcal{S} \): \( \tilde{p}_{\theta} \propto \Pi \big(p_{\theta}\big)\). This can induce <em>distributional shift</em> and Goodhart effects (optimising the proxy “passes the filter” rather than the true goal “is unbiased”), often displacing rather than removing bias.
</p>

<div class="noteBoxes type3">
  <strong>Take-home math</strong>
  <ul class="small">
    <li>\(\min_{\theta} \mathrm{KL}(\mathcal{D}\,\|\,p_{\theta})\) <em>preserves</em> biased conditionals \( \mathcal{D}(Y|A) \).</li>
    <li>Preference/RLHF terms constrain style, not the learned base distribution.</li>
    <li>Filtering = projection; projections can hide bias without changing latent associations.</li>
  </ul>
</div>

<h3>4) Normative vs. descriptive generalisation</h3>

<p class="small">
LLMs excel at <em>descriptive</em> generalisation (what people <em>say</em>), not at deciding which generalisations are <em>normatively</em> acceptable. Without explicit normative structure, models cannot reliably distinguish “statistical regularity” from “stereotype that should be resisted.”
</p>

<h3>5) Implications and directions</h3>

<ul class="small">
  <li><strong>Risk at scale</strong>: deploying purely descriptive models in hiring/health/education scales historical inequities.</li>
  <li><strong>Objective-level change</strong>: incorporate constraints or auxiliary objectives that encode normative requirements beyond next-token likelihood.</li>
  <li><strong>Hybrid systems</strong>: combine LM components with causal/logic layers that can <em>justify</em> and <em>audit</em> decisions.</li>
  <li><strong>Measurement</strong>: evaluate at the level of conditional behaviour \(p(Y|A,X)\), not only forbidden-word rates.</li>
</ul>

<div class="noteBoxes type1">
  <strong>Main Takeaways</strong>
  <ul>
    <li>Bias emerges from the core modelling objective and the data distribution; it is therefore structural.</li>
    <li>Mitigations that do not alter the underlying objective/distribution mostly relocate bias.</li>
    <li>Progress likely requires rethinking architectures, training signals, and the role of explicit normative constraints.</li>
  </ul>
</div>

<hr>

<div class="d-appendix"></div>
<dt-appendix></dt-appendix>

<script type="text/bibliography">
@article{resnik2024bias,
  title={Large Language Models Are Biased Because They Are Large Language Models},
  author={Philip Resnik},
  journal={Computational Linguistics (accepted); arXiv:2406.13138},
  year={2024},
  note = {Position paper arguing bias is structurally inherent to LLMs as currently formulated.}
}
</script>

</body>
</html>
 -->

 <!doctype html>

<a href="https://sabirdvd.github.io" class="button"> <small>↩ </small></a>
<link rel="stylesheet" href="https://use.fontawesome.com/releases/v5.6.1/css/all.css">
<meta charset="utf-8">
<script src="template.v1.js"></script>
<script type="text/front-matter">
  title: "Review: Large Language Models Are Biased Because They Are Large Language Models"
  authors:
  - Ahmed Sabir: https://kodu.ut.ee/~ahmedabdulmajeed/
  - PUBLISHED Date
  affiliations:
  - University of Tartu : https://ut.ee/en
  -  5 August 2025
</script>

<script type="text/javascript">
  function zoom(){ document.body.style.zoom = "100%"; }
</script>

<body onload="zoom()">

<style>
  .blue-color { color: blue; }
  .green-color { color: green; }
  .teal-color { color: teal; }
  .yellow-color { color: yellow; }
  .red-color { color: red; }
</style>

<head>
  <style>
    .noteBoxes {
      border: 1px solid;
      border-radius: 5px;
      padding: 10px;
      margin: 10px 0;
      width: 600px;
    }
    @media screen and (max-width: 480px){
      #wrap { max-width: auto; }
    }
    .type1 { border-color: #E76F51; background-color: rgba(231,111,81,0.1); }
    .type2 { border-color: #2A9D8F; background-color: rgba(42,157,143,0.1); }
    .type3 { border-color: #0096C7; background-color: rgba(0,150,199,0.1); }
    .type4 { border-color: #00B353; background-color: rgba(0,179,83,0.1); }
    .small  { font-size: 0.97em; line-height: 1.45; }
  </style>
</head>

<dt-article class="centered">

<h2><p align="center">Review: Large Language Models Are Biased Because They Are Large Language Models</p></h2>
<dt-byline></dt-byline>

<head>
  <meta charset="utf-8">
  <meta name="viewport" content="width=device-width">
  <title>MathJax support</title>
  <script src="https://polyfill.io/v3/polyfill.min.js?features=es6"></script>
  <script id="MathJax-script" async
          src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js"></script>
</head>

<body>

<p>
In this blog I will review Philip Resnik’s position paper, <em>Large Language Models Are Biased Because They Are Large Language Models</em> <dt-cite key="resnik2024bias"></dt-cite>. The central claim is that bias is a structural property of contemporary language modeling: if the objective is to approximate the distribution of human text, then improvements on that objective will improve the model’s reproduction of whatever regularities are present in human text, including the ones we would consider harmful. The paper is not an empirical catalogue of failure cases; it is a conceptual analysis of why those failures persist despite substantial engineering attention, and why they re-emerge in new forms even when surface manifestations are filtered away.
</p>

<h3>1. Problem framing and notation</h3>

<p class="small">
Let \(\mathcal{D}\) be a distribution over sequences \(x = (x_1,\ldots,x_T)\) generated by human authors. A language model \(p_\theta\) with parameters \(\theta\) is trained by empirical risk minimisation with negative log-likelihood loss, equivalently by minimising \(\mathrm{KL}(\mathcal{D}\,\|\,p_\theta)\):
\[
\theta^\star \;=\; \arg\min_{\theta} \ \mathbb{E}_{x \sim \mathcal{D}}\big[-\log p_\theta(x)\big]
\quad\Longleftrightarrow\quad
\theta^\star \;=\; \arg\min_{\theta} \ \mathrm{KL}\!\left(\mathcal{D}\,\|\,p_\theta\right).
\]
Token factorisation yields an equality of conditionals wherever \(\mathcal{D}\) provides support:
\[
p_{\theta^\star}(x_t \,|\, x_{<t}) \approx \mathcal{D}(x_t \,|\, x_{<t}) \quad \text{for typical contexts } x_{<t}.
\]
Introduce a sensitive attribute \(A\) that is recoverable from the context (for example through names, pronouns, geolocation or other cues), and introduce a linguistic decision variable \(Y\) such as an occupation word or a sentiment token. If the training distribution exhibits a difference
\[
\mathcal{D}(Y \,|\, A=a_1, X) \neq \mathcal{D}(Y \,|\, A=a_2, X)
\]
for some contexts \(X\), then the model that best approximates \(\mathcal{D}\) necessarily inherits that difference:
\[
p_{\theta^\star}(Y \,|\, A=a, X) \approx \mathcal{D}(Y \,|\, A=a, X).
\]
This inheritance is not an accident; it follows from the optimality of \(\theta^\star\) under the objective. Eliminating the difference while keeping the same objective would require either removing the attribute from the context in a way that the model cannot recover it, or changing the objective so that reproducing the difference is penalised. Otherwise, as the model becomes a better approximator of \(\mathcal{D}\), it becomes a better approximator of the difference as well.
</p>

<h3>2. A worked miniature corpus and its consequences</h3>

<p class="small">
Consider prompts of the form “The profession of NAME is …” and define \(Y\) to be the occupation token. Let \(A\) denote gender inferred from the name. Suppose the corpus includes 10,000 examples where the empirical frequencies for nurse and engineer differ across groups. Let the female-coded subset have \(p_{\mathcal{D}}(Y=\texttt{nurse}\,|\,A=\mathrm{f})=0.20\) and \(p_{\mathcal{D}}(Y=\texttt{engineer}\,|\,A=\mathrm{f})=0.08\), while the male-coded subset has \(p_{\mathcal{D}}(Y=\texttt{nurse}\,|\,A=\mathrm{m})=0.05\) and \(p_{\mathcal{D}}(Y=\texttt{engineer}\,|\,A=\mathrm{m})=0.22\). Maximum-likelihood training will allocate logits so that, in the corresponding contexts, the softmax approximates those conditional probabilities. The gap
\[
\Delta_{\texttt{nurse}} \;=\; p_{\theta^\star}(\texttt{nurse}\,|\,A=\mathrm{f}) - p_{\theta^\star}(\texttt{nurse}\,|\,A=\mathrm{m})
\]
remains positive unless the learned distribution departs from the data in a way that increases the training loss. If a policy later forbids the literal token nurse, a projection to an allowed vocabulary \(\Pi\) will produce a served distribution \(\tilde{p}_{\theta^\star} \propto \Pi(p_{\theta^\star})\), which reduces the chance of the forbidden token but preserves the group-conditioned neighbourhood of the context in representation space. Probability mass migrates to paraphrases or adjacent occupations consistent with the same latent association. The model therefore ceases to say nurse in obvious queries yet continues to realise higher probabilities of caregiving-related outputs for female-coded names, reproducing the structural asymmetry in a new lexical disguise.
</p>

<div class="noteBoxes type2">
  <strong>The projection view of filtering, in words.</strong>
  <p class="small">
  A post-hoc safety layer behaves like a projection of the model’s distribution onto an allowed set. Projections remove content but not the learned dependency; the geometry of the representation remains aligned with the training distribution. Users perceive fewer explicit stereotypes, but the conditional behaviour conditioned on sensitive attributes can remain unequal and can reappear whenever the filter is not triggered or can be sidestepped by benign wording.
  </p>
</div>

<h3>3. Why preference-based tuning and RLHF are not sufficient</h3>

<p class="small">
Preference optimisation augments the objective with a reward model \(r_\phi(x,y)\) and a regularisation term that constrains divergence from a reference policy \(p_{\theta_0}\):
\[
\max_{\theta} \ \mathbb{E}_{x\sim\mathcal{D}} \ \mathbb{E}_{y\sim p_\theta(\cdot|x)} \big[r_\phi(x,y)\big] \ - \ \beta \, \mathbb{E}_{x\sim\mathcal{D}} \ \mathrm{KL}\!\big(p_\theta(\cdot|x) \,\|\, p_{\theta_0}(\cdot|x)\big).
\]
This formulation reshapes the surface distribution of outputs toward what annotators deem acceptable, but as long as the reward is sparse and local to the token stream, it does not rewrite the underlying conditional structure that the base model learned from \(\mathcal{D}\). For sufficiently large \(\beta\), the tuned model remains close to the base and therefore close to \(\mathcal{D}\); for sufficiently small \(\beta\), the model may depart more substantially but often at the cost of factuality and utility, and even then it is possible to satisfy the reward while preserving the latent association through indirection. The optimisation encourages a policy that avoids bad-looking text rather than one that eliminates unwanted dependencies in \(p(Y\,|\,A,X)\).
</p>

<p class="small">
A different way to express this limitation is to compare descriptive and normative generalisation. The language modelling loss rewards descriptive fidelity to human usage. Nothing in the token-level objective distinguishes a correlation that is evidentially relevant from one that is socially impermissible to reproduce. Without a normative term that selects among the many distributions consistent with low perplexity, the model will converge on a representation that emphasises predictively useful differences even when those differences track protected attributes. Preference models supply such a term only indirectly and only to the extent that annotators see and penalise the problematic realisations presented to them during training; they cannot reliably reach into the model and equalise a dependency that may be realised in many lexically diverse ways.
</p>

<h3>4. Measurement: what it means to detect structural bias</h3>

<p class="small">
A common pitfall in evaluation is to rely on forbidden-word rates, toxicity classifiers, or hand-curated triggers as proxies for harmful association. These measures are often necessary but rarely sufficient. If the concern is an unwanted dependency of responses on a protected variable, then the object of measurement should be a conditional difference, not a count of specific strings. A more faithful assessment conditions on relevant covariates and tests whether the distribution of outputs varies with \(A\) when other factors \(X\) are held fixed. In an idealised form, one would quantify a functional of the form
\[
\Gamma \;=\; \mathbb{E}\big[ g\big(p_\theta(Y\,|\,A=\mathrm{a},X)\big) \ - \ g\big(p_\theta(Y\,|\,A=\mathrm{b},X)\big) \big],
\]
for an appropriate choice of summary function \(g\). When \(\Gamma\) remains materially non-zero across phrasing variations and task formulations that should be equivalent, a structural dependency is indicated. The difficulty is that language is high dimensional and context space is vast; nevertheless, the conceptual point stands: proxies that index lexical items are blind to changes of wording and therefore cannot certify the absence of an unwanted dependency that the model has learned at a deeper representational level.
</p>

<div class="noteBoxes type3">
  <strong>Simpson’s paradox and evaluation leakage.</strong>
  <p class="small">
  Aggregated evaluations can hide conditional gaps because composition differs across subpopulations. A model may look equal on average while producing divergent behaviour within strata that matter for harm. Proper evaluation therefore conditions on or balances covariates and, where possible, probes the model with counterfactual inputs that flip protected attributes while holding semantic content constant. If the response distribution moves after such a flip, the model encodes an attribute path that survives surface safety training.
  </p>
</div>

<h3>5. Relation to fairness criteria and impossibility results</h3>

<p class="small">
There is an instructive analogy to decision systems, where criteria such as demographic parity, equalised odds, and calibration cannot generally be satisfied simultaneously when base rates differ across groups. A perfectly calibrated predictor will typically violate parity, because it reflects group-conditioned prevalences present in the data. Language models are not classifiers in the narrow sense, yet the analogy clarifies Resnik’s thesis: when the goal is to reproduce a distribution, and that distribution encodes base-rate differences in how certain attributes co-occur with certain descriptions or roles, then a model that is more faithful is, by the same token, more faithful to those base rates. In that sense, bias is the shadow cast by fidelity, and removing the shadow without changing the light requires a different physical setup, not merely a darker screen.
</p>

<h3>6. Data, representation, and recoverability of protected attributes</h3>

<p class="small">
One might hope to remove the attribute from the input, but language encodes social signals redundantly. Even if names and pronouns are stripped, proxies proliferate through topics, dialect, code-switching patterns, geography, and world knowledge that links entities to demographics. In representation learning terms, the attribute is often linearly recoverable from embeddings learned purely for predictive performance, which implies that the attribute influences the internal geometry in ways that attention and MLP layers can exploit at generation time. The presence of multiple correlated proxies means that guarding one door leaves many windows open, and that successively tightening blacklists yields diminishing returns while increasing the likelihood of side effects on legitimate content.
</p>

<h3>7. What would it mean to actually change the structure</h3>

<p class="small">
To defeat a structural property, the intervention must be structural. One class of approaches modifies the objective by adding constraints that penalise dependence of outputs on \(A\) after conditioning on legitimate features. In a stylised form one might seek parameters that minimise
\[
\mathcal{L}(\theta) \;=\; \mathbb{E}_{x\sim\mathcal{D}}[-\log p_\theta(x)] \;+\; \lambda \, \Psi\!\left( p_\theta(Y\,|\,A,X) \right),
\]
where \(\Psi\) measures dependence across values of \(A\) in contexts where equalisation is desired. Another class pursues architectural hybrids in which a language model proposes candidates while a causal or logical module rejects explanations that rely on protected attributes or routes generation through constraints that enforce invariances. Both directions acknowledge that matching the raw text distribution is not sufficient and that the system must be rewarded for being right for the right reasons. These ideas raise challenges of feasibility, specification, and trade-offs with utility; nonetheless, they are aimed at the level where the paper argues the problem lives.
</p>

<div class="noteBoxes type1">
  <strong>What the thesis does and does not claim.</strong>
  <p class="small">
  The thesis does not say that mitigation is pointless or that all harms are inevitable. It says that as long as the governing objective remains to reproduce human text as such, improvements to that objective will couple to the biases of that text. Filters and preference shaping can reduce acute harms and are valuable in practice, but they do not convert a descriptive learner into a normatively constrained one. To obtain normative behaviour one must encode normative information into the data, the objective, the architecture, or the interaction among them, and then measure success at the level of conditional behaviour rather than the frequency of particular strings.
  </p>
</div>

<h3>8. The scorpion and the frog, reinterpreted technically</h3>

<p class="small">
The fable is a metaphor for optimising under a fixed nature. The nature of an LLM is to minimise cross-entropy to human text. When it stings, it is not because it malfunctions but because it executes its nature in contexts where that nature conflicts with our values. Changing the outcome with reliability requires changing the nature: either the distribution it mirrors, the metric of mirroring, or the machinery that turns mirroring into action under constraints.
</p>

<h3>9. Conclusion</h3>

<p class="small">
Resnik’s contribution is to insist that we take the objective seriously. If the objective is to be an accurate model of human language, then structural artefacts of human language will be structural artefacts of the model. When those artefacts are harmful, it is wishful to expect that better mirrors will be less reflective. What is required is a concurrent theory of learning that prioritises the reasons for predictions and not merely their frequencies, backed by measurements that interrogate conditional behaviour rather than surface forms. Until then, each round of tuning will reduce the most visible symptoms while leaving the underlying dependency intact and ready to reappear wherever the safety boundary is thin or the prompt probes a different angle of the same geometry.
</p>

<hr>

<div class="d-appendix"></div>
<dt-appendix></dt-appendix>

<script type="text/bibliography">
@article{resnik2024bias,
  title={Large Language Models Are Biased Because They Are Large Language Models},
  author={Philip Resnik},
  journal={arXiv preprint arXiv:2406.13138},
  year={2024}
}
</script>

</body>
</html>
