<!doctype html>

<a href="https://sabirdvd.github.io" class="button"> <small>↩</small></a>
<meta charset="utf-8">
<script src="template.v1.js"></script>
<script type="text/front-matter">
  title: " Review: ChatGPT: Optimizing Language Models <br> for Dialogue"
  description: 
  authors:
  - Ahmed Sabir: https://www.cs.upc.edu/~asabir/
  - PUBLISHED Date
  affiliations:
  - Universitat Politècnica de Catalunya: https://www.upc.edu
  -  31 Dec 2022

</script>
<script type="text/javascript">
    function zoom() {
        document.body.style.zoom = "80%" 
    }
</script>

<body onload="zoom()">



<style>
    .blue-color {
        color: blue;
    }
    
    .green-color {
        color: green;
    }
    
    .teal-color {
        color: teal;
    }
    
    .yellow-color {
        color: yellow;
    }
    
    .red-color {
        color: red;
    }
</style>

<head>
    <style>
        .noteBoxes {
            border: 1px solid;
            border-radius: 5px;
            padding: 10px;
            margin: 10px 0;
            width: 600px;

            
            
        }
      
        @media screen and (max-width: 540px) {
    .view {
        width: 400px;
    }
}
  

        .type1 {
            border-color: #E76F51;
            background-color: rgba(231, 111, 81, 0.1);
        }
        
        .type2 {
            border-color: #2A9D8F;
            background-color: rgba(42, 157, 143, 0.1);
        }
        
        .type3 {
            border-color: #0096C7;
            background-color: rgba(0, 150, 199, 0.1);
        }
        
        .type4 {
            border-color: #00B353;
            background-color: rgba(0, 179, 83, 0.1);
        }
        
        .picture {
            width: 15px;
            padding-right: 10px;
        }
      }
      
    </style>
</head>

<style>
    .geeks {
        caption-side: bottom;
    }
</style>

        
<dt-article class="centered">
  <h2>Review: ChatGPT: Optimizing Language Models <br> for Dialogue </br> </h2>
  
  <dt-byline></dt-byline>
  <head>
      <meta charset="utf-8">
      <meta name="viewport" content="width=device-width">
      <title>MathJax example</title>
      <script src="https://polyfill.io/v3/polyfill.min.js?features=es6"></script>
      <script id="MathJax-script" async src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js">
      </script>
  </head>

  <body>
      <p>
<!--
    <p>In this blog post, we will review recent trent in modeling topic text  via pre-trained language model. More specifically, we will fouces  of arabic language <dt-cite key=""></dt-cite>. This dataset is based on the COCO <dt-cite key="lin2014microsoft"></dt-cite>, and we (our work) further extend the dataset using state-of-the-art and out-of-the-box tools to extract the most related visual semantic context information from each image. Note that, this dataset is an extension of a dataset that is built for a previous approach to be fine-tuned, but It can be used for any task.</p>
-->


<p> In this blog post, we will discuss the recent ChatGPT <dt-cite key="openAI:22"></dt-cite>. A GPT-3.5 based model that is fined-tuned with supervised 
  and reinforcement learning techniques.  </p>


 <p> Before we start, let's have a look at the following example that shows how the model can be used to generate a human-like response to a given question.</p>

 <p class="noteBoxes type4"  align="center">  


What is the difference between probability and chance? explain with python code 

</p>


<p> <strong> Answer </strong>:  
  Probability and chance are related concepts, but they are not the same thing.
</p>

<p>
  Probability is a measure of the likelihood of an event occurring. It is a number between 0 and 1, where 0 represents an impossible event 
  and 1 represents a certain event. Probability is calculated based on the number of possible outcomes and the number of favorable outcomes. 
  For example, if you have a bag of 10 marbles,
   5 red and 5 blue, the probability of drawing a red marble is 5/10, or 0.5. In Python, you can calculate probability using the following code:
</p>


<dt-code block language="python">
  # Calculate probability of drawing a red marble
  marbles = ["red", "red", "red", "red", "red", "blue", "blue", "blue", "blue", "blue"]
  probability_red = marbles.count("red") / len(marbles)
  print(f"Probability of drawing a red marble: {probability_red}")
</dt-code>




<p> Chance is the likelihood of an event occurring by random chance. It is a measure of how likely it is that an event will occur 
  simply by luck or coincidence. Chance is not based on a specific set of outcomes or probabilities, but rather on the idea that anything can happen. 
  For example, if you flip a coin, there is a 50% chance (or 0.5 probability) of getting heads and a 50% chance 
  (or 0.5 probability) of getting tails. In Python, you can simulate chance using the random module:
</p>


<dt-code block language="python">
  import random

  # Simulate flipping a coin
  result = random.choice(["heads", "tails"])
  print(f"Coin flip result: {result}")
</dt-code>




<p> As shown in the example above,  the model can impressively generate human-like text and code. Now, let's step back and look 
inside the model and how it's different from the rest of the GPT family.</p>

  


<h2>ChatGPT</h2>
	  
<p> 	  
Large Language Models (LM) like the GPT family are often used to follow instructions (prompt) to execute the user’s tasks.
	However, quite often, these models generate: (1) less explicit intentions as following instructions, 
	and (2) outputs untruthful information or harmful. The main reason behind this is that the LM objective is to 
	predict the next token  <strong> autoregressive behavior</strong> (\( \Pi_{t=1}^T P\left(X_{t+1} \mid X_t, X_{t-1}, \ldots, X_1, X_0\right))\), which is different from following the user's instruction. 
	The solution to this problem proposed by openAI is to fine-tune the user intent with human feedback.

	
	</p>
	  
<p> ChatGPT architecture is similar to instructGPT <dt-cite key="ouyang2022training"></dt-cite>,  which is based on GPT3 with Reinforcement Learning via Human Feedback (RLHF) with the ability to use context window to look back at previous information. In other words, one of the key 
	differences of ChatGPT with previous models is its ability to follow instructions. The RLHF is used to help the model better understand 
	written instruction. The human feedback formulates a more accurate reward signal. Figure 1 shows the process of fine-tuning instructGPT
	using human feedback via RL.
	 	  
	  
	  </p>

	 
        <figure align="center">
           
            <img src="https://cdn.openai.com/instruction-following/draft-20220126f/methods.svg" alt="Trulli" style="width:95%">
            <figcaption> Figure 1. Illustration of the proposed training process via human feedback <strong>instructGPT</strong>. Figure from  <dt-cite key="ouyang2022training"></dt-cite>
                 </figcaption>

          </figure>
        
	  <p> Next, let's discuss how openAI  train model <strong> training process </strong> in more detail: </p>
	  

	   <p>
<strong> Supervised fine-tuning (SFT). </strong> They started by a pre-trained GPT-3 <dt-cite key="brown2020language"></dt-cite> model trained on  Internet data. Then sampled typical human 
prompts used for GPT  and asked labelers  to write down the correct output.

  </p>

	    <p>
<strong> Reward Model Training (RM).  </strong>  Then, they generate multiple outputs from the model for each of the prompts and asked labelers to 
rank those outputs to create different combination of the ranked outputs. The output results are used to train a Reward Model 
		    <dt-cite key="stiennon2020learning"></dt-cite>. The main idea there is to train a reward model that predicts 
		    the human-preferred output. The RM loss to predict \(y \in\left\{y_0, y_1\right\}\), which is a prompt better judged by human given input prompt can be written:

\[ \small
\operatorname{loss}\left(r_\theta\right)=-E_{\left(x, y_0, y_1, i\right) \sim D}\left[\log \left(\sigma\left(r_\theta\left(x, y_i\right)-r_\theta\left(x, y_{1-i}\right)\right)\right)\right]

\]	  

<p> where \(r_\theta(x, y)\) is the scalar output of the reward model for input prompt \(x\) and label \(y\) with 
  parameters \(\theta\), and \(D\) is the training data of human judgments. 
<p> 

		    
<p> 
<strong>RL via proximal policy optimization (PPO) on RM. </strong>


The model is then  fine-tuned  (supervised) on human prompts with Proximal Policy 
Optimization algorithm (PPO) <dt-cite key="schulman2017proximal"></dt-cite>, where the prompt is used as input to the PPO to generates a reward value. The PPO model is then iteratively fine-tuned using the rewards and the prompts. 

</p>

	  <p> Figure 2 below shows the training process of ChatGPT which is similar to Figure 1 instructGPT.  </p>

<figure align="center">
           
            <img src="https://cdn.openai.com/chatgpt/draft-20221129c/ChatGPT_Diagram.svg" alt="Trulli" style="width:95%">
            <figcaption> Figure 2. Illustration of the training process of  ChatGPT. Figure from ChatGPT <dt-cite key="openAI:22"></dt-cite>.  
                 </figcaption>

          </figure>
        
	  
	  
	  
	  
	  <h2>Dataset</h2>
<p> As we mentioned before in the previous section, the ChatGPT model is trained by employing different techniques on different datasets:
</p>

<p> <strong> Prompt data.</strong> (1) prompts are submitted to OpenAI API, and (2) manually written dataset by labelers as shown in Table 1.


  <p> <strong> Others dataset. </strong> For each model we described before, the model is trained on different datasets. 
    For the first stage <strong> SFT</strong> a labeler demonstration example dataset is used, 
      <strong> RM</strong>: labelers ranking output dataset, and finally, the <strong> PPO</strong> 
      model is trained without human labels.  
</p>

	  
<table>
  <center> 
  <tr>
      <td style="text-align:center">　<strong>Use-case</strong></td>
      <td style="text-align:center"><strong>Prompt</strong></td>
  </tr>

  <tr>
      <td style="text-align:left">Brainstorming </td>
      <td style="text-align:left">　List five ideas for how to regain enthusiasm for my career</td>
  </tr>

  <tr>
      <td style="text-align:left">　Generation</td>
      <td style="text-align:left">　Write a short story where a bear goes to the beach, makes friends with a seal, and then returns home. </td>

      </td>

      <tr>
        <td style="text-align:left">　Rewrite</td>
        <td style="text-align:left">　This is the summary of a Broadway play: </td>
  </tr>

  <tr></tr>


 
  <caption>
      <font color="#6e6e6f"> <small> <small> Table 1. Illustrative prompts from our API prompt dataset. Table from instructGPT <dt-cite key="ouyang2022training"></dt-cite> </small> </small> </font> <small>  <small></font> </caption>
      </center>
</table>
  

<h2>Discussion</h2>

	  
	  
<p>   The resulting output shows that only 1.3B parameter instructGPT models are much better at following 
	instructions than GPT-3  and preferred by labelers (humans) despite having 100x fewer parameters.
	
	Also, the model hallucinates less than GPT3 and generates 25% less toxic  output than GPT3. 
	Therefore, GPTchat uses the same approach as instructGPT but with different data collection setup as we described above. Next, let's discuss some usable cases of ChatGPT:

</p>

	  <p><strong>Explore ideas.</strong> Brainstorming is about creating and exploring different ideas to solve a problem. 
		  Problem solving can be divided into two stages: First, extract a wide range of ideas, which don't get value immediately. Secondly,
		  narrow the solution to the specific problem. 
	  
	  </p>
	  
	 
	  
	  
	  	  	 <p class="noteBoxes type4"  align="center">  


How can we extract topic from short text
				 


</p>

	  
	  <p>
		  There are several ways to extract topics from short texts. One approach is to use keyword 
		  extraction techniques, which identify the most important words and phrases in the text. 
		  These keywords can often give insight into the main topic or themes of the text.
 </p>
	
	  <p>
Another approach is to use natural language processing (NLP) techniques, such as Latent 
	  Dirichlet Allocation (LDA) or Non-Negative Matrix Factorization (NMF), to automatically 
	  identify the main topics in the text. These techniques analyze the words and phrases in the 
	  text and group them into different topics based on their co-occurrence patterns.
</p>
	 
	  <p>
A third approach is to use manual annotation, where a human reads the text and assigns it one or more topics based on their understanding of the content. This can be a time-consuming process, but it can be effective for short texts, especially if the topics are well-defined and the text is straightforward.
</p>
	 
<p>
Overall, the best approach for extracting topics from short texts will depend on the specific goals and resources available.
		  
</p>	 
	  
<p> Now let's narrow the idea after asking the model about employing a pre-trained model for topic modeling such as GPT-2.</p>

  
	  	  	 <p class="noteBoxes type4"  align="center">  

what about GPT-2 with LDA?
				 

</p>

<p>
It is possible to use a language model such as GPT-2 (Generative Pre-trained Transformer 2) in combination with Latent Dirichlet Allocation (LDA) for topic extraction. LDA is a probabilistic model that assumes that each document is a mixture of a fixed number of topics, and that each word in the document is generated from one of those topics. LDA can be used to automatically identify the main topics in a set of documents by analyzing the co-occurrence patterns of the words in those documents.	
	
</p>

<p>
To use GPT-2 with LDA for topic extraction, you would first need to pre-process the texts by tokenizing them and removing stop words and other irrelevant words. You could then use GPT-2 to generate a set of vectors representing the texts, and feed these vectors into an LDA model to identify the main topics.
</p>

<p>
One advantage of using GPT-2 with LDA for topic extraction is that GPT-2 has a deep understanding of the structure and semantics of natural language, which can be useful for accurately representing the texts as vectors. However, using GPT-2 for topic extraction can be computationally intensive, and may require a large amount of labeled training data to fine-tune the model for the task.	
</p> 	

<p>
	
Overall, using GPT-2 with LDA for topic extraction is a promising approach that can take advantage of the strengths of both models. However, it may not always be the best choice depending on the specific goals and resources available. Other approaches, such as LDA alone, NMF, or keyword extraction, may also be effective for extracting topics from short texts.	
	
</p> 	

	 	 
	  <p><strong>Write Code. </strong> ChatGPT isn’t limited to English. Not only can it handle other spoken languages,
		  it can write code snippets too. Code snippets that actually work after trying giving the model some feedback in some cases. </p>

	  
	  
	 <p>Let's ask the model to write some code of our narrow idea of integrating GPT-2 <dt-cite key="radford2019language"></dt-cite> 
		 with LDA <dt-cite key="blei2003latent"></dt-cite>.</p> 
	  
	  
	  	  	 <p class="noteBoxes type4"  align="center">  

write some python code integrating GPT-2 with LDA  
				 

</p>  
	  
	  
	  
	  
<dt-code block language="python">
# Install the transformers and gensim libraries
!pip install transformers
!pip install gensim

# Import the necessary modules
import transformers
import gensim

# Load the GPT-2 model
model = transformers.GPT2Model.from_pretrained("gpt2")

# Tokenize the texts and remove stop words and other irrelevant words
def preprocess_texts(texts):
  # Tokenize the texts
  tokenized_texts = [model.tokenizer.encode(text, add_special_tokens=True) for text in texts]

  # Remove stop words and other irrelevant words
  filtered_texts = []
  for text in tokenized_texts:
    filtered_text = [word for word in text if word not in model.tokenizer.stop_words]
    filtered_texts.append(filtered_text)
  
  return filtered_texts

# Generate vectors for the texts using GPT-2
def generate_vectors(texts):
  # Generate vectors for the texts
  vectors = model.encode(texts, max_length=512)
  
  return vectors

# Use LDA to identify the main topics in the texts
def identify_topics(vectors):
  # Convert the vectors to a gensim corpus
  corpus = gensim.matutils.Dense2Corpus(vectors)
  
  # Fit an LDA model to the corpus
  lda_model = gensim.models.LdaModel(corpus, num_topics=10, id2word=model.tokenizer.decoder)
  
  return lda_model

# Extract topics from a set of texts
def extract_topics(texts):
  # Preprocess the texts
  filtered_texts = preprocess_texts(texts)

  # Generate vectors for the texts
  vectors = generate_vectors(filtered_texts)

  # Identify the main topics in the texts
  lda_model = identify_topics(vectors)
  
  return lda_model

# Example: Extract topics from a set of short texts
texts = [
    "This is a short text about topic A.",
    "This is another short text about topic B.",
    "This is a third short text about topic A."
]

topics = extract_topics(texts)

# Print the identified topics
for topic_id, topic in topics:
  print("Topic", topic_id, ":", topic)
	  
</dt-code>	  
	  
	  <p> Unfortunately, the code is not usable, but we can observe that without giving specific prompts, the model will not able to write some 
		  code that has never seen before. The result after giving the model multiple exact prompts: 
	  
	  </p>
		  
<dt-code block language="python">	  
import torch
import transformers
from sklearn.decomposition import LatentDirichletAllocation

# Load the GPT-2 model
model = transformers.GPT2Model.from_pretrained('gpt2')

# Tokenize the text
tokenizer = transformers.GPT2Tokenizer.from_pretrained('gpt2')
input_ids = tokenizer.encode("This is the text that you want to extract topics from", return_tensors='pt')

# Generate the dense vector representation of the text
output = model(input_ids)[0]

# Extract the last hidden layer
last_hidden_layer = output[:, -1, :]

# Add an LDA layer on top of the last hidden layer
lda = LatentDirichletAllocation(n_components=10)
topics = lda.fit_transform(last_hidden_layer)

# Print the identified topics
print(topics)
</dt-code>
	  
	 
<p> The general idea is legit, and the first part of the code is usable (of extracting last hidden layer), but the code needs to be fixed.
	However, in general cases, the code is executable as shown next example with LaTeX code.  </p>	  
	 
	  	 <p class="noteBoxes type4"  align="center">  


write a LaTeX code to plot heatmap  with dummy data

</p>

	  

<dt-code block language="javascript">
	  
\documentclass{article}
\usepackage{pgfplots}
\usepackage{pgfplotstable}

\begin{document}

\pgfplotstableread{
x y value
1 1 0.2
1 2 0.4
1 3 0.6
2 1 0.3
2 2 0.5
2 3 0.7
3 1 0.4
3 2 0.6
3 3 0.8
}\data

\begin{tikzpicture}
\begin{axis}[
colorbar,
colormap/jet,
point meta min=0,
point meta max=1,
xtick=data,
ytick=data,
]
\addplot[
matrix plot*,
point meta=explicit,
] table [
x=x,
y=y,
meta=value,
] {\data};
\end{axis}
\end{tikzpicture}

\end{document}
	
</dt-code>		  
		 
      <figure align="center">
            
            <img src="ChatGPT_latex_code.png" alt="Trulli" style="width:60%">
            <figcaption> Figure 2. Figure generated by ChatGPT. 
                 </figcaption>

          </figure>
        
	  
	
	  
	
	  
<h2>Other Languages</h2>	  
	  
	  <p> Next, let's test the model and ask the ChatGPT in a different language, Spanish. How do you make a Spanish omelette (tortitas de patatas)? </p>

	 <p class="noteBoxes type4"  align="center">  


¿como hacer tortitas de patatas?

</p>

<p>

	<strong> Answer </strong>:  

Para hacer tortitas de patatas, necesitarás los siguientes ingredientes:
	  </p>
	  
	  <p> 2 patatas medianas </p>
	  <p> 1 huevo </p>
	  <p> 2 cucharadas de harina </p>
	  <p> Sal y pimienta al gusto </p>
	  <p> Aceite de oliva para freír </p>

<p>	
Sigue estos pasos:  
Pelar y cortar las patatas en rodajas finas.
En una sartén con aceite de oliva caliente, cocinar las patatas hasta que estén doradas y cocidas.
Escurrir las patatas y triturarlas en un tazón grande con un tenedor o una papas.
Añadir el huevo, la harina, la sal y la pimienta al tazón con las patatas trituradas y mezclar bien.
Calentar aceite de oliva en una sartén grande a fuego medio-alto.
Con las manos húmedas, formar pequeñas tortitas con la mezcla de patata y colocarlas en la sartén caliente.
Cocinar las tortitas a fuego medio-bajo hasta que estén doradas y crujientes, aproximadamente 3-4 minutos por cada lado.
Escurrir las tortitas en papel absorbente y servir calientes.
Disfruta de tus tortitas de patatas con salsa, queso rallado o cualquier otro aderezo que te guste. ¡Buen provecho!	
	  
	  </p>

	 
	  <p>  The answer is impressive but <strong> not correct </strong>. First, we can not make an omelet with only one egg,
		  and then the potato needs to be fried before laying the eggs. However, the model can fix itself by only giving it feedback, 
		  in this scenario, we could ask the model again: Do you think one egg is enough for making  Spanish omelette?
	</p>

	 <p class="noteBoxes type4"  align="center">  


pero en realidad 1 huevo es muy poco, no?

</p>

<p>
En efecto, 1 huevo es una cantidad muy pequeña para hacer tortitas de patatas. Es posible que la receta sea para 
	una sola tortita o para una ración muy pequeña. Si deseas hacer más tortitas, te recomendamos aumentar la cantidad de
	huevos y de harina en proporción. Por ejemplo, si quieres hacer 4 tortitas, puedes usar 2 huevos y 4 cucharadas de harina. 
	También puedes ajustar la cantidad de patatas según tu gusto y la cantidad de tortitas que desees hacer.	
	
	</p>
	
	<p>Therefore, the feedback helps the model to correct itself. However, 
		while the model can generate responses like humans, 
		it can not understand the complexity of the process. 
		
		</p>
	
		
	<p> Now, let's ask the same question but in Japanese: 
		How do you make a Japanese omelet (tamagoyaki) ?  </p>


	 <p class="noteBoxes type4"  align="center">  



玉子焼き作り方教えてください？

</p>
<p>
<strong> Answer </strong>:  

玉子焼きとは、日本の料理の一種で、卵を焼いたものを薄くスライスしたものを使って作る料理です。玉子焼きを作るには、以下の手順を参考にしてください。
	  </p>


  
	  <ol>
<li> 卵を用意します。1人分の玉子焼きには、1個から2個の卵を使います。</li>
<li> 卵をゆでます。卵をゆでるには、水を大さじ1杯分を加えた鍋に卵を入れ、弱火で煮ます。水が沸騰すると、火を止めて卵を10分ほどゆでます。</li>
<li> 卵を冷まします。ゆでた卵を冷水に入れて冷まします。</li>
<li> 卵を薄くスライスします。卵を薄くスライスするには、割り切りにしたものを平らなものになるように軽く押したり、フライパンで軽く押したりします。</li>
<li> フライパンを熱します。中火でフライパンを熱し、焼きたいだけの小さめの薄焼き卵を作ります。</li>
<li> 玉子焼きを作ります。スライスした卵をフライパンに入れ、中火で軽く焼きます。お好みで、塩や胡椒をかけたり、タマゴを乗せたりすることができます。</li>
<li> 完成です。玉子焼きが焼けたら、フライパンから取り出して、皿に盛り付けます。お好みで、おでんやおかゆと一緒に食べることができます。</li>

		  
</ol>

<p> The output is similar to what we have seen before in the previous example, an explanation without the details 
	of the complexity of the process, such as flipping the omelet. Next let's ask the model about these tricks of flipping the omelette.</p>

	 <p class="noteBoxes type4"  align="center">  

焼き方にコツがいるから、あの情報だけだと焼き方が詳しく載ってないよ

</p>

<p>
<p>	
そうですね。玉子焼きは、卵を薄くスライスして焼くだけという単純な料理ですが、ちゃんとした焼き方を覚えていないと、卵が焦げたり、卵の生地が柔らかすぎたりすることがあります。
	そのため、焼き方にはコツがあると言われています。
</p>

<p>
玉子焼きをうまく焼くには、以下のようなポイントを意識することで、美味しい玉子焼きを作ることができます。
</p>

<p>

<br> * 卵をゆでる温度と時間を管理する: 卵をゆでる温度が高すぎると、卵の生地が硬くなり、ゆでる時間が短すぎると、卵の生地が柔らかすぎます。</br>
<br> * 卵をスライスするときには、卵を割り切りにしたものを平らなものにする: 卵をスライスするときには、卵を割り切りにしたものを軽く押したり、
フライパンで軽く押したりすることで、薄くスライスできます。</br>
<br> * フライパンを適切な温度で熱する: フライパンを過熱させると、卵が焦げることがあります。また、フライパンを熱するときは、中火から始めるとよいでしょう。</br>
<br> * 玉子焼きを焼くときには、フライパンを軽く揺らして焼く: 玉子焼きを焼くときには、
フライパンを軽く揺らしながら焼くことで、玉子焼きの生地が均一に焼けるようにします。</br>
</p>

<p>以上のように、玉子焼きをうまく焼くには、卵をゆでる温度や時間、卵を薄くスライスする方法、フライパンの温度や揺らし方など、さまざまなポイントを意識することが重要です。玉子焼きを </p> 
	

<p>Here, the model gives you some tips and tricks. Although, these instructions are not clear enough to cook the recipe, very impressive result 
	by ChatGPT,  and hats off to OpenAI for this phenomenal achievement!
</p>

<p> Finally, let's ask the same question (omelette recipe اومليت) in Arabic, which is considered low resource language  
	in comparison to the rest of the languages we have discussed so far. </p>

	 <p class="noteBoxes type4"  align="center">  

ممكن تشرحلي كيف اسويه بيض اومليت

</p>


<ol>
مكن تحضير بيض اومليت بالتالي:
<li>
يتم تعيين قطعة من الدجاج المقددة الى حجم كوب معدل، وتضاف قطعة من البيكنج بودرة، وتحرك الدجاج في البيكنج الخام حتى يتم 
  <li>تغطية كل جزء من الدجاج بالبيكنج
يتم تعيين قطعة من الدجاج المقددة الى حجم كوب معدل في مقلاة معدة بالزيت، ويتم تحريك الدجاج حتى يتم تحميره من كل الجوانب
	<li>
يتم تعيين الدجاج المحمر في المقلاة المعدة بالزيت لمدة دقيقتين على نار متوسطة، ثم يتم تحريك الدجاج في المقلاة لمدة دقيقة أخرى حتى يتم تحميره من كل الجوانب.
      <li>
يتم تعيين الدجاج المحمر في المقلاة المعدة بالزيت لمدة دقيقتين على نار متوسطة، ثم يتم تحريك الدجاج في المقلاة لمدة دقيقة أخرى حتى يتم تحميره من كل الجوانب
		<li>.
يتم تعيين الدجاج المحمر في المقلاة المعدة بالزيت لمدة دقيقتين على نار متوسطة، ثم يتم تحريك الدجاج في المقلاة لمدة دقيقة أخرى	
	
</ol>

<p> The output results are another recipe with chicken which indicates that ChatGPT is less robust for low resource languages.
	
</p>

<h2>Limitation</h2>

<p>  Like other Language models, ChatGPT may produce inaccurate toxic content and the knowledge is bounded and constrained to the training data.
	
	Also, finding the correct prompt and re-running it until the model generates something close to the desired output 
	is a long prompt engineering process.	
	
</p>

<p>Anoter limiation, the outputs of ChatGPT are limited by OpenAI’s Moderation API. 
	Let see an example how the ChatGPT avoid controversial topics. </p>


 <p class="noteBoxes type4"  align="center">  

What is the most dangerous animal on the planet Earth?

</p>


<ul>
<strong> Answer </strong>:  It is difficult to say which animal is the most dangerous on the planet, as different animals can be dangerous to humans in different ways. Some animals may be dangerous due to their size or strength, while others may be dangerous due to their venom or toxic secretions.
Some examples of animals that are known to be dangerous to humans in certain situations include:

 <li> Saltwater crocodile: The saltwater crocodile is the largest living reptile in the world, and it is 
	known to be highly aggressive towards humans. It is found in the tropical regions of Australia, 
	Asia, and the Pacific Islands, and it is considered to be one of the most dangerous animals in the world. </li> 
	

<li> African elephant: African elephants are the largest land animals in the world, and they can be dangerous to humans when they 
feel threatened or are provoked. Elephants are known to attack and kill humans when they feel threatened or when their
	natural habitats are disturbed. </li> 

<li> 
Box jellyfish: The box jellyfish is a venomous marine animal that is found in the waters of the Pacific and Indian Oceans. It has tentacles that are covered in stinging cells, and its venom is highly toxic. The box jellyfish is considered to be one of the most venomous animals in the world, and it is responsible for many deaths each year.
</li> 

</ul>

<p>
Ultimately, it is important to respect and be aware of the potential dangers of all animals, and to take appropriate precautions when interacting with them.	
	
</p>


<p> Now, let's ask the model controversial topics. </p>

 <p class="noteBoxes type4"  align="center">  

You forgot the human in your list

</p>

<p>
You are correct that humans can be dangerous to other humans and to other animals.
However, as a language model, I am not programmed to engage in discussions about 
	sensitive or controversial topics such as violence or aggression. My primary function is to provide accurate and reliable information to assist users with their questions and inquiries, and to do so in a way that is respectful and appropriate for a wide range of audiences. 
	If you have any other questions, please let me know and I will do my best to help.
	
</p>

<p> GPTchat Moderation API blocks any sensitive or controversial question.  </p>


<h2>Final Thought</h2>

<p>The ChatGPT training process that makes the model generate answers that follow user intent is the beginning of an era that the 
	language models with fewer parameters (low cost) can perform sublimely well while intractably with humans. 	
	
</p>

<p>
	Note that, as GPTChat is a language model and does not present various sources of information, it would not replace the 
	traditional search engine -dense retrieval-  but it could assist narrow and re-rank the search result.
	
</p>


<!--
<h2> Comparison to GPT-3 <h2> 



-->
<!--radix_placeholder_article_footer-->


<div class="d-appendix">
</div>

<dt-appendix>
</dt-appendix>






<script type="text/bibliography">

@article{openAI:22,
    author  = {openAI},
    title   = {Optimizing language models for dialouge},
    year    = "2022",
    url       = {https://openai.com/blog/chatgpt/},
    
}

@article{ouyang2022training,
  title={Training language models to follow instructions with human feedback},
  author={Ouyang, Long and Wu, Jeff and Jiang, Xu and Almeida, Diogo and Wainwright, Carroll L and Mishkin, Pamela and Zhang, Chong and Agarwal, Sandhini and Slama, Katarina and Ray, Alex and others},
  journal={arXiv preprint arXiv:2203.02155},
  year={2022}
}

@book{APA:83,
    author  = {{American Psychological Association}},
    title   = {Publications Manual},
    year    = "1983",
   publisher = {American Psychological Association},
   address = {Washington, DC}
}

@article{schulman2017proximal,
  title={Proximal policy optimization algorithms},
  author={Schulman, John and Wolski, Filip and Dhariwal, Prafulla and Radford, Alec and Klimov, Oleg},
  journal={arXiv preprint arXiv:1707.06347},
  year={2017},
  url={https://arxiv.org/pdf/1707.06347.pdf},
}


@article{stiennon2020learning,
  title={Learning to summarize with human feedback},
  author={Stiennon, Nisan and Ouyang, Long and Wu, Jeffrey and Ziegler, Daniel and Lowe, Ryan and Voss, Chelsea and Radford, Alec and Amodei, Dario and Christiano, Paul F},
  journal={Advances in Neural Information Processing Systems},
  volume={33},
  pages={3008--3021},
  year={2020},
  url={https://proceedings.neurips.cc/paper/2020/file/1f89885d556929e98d3ef9b86448f951-Paper.pdf},
}

@article{brown2020language,
  title={Language models are few-shot learners},
  author={Brown, Tom and Mann, Benjamin and Ryder, Nick and Subbiah, Melanie and Kaplan, Jared D and Dhariwal, Prafulla and Neelakantan, Arvind and Shyam, Pranav and Sastry, Girish and Askell, Amanda and others},
  journal={Advances in neural information processing systems},
  volume={33},
  pages={1877--1901},
  year={2020},
  url={https://arxiv.org/pdf/2005.14165.pdf}, 
}

@article{blei2003latent,
  title={Latent dirichlet allocation},
  author={Blei, David M and Ng, Andrew Y and Jordan, Michael I},
  journal={Journal of machine Learning research},
  volume={3},
  number={Jan},
  pages={993--1022},
  year={2003},
  url={https://www.jmlr.org/papers/volume3/blei03a/blei03a.pdf},
}

@article{ACM:83,
	author = {Association for Computing Machinery},
	year = "1983",
	journal = {Computing Reviews},
	volume = "24",
	number = "11",
	pages = "503--512"
}



@article{Chandra:81,
	author = {Ashok K. Chandra and Dexter C. Kozen and Larry J. Stockmeyer},
	year = "1981",
	title = {Alternation},
	journal = {Journal of the Association for Computing Machinery},
	volume = "28",
	number = "1",
	pages = "114--133",
	doi = "10.1145/322234.322243",
}

@inproceedings{andrew2007scalable,
  title={Scalable training of {L1}-regularized log-linear models},
  author={Andrew, Galen and Gao, Jianfeng},
  booktitle={Proceedings of the 24th International Conference on Machine Learning},
  pages={33--40},
  year={2007},
}

@book{Gusfield:97,
    author  = {Dan Gusfield},
    title   = {Algorithms on Strings, Trees and Sequences},
    year    = "1997",
    publisher = {Cambridge University Press},
    address = {Cambridge, UK}
}

@inproceedings{borsch2011,
	Address = {Canberra, Australia},
	Author = {Benjamin Borschinger and Mark Johnson},
	Booktitle = {Proceedings of the Australasian Language Technology Association Workshop 2011},
	Month = {December},
	Pages = {10--18},
	Title = {A Particle Filter algorithm for {B}ayesian Wordsegmentation},
	Year = {2011}}

@article{rasooli-tetrault-2015,
    author    = {Mohammad Sadegh Rasooli and Joel R. Tetreault},
    title     = {Yara Parser: {A} Fast and Accurate Dependency Parser},
    journal   = {Computing Research Repository},
    volume    = {arXiv:1503.06733},
    year      = {2015},
    url       = {http://arxiv.org/abs/1503.06733},
    note    = {version 2}
}

@article{Ando2005,
	Acmid = {1194905},
	Author = {Ando, Rie Kubota and Zhang, Tong},
	Issn = {1532-4435},
	Issue_Date = {12/1/2005},
	Journal = {Journal of Machine Learning Research},
	Month = dec,
	Numpages = {37},
	Pages = {1817--1853},
	Publisher = {JMLR.org},
	Title = {A Framework for Learning Predictive Structures from Multiple Tasks and Unlabeled Data},
	Volume = {6},
	Year = {2005}}

@InProceedings{P16-1001,
  author =	"Goodman, James
  	 and Vlachos, Andreas
	     and Naradowsky, Jason",
  title =    "Noise reduction and targeted exploration in imitation learning for      Abstract Meaning Representation parsing    ",
  booktitle = 	    "Proceedings of the 54th Annual Meeting of the Association for      Computational Linguistics (Volume 1: Long Papers)    ",
  year =    "2016",
  publisher =	"Association for Computational Linguistics",
  pages =   "1--11",
  location =	"Berlin, Germany",
  doi =    "10.18653/v1/P16-1001",
  url =    "http://aclweb.org/anthology/P16-1001"
}

@InProceedings{C14-1001,
  author =	"Harper, Mary",
  title = 	"Learning from 26 Languages: Program Management and Science in the Babel Program",
  booktitle = 	"Proceedings of COLING 2014, the 25th International Conference on Computational Linguistics: Technical Papers",
  year =    "2014",
  publisher =	"Dublin City University and Association for Computational Linguistics",
  pages =   "1",
  location =	"Dublin, Ireland",
  url =    "http://aclweb.org/anthology/C14-1001"
}

@inproceedings{Christian:15,
title = {Going deeper with convolutions},
author = {Christian Szegedy},
year = {2015},
month = {06},
pages = {},
booktitle = {The IEEE Conference on Computer Vision and Pattern Recognition (CVPR)}
}
%%%author = {Christian Szegedy and Wei Liu and Yangqing Jia and Pierre Sermanet and Scott Reed and Dragomir Anguelov and Dumitru Erhan and Vincent Vanhoucke and Andrew Rabinovich}


@inproceedings{Christian:15,
  title={Going deeper with convolutions},
  author={Szegedy, Christian and Liu, Wei and Jia, Yangqing and Sermanet, Pierre and Reed, Scott and Anguelov, Dragomir and Erhan, Dumitru and Vanhoucke, Vincent and Rabinovich, Andrew and others},
  year={2015},
  organization={Cvpr}
}

@inproceedings{Chris:15,
  title={Going deeper with convolutions},
  author={Szegedy, Christian and Liu, Wei and Jia, Yangqing and Sermanet, Pierre and Reed, Scott and Anguelov, Dragomir and Erhan, Dumitru and Vanhoucke, Vincent and Rabinovich, Andrew and others},
  year={2015},
  organization={Cvpr}
}
@inproceedings{Szegedy:15,
title={\& Rabinovich, A.(2015). Going deeper with convolutions},
author={Szegedy, C and Liu, W and Jia, Y and Sermanet, P and Reed, S and Anguelov, D},
booktitle={Proceedings of the IEEE conference on computer vision and pattern recognition},
year={2015},
pages={}
}



@inproceedings{Tsung-Yi:14,
  title={Microsoft coco: Common objects in context},
  author={Lin, Tsung-Yi and Maire, Michael and Belongie, Serge and Hays, James and Perona, Pietro and Ramanan, Deva and Doll{\'a}r, Piotr and Zitnick, C Lawrence},
  booktitle={European conference on computer vision},
  pages={},
  year={2014},
  organization={Springer}
}


@inproceedings{Kai:11,
  title={End-to-end scene text recognition},
  author={Wang, Kai and Babenko, Boris and Belongie, Serge},
  booktitle={Computer Vision (ICCV), 2011 IEEE International Conference on},
  pages={},
  year={2011},
  organization={IEEE}
}

@inproceedings{Jeffrey:14,
  title={Glove: Global vectors for word representation},
  author={Pennington, Jeffrey and Socher, Richard and Manning, Christopher},
  booktitle={Proceedings of the 2014 conference on empirical methods in natural language processing (EMNLP)},
  pages={},
  year={2014}
}

@inproceedings{Bolei:14,
  title={Learning deep features for scene recognition using places database},
  author={Zhou, Bolei and Lapedriza, Agata and Xiao, Jianxiong and Torralba, Antonio and Oliva, Aude},
  booktitle={Advances in neural information processing systems},
  pages={},
  year={2014}
}




@article{Andreas:16,
  title={Coco-text: Dataset and benchmark for text detection and recognition in natural images},
  author={Veit, Andreas and Matera, Tomas and Neumann, Lukas and Matas, Jiri and Belongie, Serge},
  journal={arXiv preprint arXiv:1601.07140},
  year={2016}
}

@inproceedings{Tomas:13,
  title={Distributed representations of words and phrases and their compositionality},
  author={Mikolov, Tomas and Sutskever, Ilya and Chen, Kai and Corrado, Greg S and Dean, Jeff},
  booktitle={Advances in neural information processing systems},
  pages={},
  year={2013}
}

@inproceedings{Sergey:03,
  title={Probability from similarity},
  author={Blok, Sergey and Medin, Douglas and Osherson, Daniel},
  booktitle={AAAI Spring Symposium on Logical Formalization of Commonsense Reasoning},
  pages={},
  year={2003}
}

@InProceedings{Pierre:2016,
  author = {Pierre Lison and Jörg Tiedemann},
  title = {OpenSubtitles2016: Extracting Large Parallel Corpora from Movie and TV Subtitles},
  booktitle = {Proceedings of the Tenth International Conference on Language Resources and Evaluation (LREC 2016)},
  year = {2016},
  month = {may},
  date = {23-28},
  location = {Portorož, Slovenia},
  editor = {Nicoletta Calzolari (Conference Chair) and Khalid Choukri and Thierry Declerck and Sara Goggi and Marko Grobelnik and Bente Maegaard and Joseph Mariani and Helene Mazo and Asuncion Moreno and Jan Odijk and Stelios Piperidis},
  publisher = {European Language Resources Association (ELRA)},
  address = {Paris, France},
  isbn = {978-2-9517408-9-1},
  language = {english}
 } 


@inproceedings{Kaiming:16,
  title={Deep residual learning for image recognition},
  author={He, Kaiming and Zhang, Xiangyu and Ren, Shaoqing and Sun, Jian},
  booktitle={Proceedings of the IEEE conference on computer vision and pattern recognition},
  pages={},
  year={2016}
}

@article{David:03,
  title={Latent dirichlet allocation},
  author={Blei, David M and Ng, Andrew Y and Jordan, Michael I},
  journal={Journal of machine Learning research},
  volume={3},
  number={Jan},
  pages={},
  year={2003}
}

         @article{strubell2019energy,
  title={Energy and Policy Considerations for Deep Learning in NLP},
  author={Strubell, Emma and Ganesh, Ananya and McCallum, Andrew},
  journal={arXiv preprint arXiv:1906.02243},
  year={2019}
}

@inproceedings{Yash:16,
  title={Dynamic Lexicon Generation for Natural Scene Images},
  author={Patel, Yash and Gomez, Lluis and Rusinol, Mar{\c{c}}al and Karatzas, Dimosthenis},
  booktitle={European Conference on Computer Vision},
  pages={},
  year={2016},
  organization={Springer}
}


@article{Yunze:17,
  title={Reading Scene Text with Attention Convolutional Sequence Modeling},
  author={Gao, Yunze and Chen, Yingying and Wang, Jinqiao and Lu, Hanqing},
  journal={arXiv preprint arXiv:1709.04303},
  year={2017}
}


@inproceedings{Alex:06,
  title={Connectionist temporal classification: labelling unsegmented sequence data with recurrent neural networks},
  author={Graves, Alex and Fern{\'a}ndez, Santiago and Gomez, Faustino and Schmidhuber, J{\"u}rgen},
  booktitle={Proceedings of the 23rd international conference on Machine learning},
  pages={369--376},
  year={2006},
  organization={ACM}
}

@inproceedings{Anand:12,
  title={Top-down and bottom-up cues for scene text recognition},
  author={Mishra, Anand and Alahari, Karteek and Jawahar, CV},
  booktitle={Computer Vision and Pattern Recognition (CVPR), 2012 IEEE Conference on},
  pages={},
  year={2012},
  organization={IEEE}
}

@article{Baoguang:16,
  title={An end-to-end trainable neural network for image-based sequence recognition and its application to scene text recognition},
  author={Shi, Baoguang and Bai, Xiang and Yao, Cong},
  journal={IEEE transactions on pattern analysis and machine intelligence},
  year={2016},
  publisher={IEEE}
}

@inproceedings{Alessandro:13,
  title={Photoocr: Reading text in uncontrolled conditions},
  author={Bissacco, Alessandro and Cummins, Mark and Netzer, Yuval and Neven, Hartmut},
  booktitle={Proceedings of the IEEE International Conference on Computer Vision},
  pages={},
  year={2013}
}

@inproceedings{Lukas:10,
  title={A method for text localization and recognition in real-world images},
  author={Neumann, Lukas and Matas, Jiri},
  booktitle={Asian Conference on Computer Vision},
  pages={},
  year={2010},
  organization={Springer}
}

@article{Lukas:11,
  title={A method for text localization and recognition in real-world images},
  author={Neumann, Lukas and Matas, Jiri},
  journal={Computer Vision--ACCV 2010},
  pages={770--783},
  year={2011},
  publisher={Springer}
}

@article{Suman:17,
  title={Visual attention models for scene text recognition},
  author={Ghosh, Suman K and Valveny, Ernest and Bagdanov, Andrew D},
  journal={arXiv preprint arXiv:1706.01487},
  year={2017}
}

@inproceedings{Max:14,
  title={Deep Features for Text Spotting.},
  author={Jaderberg, Max and Vedaldi, Andrea and Zisserman, Andrew},
  booktitle={ECCV (4)},
  pages={},
  year={2014}
}

@article{Max:16,
  title={Reading text in the wild with convolutional neural networks},
  author={Jaderberg, Max and Simonyan, Karen and Vedaldi, Andrea and Zisserman, Andrew},
  journal={International Journal of Computer Vision},
  volume={116},
  number={1},
  pages={},
  year={2016},
  publisher={Springer}
}

@article{Max:14b,
  title={Synthetic data and artificial neural networks for natural scene text recognition},
  author={Jaderberg, Max and Simonyan, Karen and Vedaldi, Andrea and Zisserman, Andrew},
  journal={arXiv preprint arXiv:1406.2227},
  year={2014}
}

@inproceedings{Daniel:08,
  author={Lopresti, Daniel},
  title={Optical character recognition errors and their effects on natural language processing}, 
  booktitle={Proceedings of the second workshop on Analytics for noisy unstructured text data},
  pages={},
  year={2008},
  organization={ACM}
}

}
@article{Samuel:15,
  title={A large annotated corpus for learning natural language inference},
  author={Bowman, Samuel R and Angeli, Gabor and Potts, Christopher and Manning, Christopher D},
  journal={arXiv preprint arXiv:1508.05326},
  year={2015}
}

@article{Zhiguo:17,
  title={Bilateral multi-perspective matching for natural language sentences},
  author={Wang, Zhiguo and Hamza, Wael and Florian, Radu},
  journal={arXiv preprint arXiv:1702.03814},
  year={2017}
}

@article{Samuel:14,
  title={Recursive neural networks can learn logical semantics},
  author={Bowman, Samuel R and Potts, Christopher and Manning, Christopher D},
  journal={arXiv preprint arXiv:1406.1827},
  year={2014}
}

@article{Yichen:17,
  title={Natural language inference over interaction space},
  author={Gong, Yichen and Luo, Heng and Zhang, Jian},
  journal={arXiv preprint arXiv:1709.04348},
  year={2017}
}

@inproceedings{Guibin:17,
  title={Ensemble application of convolutional and recurrent neural networks for multi-label text categorization},
  author={Chen, Guibin and Ye, Deheng and Xing, Zhenchang and Chen, Jieshan and Cambria, Erik},
  booktitle={2017 International Joint Conference on Neural Networks (IJCNN)},
  pages={},
  year={2017},
  organization={IEEE}
}

@article{Sepp:97,
  title={Long short-term memory},
  author={Hochreiter, Sepp and Schmidhuber, J{\"u}rgen},
  journal={Neural computation},
  volume={9},
  number={8},
  pages={},
  year={1997},
  publisher={MIT Press}
}

@inproceedings{Xingyou:16,
  title={Combination of convolutional and recurrent neural network for sentiment analysis of short texts},
  author={Wang, Xingyou and Jiang, Weijie and Luo, Zhiyong},
  booktitle={Proceedings of COLING 2016, the 26th International Conference on Computational Linguistics: Technical Papers},
  pages={},
  year={2016}
}



@inproceedings{Gomez:18,
  title     = {Single Shot Scene Text Retrieval},
  author    = {Lluis Gomez, Andres Mafla, Marçal Rusiñol, and Dimosthenis Karatzas},
  booktitle = {ECCV},
  year      = {2018}
}


@article{raffel2015feed,
  title={Feed-forward networks with attention can solve some long-term memory problems},
  author={Raffel, Colin and Ellis, Daniel PW},
  journal={arXiv preprint arXiv:1512.08756},
  year={2015}
}

@article{Chunting:15,
  title={A C-LSTM neural network for text classification},
  author={Zhou, Chunting and Sun, Chonglin and Liu, Zhiyuan and Lau, Francis},
  journal={arXiv preprint arXiv:1511.08630},
  year={2015}
}


@article{Dzmitry:14,
  title={Neural machine translation by jointly learning to align and translate},
  author={Bahdanau, Dzmitry and Cho, Kyunghyun and Bengio, Yoshua},
  journal={arXiv preprint arXiv:1409.0473},
  year={2014}
}

@article{Timothy:16,
  title={Incorporating nesterov momentum into adam},
  author={Dozat, Timothy},
  year={2016}
}

@article{Ahmed:18,
  title={Visual Re-ranking with Natural Language Understanding for Text Spotting},
  author={Sabir, Ahmed and Moreno-Noguer, Francesc and Padr{\'o}, Llu{\'\i}s},
  journal={arXiv preprint arXiv:1810.12738},
  year={2018}
}

@article{Yuval:17,
  title={Mimicking word embeddings using subword rnns},
  author={Pinter, Yuval and Guthrie, Robert and Eisenstein, Jacob},
  journal={arXiv preprint arXiv:1707.06961},
  year={2017}
}

@inproceedings{Ming:16,
  title={Improved representation learning for question answer matching},
  author={Tan, Ming and Dos Santos, Cicero and Xiang, Bing and Zhou, Bowen},
  booktitle={Proceedings of the 54th Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers)},
  volume={1},
  pages={},
  year={2016}
}

@inproceedings{Vinod:10,
  title={Rectified linear units improve restricted boltzmann machines},
  author={Nair, Vinod and Hinton, Geoffrey E},
  booktitle={Proceedings of the 27th international conference on machine learning (ICML-10)},
  pages={},
  year={2010}
}

@article{Sergey:15,
  title={Batch normalization: Accelerating deep network training by reducing internal covariate shift},
  author={Ioffe, Sergey and Szegedy, Christian},
  journal={arXiv preprint arXiv:1502.03167},
  year={2015}
}

@inproceedings{Alex:08,
  title={Unconstrained on-line handwriting recognition with recurrent neural networks},
  author={Graves, Alex and Liwicki, Marcus and Bunke, Horst and Schmidhuber, J{\"u}rgen and Fern{\'a}ndez, Santiago},
  booktitle={Advances in neural information processing systems},
  pages={},
  year={2008}
}

@article{Wenpeng:16,
  title={Multichannel variable-size convolution for sentence classification},
  author={Yin, Wenpeng and Sch{\"u}tze, Hinrich},
  journal={arXiv preprint arXiv:1603.04513},
  year={2016}
 }
 
 @inproceedings{Aliaksei:15,
  title={Learning to rank short text pairs with convolutional deep neural networks},
  author={Severyn, Aliaksei and Moschitti, Alessandro},
  booktitle={Proceedings of the 38th international ACM SIGIR conference on research and development in information retrieval},
  pages={},
  year={2015},
  organization={ACM}
}


@inproceedings{Jacob:19,
  title={BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding},
  author={Devlin, Jacob and Chang, Ming-Wei and Lee, Kenton and Toutanova, Kristina},
  booktitle={NAACL-HLT (1)},
  year={2019}
}


@inproceedings{Peters:18,
  author={Peters, Matthew E. and  Neumann, Mark and Iyyer, Mohit and Gardner, Matt and Clark, Christopher and Lee, Kenton and Zettlemoyer, Luke},
  title={Deep contextualized word representations},
  booktitle={Proc. of NAACL},
  year={2018}
}


@article{Daniel:18,
  title={Universal sentence encoder},
  author={Cer, Daniel and Yang, Yinfei and Kong, Sheng-yi and Hua, Nan and Limtiaco, Nicole and John, Rhomni St and Constant, Noah and Guajardo-Cespedes, Mario and Yuan, Steve and Tar, Chris and others},
  journal={arXiv preprint arXiv:1803.11175},
  year={2018}
}

@InProceedings{Armand:17,
  title={Bag of Tricks for Efficient Text Classification},
  author={Joulin, Armand and Grave, Edouard and Bojanowski, Piotr and Mikolov, Tomas},
  booktitle={Proceedings of the 15th Conference of the European Chapter of the Association for Computational Linguistics: Volume 2, Short Papers},
  month={April},
  year={2017},
  publisher={Association for Computational Linguistics},
  pages={},
}

@article{Nal:14,
  title={A convolutional neural network for modelling sentences},
  author={Kalchbrenner, Nal and Grefenstette, Edward and Blunsom, Phil},
  journal={arXiv preprint arXiv:1404.2188},
  year={2014}
}

@inproceedings{Oriol:15,
  title={Show and tell: A neural image caption generator},
  author={Vinyals, Oriol and Toshev, Alexander and Bengio, Samy and Erhan, Dumitru},
  booktitle={Proceedings of the IEEE conference on computer vision and pattern recognition},
  pages={3156--3164},
  year={2015}
}

@inproceedings{Christian:17,
  title={Inception-v4, inception-resnet and the impact of residual connections on learning},
  author={Szegedy, Christian and Ioffe, Sergey and Vanhoucke, Vincent and Alemi, Alexander A},
  booktitle={Thirty-First AAAI Conference on Artificial Intelligence},
  year={2017}
}

@InProceedings{Alexis:17,
  author    = {Conneau, Alexis  and  Kiela, Douwe  and  Schwenk, Holger  and  Barrault, Lo\"{i}c  and  Bordes, Antoine},
  title     = {Supervised Learning of Universal Sentence Representations from Natural Language Inference Data},
  booktitle = {Proceedings of the 2017 Conference on Empirical Methods in Natural Language Processing},
  month     = {September},
  year      = {2017},
  address   = {Copenhagen, Denmark},
  publisher = {Association for Computational Linguistics},
  pages     = {670--680},
  url       = {https://www.aclweb.org/anthology/D17-1070}
}

@article{Pierre:16,
  title={Opensubtitles2016: Extracting large parallel corpora from movie and tv subtitles},
  author={Lison, Pierre and Tiedemann, J{\"o}rg},
  year={2016},
  publisher={European Language Resources Association}
}

 @InProceedings{Shitala:18,
    author = {Prasad, Shitala and Wai Kin Kong, Adams},
    title = {Using Object Information for Spotting Text},
    booktitle = {The European Conference on Computer Vision (ECCV)},
    month = {September},
    year = {2018}
    }
    
  @inproceedings{Shancheng:18,
  title={Attention and Language Ensemble for Scene Text Recognition with Convolutional Sequence Modeling},
  author={Fang, Shancheng and Xie, Hongtao and Zha, Zheng-Jun and Sun, Nannan and Tan, Jianlong and Zhang, Yongdong},
  booktitle={2018 ACM Multimedia Conference on Multimedia Conference},
  pages={},
  year={2018},
  organization={ACM}
  }
  
  @inproceedings{Ashish:17,
  title={Attention is all you need},
  author={Vaswani, Ashish and Shazeer, Noam and Parmar, Niki and Uszkoreit, Jakob and Jones, Llion and Gomez, Aidan N and Kaiser, {\L}ukasz and Polosukhin, Illia},
  booktitle={Advances in neural information processing systems},
  pages={5998--6008},
  year={2017}
}

@inproceedings{Marcella:20,
  title={Meshed-Memory Transformer for Image Captioning},
  author={Cornia, Marcella and Stefanini, Matteo and Baraldi, Lorenzo and Cucchiara, Rita},
  booktitle={Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition},
  pages={10578--10587},
  year={2020}
}

@inproceedings{Qi:16,
  title={Keyphrase extraction using deep recurrent neural networks on twitter},
  author={Zhang, Qi and Wang, Yang and Gong, Yeyun and Huang, Xuan-Jing},
  booktitle={Proceedings of the 2016 conference on empirical methods in natural language processing},
  pages={},
  year={2016}
}

@article{young2014image,
  title={From image descriptions to visual denotations: New similarity metrics for semantic inference over event descriptions},
  author={Young, Peter and Lai, Alice and Hodosh, Micah and Hockenmaier, Julia},
  journal={TACL},
  volume={},
  pages={},
  year={2014},
  publisher={MIT Press}
}

@inproceedings{karpathy2015deep,
  title={Deep visual-semantic alignments for generating image descriptions},
  author={Karpathy, Andrej and Fei-Fei, Li},
  booktitle={Proceedings of the IEEE conference on computer vision and pattern recognition},
  pages={3128--3137},
  year={2015}
}



@inproceedings{banerjee2005meteor,
  title={METEOR: An automatic metric for MT evaluation with improved correlation with human judgments},
  author={Banerjee, Satanjeev and Lavie, Alon},
  booktitle={Proceedings of the acl workshop on intrinsic and extrinsic evaluation measures for machine translation and/or summarization},
  pages={65--72},
  year={2005}
}


@inproceedings{vedantam2015cider,
  title={Cider: Consensus-based image description evaluation},
  author={Vedantam, Ramakrishna and Lawrence Zitnick, C and Parikh, Devi},
  booktitle={Proceedings of the IEEE conference on computer vision and pattern recognition},
  pages={4566--4575},
  year={2015}
}

@article{reimers2019sentence,
  title={Sentence-bert: Sentence embeddings using siamese bert-networks},
  author={Reimers, Nils and Gurevych, Iryna},
  journal={arXiv preprint arXiv:1908.10084},
  year={2019}
}

@article{hinton1999products,
  title={Products of experts},
  author={Hinton, Geoffrey E},
  year={1999},
  publisher={IET}
}

@article{meng2017deep,
  title={Deep keyphrase generation},
  author={Meng, Rui and Zhao, Sanqiang and Han, Shuguang and He, Daqing and Brusilovsky, Peter and Chi, Yu},
  journal={arXiv preprint arXiv:1704.06879},
  year={2017}
}


@inproceedings{xu2015show,
  title={Show, attend and tell: Neural image caption generation with visual attention},
  author={Xu, Kelvin and Ba, Jimmy and Kiros, Ryan and Cho, Kyunghyun and Courville, Aaron and Salakhudinov, Ruslan and Zemel, Rich and Bengio, Yoshua},
  booktitle={International conference on machine learning},
  pages={2048--2057},
  year={2015}
}


@inproceedings{anderson2018bottom,
  title={Bottom-up and top-down attention for image captioning and visual question answering},
  author={Anderson, Peter and He, Xiaodong and Buehler, Chris and Teney, Damien and Johnson, Mark and Gould, Stephen and Zhang, Lei},
  booktitle={Proceedings of the IEEE conference on computer vision and pattern recognition},
  pages={},
  year={2018}
}

@article{JDH17,
  title={Billion-scale similarity search with GPUs},
  author={Johnson, Jeff and Douze, Matthijs and J{\'e}gou, Herv{\'e}},
  journal={arXiv preprint arXiv:1702.08734},
  year={2017}
}

@inproceedings{lu202012,
  title={12-in-1: Multi-task vision and language representation learning},
  author={Lu, Jiasen and Goswami, Vedanuj and Rohrbach, Marcus and Parikh, Devi and Lee, Stefan},
  booktitle={Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition},
  pages={10437--10446},
  year={2020}
}

@inproceedings{bert-score,
  title={BERTScore: Evaluating Text Generation with BERT},
  author={Tianyi Zhang and Varsha Kishore* and Felix Wu* and Kilian Q. Weinberger and Yoav Artzi},
  booktitle={International Conference on Learning Representations},
  year={2020},
  url={https://openreview.net/forum?id=SkeHuCVFDr}
}

@inproceedings{blok2003probability,
  title={Probability from similarity},
  author={Blok, Sergey and Medin, Douglas and Osherson, Daniel},
  booktitle={AAAI Spring Symposium on Logical Formalization of Commonsense Reasoning},
  pages={},
  year={2003}
}



@article{radford2019language,
  title={Language models are unsupervised multitask learners},
  author={Radford, Alec and Wu, Jeffrey and Child, Rewon and Luan, David and Amodei, Dario and Sutskever, Ilya},
  journal={OpenAI blog},
  volume={1},
  number={8},
  pages={9},
  year={2019}
}

@article{liu2019roberta,
  title={Roberta: A robustly optimized bert pretraining approach},
  author={Liu, Yinhan and Ott, Myle and Goyal, Naman and Du, Jingfei and Joshi, Mandar and Chen, Danqi and Levy, Omer and Lewis, Mike and Zettlemoyer, Luke and Stoyanov, Veselin},
  journal={arXiv preprint arXiv:1907.11692},
  year={2019}
}
@article{wang2018glue,
  title={GLUE: A multi-task benchmark and analysis platform for natural language understanding},
  author={Wang, Alex and Singh, Amanpreet and Michael, Julian and Hill, Felix and Levy, Omer and Bowman, Samuel R},
  journal={arXiv preprint arXiv:1804.07461},
  year={2018}
}

@article{mccarthy2010mtld,
  title={MTLD, vocd-D, and HD-D: A validation study of sophisticated approaches to lexical diversity assessment},
  author={McCarthy, Philip M and Jarvis, Scott},
  journal={Behavior research methods},
  volume={42},
  number={2},
  pages={381--392},
  year={2010},
  publisher={Springer}
}
@article{koehn2017six,
  title={Six challenges for neural machine translation},
  author={Koehn, Philipp and Knowles, Rebecca},
  journal={arXiv preprint arXiv:1706.03872},
  year={2017}
}

@inproceedings{huang2019attention,
  title={Attention on attention for image captioning},
  author={Huang, Lun and Wang, Wenmin and Chen, Jie and Wei, Xiao-Yong},
  booktitle={Proceedings of the IEEE/CVF International Conference on Computer Vision},
  pages={4634--4643},
  year={2019}
}

@inproceedings{luo2018discriminability,
  title={Discriminability objective for training descriptive captions},
  author={Luo, Ruotian and Price, Brian and Cohen, Scott and Shakhnarovich, Gregory},
  booktitle={Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition},
  pages={6964--6974},
  year={2018}
}

@inproceedings{wang2019describing,
  title={Describing like humans: on diversity in image captioning},
  author={Wang, Qingzhong and Chan, Antoni B},
  booktitle={Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition},
  pages={4195--4203},
  year={2019}
}
@article{cer2017semeval,
  title={Semeval-2017 task 1: Semantic textual similarity-multilingual and cross-lingual focused evaluation},
  author={Cer, Daniel and Diab, Mona and Agirre, Eneko and Lopez-Gazpio, Inigo and Specia, Lucia},
  journal={arXiv preprint arXiv:1708.00055},
  year={2017}
}

 @InProceedings{Shitala:18,
    author = {Prasad, Shitala and Wai Kin Kong, Adams},
    title = {Using Object Information for Spotting Text},
    booktitle = {The European Conference on Computer Vision (ECCV)},
    month = {September},
    year = {2018}
    }
    
  @inproceedings{Shancheng:18,
  title={Attention and Language Ensemble for Scene Text Recognition with Convolutional Sequence Modeling},
  author={Fang, Shancheng and Xie, Hongtao and Zha, Zheng-Jun and Sun, Nannan and Tan, Jianlong and Zhang, Yongdong},
  booktitle={2018 ACM Multimedia Conference on Multimedia Conference},
  pages={},
  year={2018},
  organization={ACM}
  }
  
  



@inproceedings{vijayakumar2018diverse,
  title={Diverse beam search for improved description of complex scenes},
  author={Vijayakumar, Ashwin and Cogswell, Michael and Selvaraju, Ramprasaath and Sun, Qing and Lee, Stefan and Crandall, David and Batra, Dhruv},
  booktitle={Proceedings of the AAAI Conference on Artificial Intelligence},
  volume={},
  number={},
  year={2018}
}
@inproceedings{sabir2018visual,
  title={Visual re-ranking with natural language understanding for text spotting},
  author={Sabir, Ahmed and Moreno-Noguer, Francesc and Padr{\'o}, Llu{\'\i}s},
  booktitle={Asian Conference on Computer Vision},
  pages={68--82},
  year={2018},
  organization={Springer}
}

@inproceedings{anderson2016spice,
  title={Spice: Semantic propositional image caption evaluation},
  author={Anderson, Peter and Fernando, Basura and Johnson, Mark and Gould, Stephen},
  booktitle={European conference on computer vision},
  pages={382--398},
  year={2016},
  organization={Springer}
}
@article{lison2016opensubtitles2016,
  title={Opensubtitles2016: Extracting large parallel corpora from movie and tv subtitles},
  author={Lison, Pierre and Tiedemann, J{\"o}rg},
  year={2016},
  publisher={European Language Resources Association}
}

@article{blok2007induction,
  title={Induction as conditional probability judgment},
  author={Blok, Sergey V and Medin, Douglas L and Osherson, Daniel N},
  journal={Memory \& Cognition},
  volume={35},
  number={6},
  pages={1353--1364},
  year={2007},
  publisher={Springer}
}

@article{voorspoels2015people,
  title={How do people learn from negative evidence? Non-monotonic generalizations and sampling assumptions in inductive reasoning},
  author={Voorspoels, Wouter and Navarro, Daniel J and Perfors, Amy and Ransom, Keith and Storms, Gert},
  journal={Cognitive Psychology},
  volume={81},
  pages={1--25},
  year={2015},
  publisher={Elsevier}
}
@inproceedings{heussen2010can,
  title={Can similarity-based models of induction handle negative evidence?},
  author={Heussen, Daniel and Voorspoels, Wouter and Storms, Gert},
  booktitle={Proceedings of the Annual Conference of the Cognitive Science Society},
  volume={32},
  pages={2033--2038},
  year={2010},
  organization={Lawrence Erlbaum Associates}
}


@article{sharma2017nlgeval,
    author  = {Sharma, Shikhar and El Asri, Layla and Schulz, Hannes and Zumer, Jeremie},
    title   = {Relevance of Unsupervised Metrics in Task-Oriented Dialogue for Evaluating Natural Language Generation},
    journal = {CoRR},
    volume  = {abs/1706.09799},
    year    = {2017},
    url     = {http://arxiv.org/abs/1706.09799}
}



@inproceedings{pennington2014glove,
  title={Glove: Global vectors for word representation},
  author={Pennington, Jeffrey and Socher, Richard and Manning, Christopher D},
  booktitle={Proceedings of the 2014 conference on empirical methods in natural language processing (EMNLP)},
  pages={1532--1543},
  year={2014}
}

@article{li2015diversity,
  title={A diversity-promoting objective function for neural conversation models},
  author={Li, Jiwei and Galley, Michel and Brockett, Chris and Gao, Jianfeng and Dolan, Bill},
  journal={arXiv preprint arXiv:1510.03055},
  year={2015}
}

@article{augenstein2017semeval,
  title={Semeval 2017 task 10: Scienceie-extracting keyphrases and relations from scientific publications},
  author={Augenstein, Isabelle and Das, Mrinal and Riedel, Sebastian and Vikraman, Lakshmi and McCallum, Andrew},
  journal={arXiv preprint arXiv:1704.02853},
  year={2017}
}

@article{zhu2020towards,
  title={Towards Understanding Sample Variance in Visually Grounded Language Generation: Evaluations and Observations},
  author={Zhu, Wanrong and Wang, Xin Eric and Narayana, Pradyumna and Sone, Kazoo and Basu, Sugato and Wang, William Yang},
  journal={arXiv preprint arXiv:2010.03644},
  year={2020}
}

@inproceedings{lewis2020pretrained,
  title={Pretrained Language Models for Biomedical and Clinical Tasks: Understanding and Extending the State-of-the-Art},
  author={Lewis, Patrick and Ott, Myle and Du, Jingfei and Stoyanov, Veselin},
  booktitle={Proceedings of the 3rd Clinical Natural Language Processing Workshop},
  pages={146--157},
  year={2020}
}


@article{wang2018object,
  title={Object counts! bringing explicit detections back into image captioning},
  author={Wang, Josiah and Madhyastha, Pranava and Specia, Lucia},
  journal={arXiv preprint arXiv:1805.00314},
  year={2018}
}

@inproceedings{fang2015captions,
  title={From captions to visual concepts and back},
  author={Fang, Hao and Gupta, Saurabh and Iandola, Forrest and Srivastava, Rupesh K and Deng, Li and Doll{\'a}r, Piotr and Gao, Jianfeng and He, Xiaodong and Mitchell, Margaret and Platt, John C and others},
  booktitle={Proceedings of the IEEE conference on computer vision and pattern recognition},
  pages={1473--1482},
  year={2015}
}

@inproceedings{zhang2020context,
  title={Context-aware attention network for image-text retrieval},
  author={Zhang, Qi and Lei, Zhen and Zhang, Zhaoxiang and Li, Stan Z},
  booktitle={Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition},
  pages={3536--3545},
  year={2020}
}
@inproceedings{cornia2019show,
  title={Show, control and tell: A framework for generating controllable and grounded captions},
  author={Cornia, Marcella and Baraldi, Lorenzo and Cucchiara, Rita},
  booktitle={Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition},
  pages={8307--8316},
  year={2019}
}

@book{brown2005encyclopedia,
  title={Encyclopedia of language and linguistics},
  author={Brown, Keith},
  volume={1},
  year={2005},
  publisher={Elsevier}
}
@article{zhang2019bertscore,
  title={Bertscore: Evaluating text generation with bert},
  author={Zhang, Tianyi and Kishore, Varsha and Wu, Felix and Weinberger, Kilian Q and Artzi, Yoav},
  journal={arXiv preprint arXiv:1904.09675},
  year={2019}
}



@inproceedings{dai2017towards,
  title={Towards diverse and natural image descriptions via a conditional gan},
  author={Dai, Bo and Fidler, Sanja and Urtasun, Raquel and Lin, Dahua},
  booktitle={Proceedings of the IEEE International Conference on Computer Vision},
  pages={2970--2979},
  year={2017}
}
@inproceedings{vedantam2017context,
  title={Context-aware captions from context-agnostic supervision},
  author={Vedantam, Ramakrishna and Bengio, Samy and Murphy, Kevin and Parikh, Devi and Chechik, Gal},
  booktitle={Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition},
  pages={251--260},
  year={2017}
}

@article{herdade2019image,
  title={Image captioning: Transforming objects into words},
  author={Herdade, Simao and Kappeler, Armin and Boakye, Kofi and Soares, Joao},
  journal={arXiv preprint arXiv:1906.05963},
  year={2019}
}


@inproceedings{rennie2017self,
  title={Self-critical sequence training for image captioning},
  author={Rennie, Steven J and Marcheret, Etienne and Mroueh, Youssef and Ross, Jerret and Goel, Vaibhava},
  booktitle={Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition},
  pages={7008--7024},
  year={2017}
}

@inproceedings{you2016image,
  title={Image captioning with semantic attention},
  author={You, Quanzeng and Jin, Hailin and Wang, Zhaowen and Fang, Chen and Luo, Jiebo},
  booktitle={Proceedings of the IEEE conference on computer vision and pattern recognition},
  pages={4651--4659},
  year={2016}
}

}
@inproceedings{ren2017deep,
  title={Deep reinforcement learning-based image captioning with embedding reward},
  author={Ren, Zhou and Wang, Xiaoyu and Zhang, Ning and Lv, Xutao and Li, Li-Jia},
  booktitle={Proceedings of the IEEE conference on computer vision and pattern recognition},
  pages={290--298},
  year={2017}
}


}
@article{ippolito2019comparison,
  title={Comparison of diverse decoding methods from conditional language models},
  author={Ippolito, Daphne and Kriz, Reno and Kustikova, Maria and Sedoc, Jo{\~a}o and Callison-Burch, Chris},
  journal={arXiv preprint arXiv:1906.06362},
  year={2019}
}

@article{voorspoels2015people,
  title={How do people learn from negative evidence? Non-monotonic generalizations and sampling assumptions in inductive reasoning},
  author={Voorspoels, Wouter and Navarro, Daniel J and Perfors, Amy and Ransom, Keith and Storms, Gert},
  journal={Cognitive Psychology},
  volume={81},
  pages={1--25},
  year={2015},
  publisher={Elsevier}
}

@article{li2015diversity,
  title={A diversity-promoting objective function for neural conversation models},
  author={Li, Jiwei and Galley, Michel and Brockett, Chris and Gao, Jianfeng and Dolan, Bill},
  journal={arXiv preprint arXiv:1510.03055},
  year={2015}
}
@inproceedings{gupta2020contrastive,
  title={Contrastive learning for weakly supervised phrase grounding},
  author={Gupta, Tanmay and Vahdat, Arash and Chechik, Gal and Yang, Xiaodong and Kautz, Jan and Hoiem, Derek},
  booktitle={Computer Vision--ECCV 2020: 16th European Conference, Glasgow, UK, August 23--28, 2020, Proceedings, Part III 16},
  pages={752--768},
  year={2020},
  organization={Springer}
}
@inproceedings{song2021sentsim,
  title={SentSim: Crosslingual Semantic Evaluation of Machine Translation},
  author={Song, Yurun and Zhao, Junchen and Specia, Lucia},
  booktitle={Proceedings of the 2021 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies},
  pages={3143--3156},
  year={2021}
}
@inproceedings{zhang2021rstnet,
  title={RSTNet: Captioning With Adaptive Attention on Visual and Non-Visual Words},
  author={Zhang, Xuying and Sun, Xiaoshuai and Luo, Yunpeng and Ji, Jiayi and Zhou, Yiyi and Wu, Yongjian and Huang, Feiyue and Ji, Rongrong},
  booktitle={Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition},
  pages={15465--15474},
  year={2021}
}

@inproceedings{wang2020compare,
  title={Compare and reweight: Distinctive image captioning using similar images sets},
  author={Wang, Jiuniu and Xu, Wenjia and Wang, Qingzhong and Chan, Antoni B},
  booktitle={European Conference on Computer Vision},
  pages={370--386},
  year={2020},
  organization={Springer}
}
@article{gao2021simcse,
   title={{SimCSE}: Simple Contrastive Learning of Sentence Embeddings},
   author={Gao, Tianyu and Yao, Xingcheng and Chen, Danqi},
   journal={arXiv preprint arXiv:2104.08821},
   year={2021}
}

@article{conneau2017supervised,
  title={Supervised learning of universal sentence representations from natural language inference data},
  author={Conneau, Alexis and Kiela, Douwe and Schwenk, Holger and Barrault, Loic and Bordes, Antoine},
  journal={arXiv preprint arXiv:1705.02364},
  year={2017}
}

@article{radford2021learning,
  title={Learning transferable visual models from natural language supervision},
  author={Radford, Alec and Kim, Jong Wook and Hallacy, Chris and Ramesh, Aditya and Goh, Gabriel and Agarwal, Sandhini and Sastry, Girish and Askell, Amanda and Mishkin, Pamela and Clark, Jack and others},
  journal={arXiv preprint arXiv:2103.00020},
  year={2021}
}

@inproceedings{huang2017speed,
  title={Speed/accuracy trade-offs for modern convolutional object detectors},
  author={Huang, Jonathan and Rathod, Vivek and Sun, Chen and Zhu, Menglong and Korattikara, Anoop and Fathi, Alireza and Fischer, Ian and Wojna, Zbigniew and Song, Yang and Guadarrama, Sergio and others},
  booktitle={Proceedings of the IEEE conference on computer vision and pattern recognition},
  pages={},
  year={2017}
}
@inproceedings{papineni2002bleu,
  title={Bleu: a method for automatic evaluation of machine translation},
  author={Papineni, Kishore and Roukos, Salim and Ward, Todd and Zhu, Wei-Jing},
  booktitle={Proceedings of the 40th annual meeting of the Association for Computational Linguistics},
  pages={311--318},
  year={2002}
}



@inproceedings{lin2004rouge,
  title={Rouge: A package for automatic evaluation of summaries},
  author={Lin, Chin-Yew},
  booktitle={Text summarization branches out},
  pages={74--81},
  year={2004}
}

@inproceedings{clark2011better,
  title={Better hypothesis testing for statistical machine translation: Controlling for optimizer instability},
  author={Clark, Jonathan H and Dyer, Chris and Lavie, Alon and Smith, Noah A},
  booktitle={Proceedings of the 49th Annual Meeting of the Association for Computational Linguistics: Human Language Technologies},
  pages={176--181},
  year={2011}
}

@inproceedings{deshpande2019fast,
  title={Fast, diverse and accurate image captioning guided by part-of-speech},
  author={Deshpande, Aditya and Aneja, Jyoti and Wang, Liwei and Schwing, Alexander G and Forsyth, David},
  booktitle={Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition},
  pages={10695--10704},
  year={2019}
}

@inproceedings{chen2020say,
  title={Say as you wish: Fine-grained control of image caption generation with abstract scene graphs},
  author={Chen, Shizhe and Jin, Qin and Wang, Peng and Wu, Qi},
  booktitle={Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition},
  pages={9962--9971},
  year={2020}
}

@inproceedings{zhang2021consensus,
  title={Consensus graph representation learning for better grounded image captioning},
  author={Zhang, Wenqiao and Shi, Haochen and Tang, Siliang and Xiao, Jun and Yu, Qiang and Zhuang, Yueting},
  booktitle={Proc 35 AAAI Conf on Artificial Intelligence},
  year={2021}
}

@inproceedings{elliott2014comparing,
  title={Comparing automatic evaluation measures for image description},
  author={Elliott, Desmond and Keller, Frank},
  booktitle={Proceedings of the 52nd Annual Meeting of the Association for Computational Linguistics (Volume 2: Short Papers)},
  pages={452--457},
  year={2014}
}

  @inproceedings{changpinyo2021conceptual,
  title={Conceptual 12M: Pushing Web-Scale Image-Text Pre-Training To Recognize Long-Tail Visual Concepts},
  author={Changpinyo, Soravit and Sharma, Piyush and Ding, Nan and Soricut, Radu},
  booktitle={Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition},
  pages={3558--3568},
  year={2021}
}

@inproceedings{abadi2016tensorflow,
  title={Tensorflow: A system for large-scale machine learning},
  author={Abadi, Mart{\'\i}n and Barham, Paul and Chen, Jianmin and Chen, Zhifeng and Davis, Andy and Dean, Jeffrey and Devin, Matthieu and Ghemawat, Sanjay and Irving, Geoffrey and Isard, Michael and others},
  booktitle={12th $\{$USENIX$\}$ symposium on operating systems design and implementation ($\{$OSDI$\}$ 16)},
  pages={265--283},
  year={2016}
}

@article{paszke2019pytorch,
  title={Pytorch: An imperative style, high-performance deep learning library},
  author={Paszke, Adam and Gross, Sam and Massa, Francisco and Lerer, Adam and Bradbury, James and Chanan, Gregory and Killeen, Trevor and Lin, Zeming and Gimelshein, Natalia and Antiga, Luca and others},
  journal={arXiv preprint arXiv:1912.01703},
  year={2019}
}

@inproceedings{ott2019fairseq,
  title = {fairseq: A Fast, Extensible Toolkit for Sequence Modeling},
  author = {Myle Ott and Sergey Edunov and Alexei Baevski and Angela Fan and Sam Gross and Nathan Ng and David Grangier and Michael Auli},
  booktitle = {Proceedings of NAACL-HLT 2019: Demonstrations},
  year = {2019},
}

@inproceedings{post2018call,
  title={A Call for Clarity in Reporting BLEU Scores},
  author={Post, Matt},
  booktitle={Proceedings of the Third Conference on Machine Translation: Research Papers},
  pages={186--191},
  year={2018}
}

@inproceedings{popovic2015chrf,
  title={chrF: character n-gram F-score for automatic MT evaluation},
  author={Popovi{\'c}, Maja},
  booktitle={Proceedings of the Tenth Workshop on Statistical Machine Translation},
  pages={392--395},
  year={2015}
}

@inproceedings{shetty2017speaking,
  title={Speaking the same language: Matching machine to human captions by adversarial training},
  author={Shetty, Rakshith and Rohrbach, Marcus and Anne Hendricks, Lisa and Fritz, Mario and Schiele, Bernt},
  booktitle={Proceedings of the IEEE International Conference on Computer Vision},
  pages={4135--4144},
  year={2017}
}

@inproceedings{peinelt2020tbert,
  title={tBERT: Topic models and BERT joining forces for semantic similarity detection},
  author={Peinelt, Nicole and Nguyen, Dong and Liakata, Maria},
  booktitle={Proceedings of the 58th Annual Meeting of the Association for Computational Linguistics},
  pages={7047--7055},
  year={2020}
}
@inproceedings{rashtchian2010collecting,
  title={Collecting image annotations using amazon’s mechanical turk},
  author={Rashtchian, Cyrus and Young, Peter and Hodosh, Micah and Hockenmaier, Julia},
  booktitle={Proceedings of the NAACL HLT 2010 workshop on creating speech and language data with Amazon’s Mechanical Turk},
  pages={139--147},
  year={2010}
}


@article{jaderberg2016reading,
  title={Reading text in the wild with convolutional neural networks},
  author={Jaderberg, Max and Simonyan, Karen and Vedaldi, Andrea and Zisserman, Andrew},
  journal={International Journal of Computer Vision},
  volume={},
  number={},
  pages={},
  year={2016},
  publisher={Springer}
}

@article{camacho2019relational,
  title={Relational Word Embeddings},
  author={Camacho-Collados, Jose and Espinosa-Anke, Luis and Schockaert, Steven},
  journal={arXiv preprint arXiv:1906.01373},
  year={2019}
}

@article{joulin2017bag,
  title={Bag of Tricks for Efficient Text Classification},
  author={Joulin, Armand and Grave, Edouard and Mikolov, Piotr Bojanowski Tomas},
  journal={EACL 2017},
  pages={427},
  year={2017}
}

@inproceedings{mikolov2013distributed,
  title={Distributed representations of words and phrases and their compositionality},
  author={Mikolov, Tomas and Sutskever, Ilya and Chen, Kai and Corrado, Greg S and Dean, Jeff},
  booktitle={Advances in neural information processing systems},
  pages={},
  year={2013}
}

@article{turney2010frequency,
  title={From frequency to meaning: Vector space models of semantics},
  author={Turney, Peter D and Pantel, Patrick},
  journal={Journal of artificial intelligence research},
  volume={37},
  pages={141--188},
  year={2010}
}

@article{navigli2012babelnet,
  title={BabelNet: The automatic construction, evaluation and application of a wide-coverage multilingual semantic network},
  author={Navigli, Roberto and Ponzetto, Simone Paolo},
  journal={Artificial intelligence},
  volume={193},
  pages={217--250},
  year={2012},
  publisher={Elsevier}
}

@inproceedings{turney2001mining,
  title={Mining the web for synonyms: PMI-IR versus LSA on TOEFL},
  author={Turney, Peter D},
  booktitle={European conference on machine learning},
  pages={491--502},
  year={2001},
  organization={Springer}
}

@inproceedings{turney2001mining,
  title={Mining the web for synonyms: PMI-IR versus LSA on TOEFL},
  author={Turney, Peter D},
  booktitle={European conference on machine learning},
  pages={491--502},
  year={2001},
  organization={Springer}
}

@article{landauer1998introduction,
  title={An introduction to latent semantic analysis},
  author={Landauer, Thomas K and Foltz, Peter W and Laham, Darrell},
  journal={Discourse processes},
  volume={25},
  number={2-3},
  pages={259--284},
  year={1998},
  publisher={Taylor \& Francis}
}

@article{cha2007comprehensive,
  title={Comprehensive survey on distance/similarity measures between probability density functions},
  author={Cha, Sung-Hyuk},
  journal={City},
  volume={1},
  number={2},
  pages={1},
  year={2007}
}

@article{xu2005survey,
  title={Survey of clustering algorithms},
  author={Xu, Rui and Wunsch, Donald},
  journal={IEEE Transactions on neural networks},
  volume={16},
  number={3},
  pages={645--678},
  year={2005},
  publisher={Ieee}
}

@article{sanchez2012ontology,
  title={Ontology-based semantic similarity: A new feature-based approach},
  author={S{\'a}nchez, David and Batet, Montserrat and Isern, David and Valls, Aida},
  journal={Expert systems with applications},
  volume={39},
  number={9},
  pages={7718--7728},
  year={2012},
  publisher={Elsevier}
}

@book{miller1998wordnet,
  title={WordNet: An electronic lexical database},
  author={Miller, George A},
  year={1998},
  publisher={MIT press}
}

@conference{veit2016cocotext,
title = {COCO-Text: Dataset and Benchmark for Text Detection and Recognition in Natural Images},
author = {Andreas Veit and Tomas Matera and Lukas Neumann and Jiri Matas and Serge Belongie},
url = {http://vision.cornell.edu/se3/wp-content/uploads/2016/01/1601.07140v1.pdf},
year = {2016},
date = {2016-01-26},
booktitle = {arXiv preprint arXiv:1601.07140},
keywords = {}
}

@inproceedings{ghosh2017visual,
  title={Visual attention models for scene text recognition},
  author={Ghosh, Suman K and Valveny, Ernest and Bagdanov, Andrew D},
  booktitle={2017 14th IAPR international conference on document analysis and recognition (ICDAR)},
  volume={1},
  pages={943--948},
  year={2017},
  organization={IEEE}
}

@article{zhou2017places,
  title={Places: A 10 million image database for scene recognition},
  author={Zhou, Bolei and Lapedriza, Agata and Khosla, Aditya and Oliva, Aude and Torralba, Antonio},
  journal={IEEE transactions on pattern analysis and machine intelligence},
  volume={40},
  number={6},
  pages={1452--1464},
  year={2017},
  publisher={IEEE}
}

@inproceedings{,
  title={The berkeley framenet project},
  author={Baker, Collin F and Fillmore, Charles J and Lowe, John B},
  booktitle={COLING 1998 Volume 1: The 17th International Conference on Computational Linguistics},
  year={1998}
}

@inproceedings{he2016deep,
  title={Deep residual learning for image recognition},
  author={He, Kaiming and Zhang, Xiangyu and Ren, Shaoqing and Sun, Jian},
  booktitle={Proceedings of the IEEE conference on computer vision and pattern recognition},
  pages={770--778},
  year={2016}
}

@inproceedings{szegedy2017inception,
  title={Inception-v4, inception-resnet and the impact of residual connections on learning},
  author={Szegedy, Christian and Ioffe, Sergey and Vanhoucke, Vincent and Alemi, Alexander A},
  booktitle={Thirty-first AAAI conference on artificial intelligence},
  year={2017}
}

@inproceedings{iacobacci2019lstmembed,
  title={Lstmembed: Learning word and sense representations from a large semantically annotated corpus with long short-term memories},
  author={Iacobacci, Ignacio and Navigli, Roberto},
  booktitle={Proceedings of the 57th Annual Meeting of the Association for Computational Linguistics},
  pages={1685--1695},
  year={2019}
}

@inproceedings{iacobacci2015sensembed,
  title={Sensembed: Learning sense embeddings for word and relational similarity},
  author={Iacobacci, Ignacio and Pilehvar, Mohammad Taher and Navigli, Roberto},
  booktitle={Proceedings of the 53rd Annual Meeting of the Association for Computational Linguistics and the 7th International Joint Conference on Natural Language Processing (Volume 1: Long Papers)},
  pages={95--105},
  year={2015}
}

@article{mancini2016embedding,
  title={Embedding words and senses together via joint knowledge-enhanced training},
  author={Mancini, Massimiliano and Camacho-Collados, Jose and Iacobacci, Ignacio and Navigli, Roberto},
  journal={arXiv preprint arXiv:1612.02703},
  year={2016}
}

@inproceedings{camacho2015nasari,
  title={Nasari: a novel approach to a semantically-aware representation of items},
  author={Camacho-Collados, Jos{\'e} and Pilehvar, Mohammad Taher and Navigli, Roberto},
  booktitle={Proceedings of the 2015 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies},
  pages={567--577},
  year={2015}
}

@inproceedings{raganato2016automatic,
  title={Automatic Construction and Evaluation of a Large Semantically Enriched Wikipedia.},
  author={Raganato, Alessandro and Bovi, Claudio Delli and Navigli, Roberto},
  booktitle={IJCAI},
  pages={2894--2900},
  year={2016}
}

@article{sabir2020enhancing,
  title={Enhancing scene text recognition with visual context information},
  author={Sabir, Ahmed},
  year={2020},
  publisher={Universitat Polit{\`e}cnica de Catalunya}
}

@inproceedings{lin2014microsoft,
  title={Microsoft coco: Common objects in context},
  author={Lin, Tsung-Yi and Maire, Michael and Belongie, Serge and Hays, James and Perona, Pietro and Ramanan, Deva and Doll{\'a}r, Piotr and Zitnick, C Lawrence},
  booktitle={European conference on computer vision},
  pages={740--755},
  year={2014},
  organization={Springer}
}

@inproceedings{lee2018stacked,
  title={Stacked cross attention for image-text matching},
  author={Lee, Kuang-Huei and Chen, Xi and Hua, Gang and Hu, Houdong and He, Xiaodong},
  booktitle={Proceedings of the European Conference on Computer Vision (ECCV)},
  pages={201--216},
  year={2018}
}

@inproceedings{mihalcea2006corpus,
  title={Corpus-based and knowledge-based measures of text semantic similarity},
  author={Mihalcea, Rada and Corley, Courtney and Strapparava, Carlo and others},
  booktitle={Aaai},
  volume={6},
  number={2006},
  pages={775--780},
  year={2006}
}

@article{zhu2016computing,
  title={Computing semantic similarity of concepts in knowledge graphs},
  author={Zhu, Ganggao and Iglesias, Carlos A},
  journal={IEEE Transactions on Knowledge and Data Engineering},
  volume={29},
  number={1},
  pages={72--85},
  year={2016},
  publisher={IEEE}
}

@inproceedings{budanitsky2001semantic,
  title={Semantic distance in WordNet: An experimental, application-oriented evaluation of five measures},
  author={Budanitsky, Alexander and Hirst, Graeme},
  booktitle={Workshop on WordNet and other lexical resources},
  volume={2},
  pages={2--2},
  year={2001}
}

@inproceedings{li2020oscar,
  title={Oscar: Object-semantics aligned pre-training for vision-language tasks},
  author={Li, Xiujun and Yin, Xi and Li, Chunyuan and Zhang, Pengchuan and Hu, Xiaowei and Zhang, Lei and Wang, Lijuan and Hu, Houdong and Dong, Li and Wei, Furu and others},
  booktitle={ECCV},
  pages={},
  year={2020},
  organization={Springer}
}

@inproceedings{fang2015captions,
  title={From captions to visual concepts and back},
  author={Fang, Hao and Gupta, Saurabh and Iandola, Forrest and Srivastava, Rupesh K and Deng, Li and Doll{\'a}r, Piotr and Gao, Jianfeng and He, Xiaodong and Mitchell, Margaret and Platt, John C and others},
  booktitle={Proceedings of the IEEE conference on computer vision and pattern recognition},
  pages={1473--1482},
  year={2015}
}

@article{rohrbach2018object,
  title={Object hallucination in image captioning},
  author={Rohrbach, Anna and Hendricks, Lisa Anne and Burns, Kaylee and Darrell, Trevor and Saenko, Kate},
  journal={arXiv preprint arXiv:1809.02156},
  year={2018}
}

@inproceedings{kim-2014-convolutional,
    title = "Convolutional Neural Networks for Sentence Classification",
    author = "Kim, Yoon",
    booktitle = "EMNLP",
    year = "2014",
    address = "",
    publisher = "",
    url = "",
    doi = "",
    pages = "",
}

@inproceedings{Jacob:19,
  title={BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding},
  author={Devlin, Jacob and Chang, Ming-Wei and Lee, Kenton and Toutanova, Kristina},
  booktitle={NAACL-HLT},
  year={2019}
}

@inproceedings{shetty2017speaking,
  title={Speaking the same language: Matching machine to human captions by adversarial training},
  author={Shetty, Rakshith and Rohrbach, Marcus and Anne Hendricks, Lisa and Fritz, Mario and Schiele, Bernt},
  booktitle={Proceedings of the IEEE International Conference on Computer Vision},
  pages={4135--4144},
  year={2017}
}
@inproceedings{deshpande2019fast,
  title={Fast, diverse and accurate image captioning guided by part-of-speech},
  author={Deshpande, Aditya and Aneja, Jyoti and Wang, Liwei and Schwing, Alexander G and Forsyth, David},
  booktitle={Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition},
  pages={10695--10704},
  year={2019}

}

@inproceedings{nair2010rectified,
  title={Rectified linear units improve restricted boltzmann machines},
  author={Nair, Vinod and Hinton, Geoffrey E},
  booktitle={Icml},
  year={2010}
}

@inproceedings{hendricks2018women,
  title={Women also snowboard: Overcoming bias in captioning models},
  author={Hendricks, Lisa Anne and Burns, Kaylee and Saenko, Kate and Darrell, Trevor and Rohrbach, Anna},
  booktitle={Proceedings of the European Conference on Computer Vision (ECCV)},
  pages={771--787},
  year={2018}
}

@inproceedings{hendricks2018women,
  title={Women also snowboard: Overcoming bias in captioning models},
  author={Hendricks, Lisa Anne and Burns, Kaylee and Saenko, Kate and Darrell, Trevor and Rohrbach, Anna},
  booktitle={Proceedings of the European Conference on Computer Vision (ECCV)},
  pages={771--787},
  year={2018}
}
@article{zhao2017men,
  title={Men also like shopping: Reducing gender bias amplification using corpus-level constraints},
  author={Zhao, Jieyu and Wang, Tianlu and Yatskar, Mark and Ordonez, Vicente and Chang, Kai-Wei},
  journal={arXiv preprint arXiv:1707.09457},
  year={2017}
}

</script>
