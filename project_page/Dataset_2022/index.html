
<!DOCTYPE html PUBLIC "-//W3C//DTD XHTML 1.0 Transitional//EN" "http://www.w3.org/TR/xhtml1/DTD/xhtml1-transitional.dtd">
<html xmlns="http://www.w3.org/1999/xhtml">
<head>
<meta http-equiv="Content-Type" content="text/html; charset=utf-8" />
<title>Visual Semantic Relatedness Dataset for Image Captioning</title>

  <link rel="stylesheet" href="https://maxcdn.bootstrapcdn.com/bootstrap/3.2.0/css/bootstrap.min.css">
  <link href='http://fonts.googleapis.com/css?family=Lato:400,700' rel='stylesheet' type='text/css'>
  <link href="style.css" rel="stylesheet" type="text/css" />

<script type="text/javascript" src="../js/hidebib.js"></script>



</head>

<body> 

<div class="container">
  <table border="0" align="center">
    <tr><td width="623" align="center" valign="middle"><h2>Visual Semantic Relatedness Dataset for Image Captioning </h2></td></tr>
    <tr><td width="300" align="center" valign="middle">   <a href="https://www.cs.upc.edu/~asabir/">Ahmed Sabir</a>, <a href="https://www.iri.upc.edu/people/fmoreno/">Francesc Moreno-Noguer</a>, 
         <a href="https://www.cs.upc.edu/~padro/">Lluís Padró</a></td></tr>
  </table>
  </br>
  



<p align="center"> <img src="overview.png" width="500" align="middle" /></p> 
  
 <!-- <p align="center"> <img src="overview_v2.png" width="500" align="middle" /></p> -->
  
  



  


</table>
<table width="400px" align="center">
  <tr>
    <td align=center width=100px>
      <center>
        <ul class="nav">
          <li class="nav-item text-center">
              <a href="https://arxiv.org/abs/2301.08784" class="nav-link" title="Temp link">
                  <svg style="width:48px;height:48px" viewBox="0 0 24 24">
                      <path fill="currentColor" d="M16 0H8C6.9 0 6 .9 6 2V18C6 19.1 6.9 20 8 20H20C21.1 20 22 19.1 22 18V6L16 0M20 18H8V2H15V7H20V18M4 4V22H20V24H4C2.9 24 2 23.1 2 22V4H4M10 10V12H18V10H10M10 14V16H15V14H10Z" />
                  </svg><br>
                  Paper
              </a>
      </center>
    </td>
    <td align=center width=200px>
      <center>
        
        <ul class="nav">
        <li class="nav-item text-center">
          <a href="https://github.com/ahmedssabir/Textual-Visual-Semantic-Dataset" class="nav-link">
              <svg style="width:48px;height:48px" viewBox="0 0 65 65">
                  <path fill="currentColor" d="M32 0a32.021 32.021 0 0 0-10.1 62.4c1.6.3 2.2-.7 2.2-1.5v-6c-8.9 1.9-10.8-3.8-10.8-3.8-1.5-3.7-3.6-4.7-3.6-4.7-2.9-2 .2-1.9.2-1.9 3.2.2 4.9 3.3 4.9 3.3 2.9 4.9 7.5 3.5 9.3 2.7a6.93 6.93 0 0 1 2-4.3c-7.1-.8-14.6-3.6-14.6-15.8a12.27 12.27 0 0 1 3.3-8.6 11.965 11.965 0 0 1 .3-8.5s2.7-.9 8.8 3.3a30.873 30.873 0 0 1 8-1.1 30.292 30.292 0 0 1 8 1.1c6.1-4.1 8.8-3.3 8.8-3.3a11.965 11.965 0 0 1 .3 8.5 12.1 12.1 0 0 1 3.3 8.6c0 12.3-7.5 15-14.6 15.8a7.746 7.746 0 0 1 2.2 5.9v8.8c0 .9.6 1.8 2.2 1.5A32.021 32.021 0 0 0 32 0z" />
              </svg> <br>
             
              Code
          </a>
      </center>
      <td align=center width=100px>
        <center>
          <ul class="nav">
          <li class="nav-item text-center">
            <a href="https://huggingface.co/datasets/AhmedSSabir/Textual-Image-Caption-Dataset" class="nav-link">
              <svg style="width:48px;height:48px" viewBox="0 0 24 24">
                <path fill="currentColor" d="M21,16.5C21,16.88 20.79,17.21 20.47,17.38L12.57,21.82C12.41,21.94 12.21,22 12,22C11.79,22 11.59,21.94 11.43,21.82L3.53,17.38C3.21,17.21 3,16.88 3,16.5V7.5C3,7.12 3.21,6.79 3.53,6.62L11.43,2.18C11.59,2.06 11.79,2 12,2C12.21,2 12.41,2.06 12.57,2.18L20.47,6.62C20.79,6.79 21,7.12 21,7.5V16.5M12,4.15L6.04,7.5L12,10.85L17.96,7.5L12,4.15M5,15.91L11,19.29V12.58L5,9.21V15.91M19,15.91V9.21L13,12.58V19.29L19,15.91Z" />
              </svg><br>
              data
            </a>
                  </ul>
        </center>
    </td>
</table>

  
</div>

</br>

<div class="container">
  <h2>Abstract</h2>

  

  




    <p align="justify"> Modern image captioning system relies heavily on extracting knowledge from images 
  to capture the concept of a static story. In this paper, we propose a textual 
  visual context dataset for captioning, in which the publicly available 
  dataset COCO Captions  (<a href="https://arxiv.org/pdf/1405.0312.pdf">Lin et al., 2014</a>) has been extended with information 
  about the scene (such as objects in the image). Since this information has a textual
   form, it can be used to leverage any NLP task, such as text similarity or semantic relation methods, 
   into captioning systems, either as an end-to-end training strategy or a post-processing based approach.

  
      <!-- Modern image captaining relies heavily on extracting knowledge, from images such as objects,
      to capture the concept of a static story in an image. 
      In this paper, we propose a textual visual context dataset for captioning, where the publicly available 
      dataset COCO Captions (<a href="https://arxiv.org/pdf/1405.0312.pdf">Lin et al., 2014</a>) 
      has been extended with information about the scene (such as objects in the image). Since this information has textual form, 
      it can be used to leverage any NLP task, such as text similarity or semantic relation methods, into captioning systems, 
      either as an end-to-end training strategy or a post-processing based approach.
      --> 
      
  </a></tr>
</div>

</br>


<div class="container" text-align="left">
  <h2>Overview</h2>
    <table border="0" align="left">
      <tr><td width="1000" align="justify"> 
        <p align="justify"> 
          We enrich COCO Captions with textual Visual Context information. We use ResNet152, CLIP, 
          and Faster R-CNN to extract object information for each  image. We use three filter approaches 
          to ensure the quality of the dataset (1) Threshold: to filter out predictions where the object classifier 
          is not confident enough,  and (2) semantic alignment with semantic similarity to remove duplicated objects. 
          (3) semantic relatedness score as soft-label: to guarantee the visual context and caption have a strong 
          relation. In particular, we use Sentence-RoBERTa via cosine similarity to give a soft score, and then 
          we use a threshold to annotate the final label (if th &ge; 0.2, 0.3, 0.4 then 1,0). 
          
          <!-- Finally, to take advantage 
          of the visual overlap  between caption and visual context,  
          and to extract global information, we use BERT followed by a shallow CNN (<a href="https://arxiv.org/abs/1408.5882">Kim, 2014</a>)
          to estimate the visual relatedness using the caption description. -->
       </br>
        </br>

        
        
  
        <!--
    Here is a  <a href="https://colab.research.google.com/drive/1N0JVa6y8FKGLLSpiG7hd_W75UYhHRe2j?usp=sharing">Colab</a> demo </td></tr>
    -->
        For quick start please have a look at the <a href="https://github.com/ahmedssabir/Textual-Visual-Semantic-Dataset/blob/main/BERT_CNN_Visual_re_ranker_demo.ipynb">demo</a></tr>

       
        
   
    
  
      <tr><td width="1000" align="left"> 
        <div class="paper" id="ap_agent">
          <h3>Proposed Approach</h3>
          <p>  We also propose a strategy to estimate the most closely related/not-related visual concepts using the caption description via BERT-CNN.
            <!--
            We also propose a strategy to estimate the most closely related/not-related visual
concepts using the caption description. In partuoular, We employ BERT followed by a shallow CNN
          to estimate the visual relatedness using the caption description.
-->
            
            
      <!--      We also proposed a data filtering strategy and visual semantic model to
   
            estimate the degree of the relatedness between the caption and its related visual context in the image. 

--> 
            
<!--
     In this work, we also propose a visual context-based COCO dataset and visual context 
            baseline that can measure the degree of the relatedness between the caption and its related visual context in the image. 
 -->
            
            
            
            
         
</p>
         
          
<p align="center"> 
<img src="BERT-CNN.png"  width="300" align="center" />
<img src="BLEU_result.png"  width="320" align="center" />
</p>
<!---      
<p align="center"> <img src="BERT-CNN.png" width="300" align="middle" /></p>
-->

    

<h3>Resulting Dataset</h3>
          <p>
                 We rely on COCO-Captions dataset to extract the visual context, and we propose two visual context datasets:
                <br>
                <br>
          <!---
          <a>Training data <sub>COCO</sub></a>
          --> 
          <a shape="rect" href="https://huggingface.co/datasets/AhmedSSabir/Textual-Image-Caption-Dataset" class="togglebib">Visual<sub>COCO</sub></a>
          <pre xml:space="preserve">
            visual context, caption descriptions
            umbrella dress human face, a woman with an umbrella near the sea.
            bathtub tub, this is a bathroom with a jacuzzi shower sink and toilet.
            snowplow shovel, the fire hydrant is partially buried under the snow.
            desktop computer monitor, a computer with a flower as its background sits on a desk.
            pitcher ballplayer, a baseball player preparing to throw the ball.
            groom restaurant, a black and white picture of a centerpiece to a table at a wedding.
          </pre>       
        </div>
      </td></tr>


      <tr><td width="3000" align="left"> 
        <div class="paper" id="ap_role_scenario1">
          <a shape="rect" href="https://huggingface.co/datasets/AhmedSSabir/Textual-Image-Caption-Dataset" class="togglebib">Overlaping<sub>COCO</sub> </a>
          <pre >
            visual context, caption descriptions, overlapping information
            pole streetsign flagpole, a house that has a pole with a sign on it,{'pole'}.
            stove microwave refrigerator, an older stove sits in the kitchen next to a bottle of cleaner,{'stove'}.
            racket tennis ball ballplayer, a tennis player swinging a racket at a ball,{'tennis', 'racket', 'ball'}.
            grocery store dining table restaurant, a table is full of different kinds of food and drinks,{'table'}.
          </pre>       
        </div>
        
             <p align="center"> <img src="CVPR_figures_hist.png" width="620" align="middle" /></p>
        
         <tr><td width="1000" align="left"> 
       <div class="paper" id="ap_agent"> 
 <p>  Another task that can benefit from the proposed dataset is investigating the contribution of the 
            visual context to gender bias. We also propose gender neutral dataset.    </p>

           </div>

      </td></tr>
      <tr><td width="1000" align="left"> 
        <div class="paper" id="ap_role_scenario1">
          <a shape="rect" href="https://huggingface.co/datasets/AhmedSSabir/Textual-Image-Caption-Dataset" class="togglebib">Gender Neutral<sub>COCO</sub> </a>
          <pre >
            visual context, caption descriptions
            pizza, a person cutting a pizza with a fork and knife.
            suit, a person in a suit and tie sitting with his hands between his legs.
            paddle, a person riding a colorful surfboard in the water.
            ballplayer, a young person in a batting stance in a baseball game.
           
          </pre>       
        </div>

      </td></tr>

          <center>
            <table>
              <tr>
          
          
                    <td>Visual</td>
                  <td style="text-align:center"> + person</td>
                    <td style="text-align:center"> + man</td>
                    <td style="text-align:center"> + woman</td>
                    <td style="text-align:center"> m</td>
                    <td style="text-align:center"> w</td>
                    <td style="text-align:center"> to-m </td>

          
          
          
              </tr>
              <tr>
          
                      <td> Clothing</td>
                     
                     <td style="text-align:center"><font color="#ccdfc0">3950</font></td>
                     
                  <td style="text-align:center"> 3360</td>
                    <td style="text-align:center"> 1490 </td>
                    <td style="text-align:center"> .85</td>
                    <td style="text-align:center"> .37 </td>
                    <td style="text-align:center">  <font color="#f5acad">.69 </font></td>
          
          
          
          
              </tr>
              <tr>
                  <td>footwear</td>
                    <td style="text-align:center"><font color="#ccdfc0">2810</font></td>
                    <td style="text-align:center"> 1720</td>
                    <td style="text-align:center"> 220</td>
                    <td style="text-align:center"> .61</td>
                    <td style="text-align:center"> .07</td>
                    <td style="text-align:center">  <font color="#f5acad">.88 </font></td>
              
                  </tr>
              <tr>
          
                  <td>racket</td>
                  <td style="text-align:center"><font color="#ccdfc0">1360</font></td>
                  <td style="text-align:center">  440</td>
                  <td style="text-align:center"> 150</td>
                  <td style="text-align:center"> .32</td>
                  <td style="text-align:center"> .11</td>
                  <td style="text-align:center"> <font color="#f5acad">.74</font></td>
          
          
          
              </tr>
              <tr>
          
          
          
          
          
                  <td>surfboard</td>
                   <td style="text-align:center"><font color="#ccdfc0">820</font></td>
                   <td style="text-align:center">80</td>
                    <td style="text-align:center">10</td>
                    <td style="text-align:center">.09</td>
                    <td style="text-align:center">.01</td>
                    <td style="text-align:center"><font color="#f5acad">.88</font></td>
              </tr>
              <tr>
                  <td>tennis</td>
          
                     <td style="text-align:center">140</td>
                  <td style="text-align:center">200</td>
                    <td style="text-align:center">60</td>
                    <td style="text-align:center">1.4</td>
                    <td style="text-align:center">.42</td>
                    <td style="text-align:center"><font color="#f5acad">.76</font></td>
              </tr>
          
              </tr>
          
              <tr>
          
                  <td>motorcycle</td>
          
                        <td style="text-align:center">480</td>
                  <td style="text-align:center"> 40</td>
                    <td style="text-align:center"> 20 </td>
                   <td style="text-align:center"> .8</td>
                    <td style="text-align:center">.04</td>
                    <td style="text-align:center"><font color="#f5acad">.66</font></td>
          
              </tr>


          
              </tr><caption> <font color="#6e6e6f"> <small> <small>  Frequency count of object + gender. The gender bias toward man. <small>  <small></font> </caption>
              
            </table>
          
          </center>
        
        
<!--
      <tr><td width="1000" align="left"> 
        <div class="paper" id="ap_role_scenario1">
          <a shape="rect" href="javascript:togglebib('ap_role_scenario1')" class="togglebib">unsupervised<sub>CC</sub> </a>
          <pre xml:space="preserve">
            some tea in a wooden bowl with a scoop and blue flowers next to it
            two women standing side by side at an event holding their certificates
            an elderly man rides a bicycle in the street while people walk around
            the soccer player has his arms up as he is celebrating
            a stream runs through a forested green and leafy area
            two teams of men playing a game in a basketball court
            three ladies looking at something while sitting next to each other
            there is a shark swimming in the blue water
            
          </pre>       
        </div>
      </td></tr>
-->
      
    </table> 
</div>

</table> 
</div>

</div>

</br>

<div style="text-align: left;" class="containersmall">
  <h3>Citation</h3>
<pre class='citation'>
@article{sabir2023visual,
  title={Visual Semantic Relatedness Dataset for Image Captioning},
  author={Sabir, Ahmed and Moreno-Noguer, Francesc and Padr{\'o}, Llu{\'\i}s},
  journal={arXiv preprint arXiv:2301.08784},
  year={2023}
}
</div>
<div style="text-align: center;" class="containersmall">
  <p>Contact: <a href="mailto:asabir@cs.upc.edu">Ahmed Sabir</a></p>
</div>
<script xml:space="preserve" language="JavaScript">
hideallbibs();
<script xml:space="preserve" language="JavaScript">
hideallbibs();
</script>

</body>
</html>
