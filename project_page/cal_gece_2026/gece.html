<!DOCTYPE html>
<html lang="en">
<head>
  <meta charset="utf-8" />
  <title>The Confidence Trap: Gender Bias and Predictive Certainty in LLMs</title>
  <meta name="viewport" content="width=device-width, initial-scale=1" />
  <link rel="stylesheet" href="https://maxcdn.bootstrapcdn.com/bootstrap/3.2.0/css/bootstrap.min.css" />
  <link href="https://fonts.googleapis.com/css?family=Lato:400,700" rel="stylesheet" type="text/css" />
  <link href="style.css" rel="stylesheet" type="text/css" />
  <script type="text/javascript" src="../js/hidebib.js"></script>
  <script id="MathJax-script" async src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js"></script>
  <style>
    body {
      font-size: 19px;
      font-family: 'Lato', sans-serif;
      background: #f8fafc;
      color: #1f2a37;
      line-height: 1.65;
    }

    .container {
      margin-bottom: 35px;
    }

    .hero {
      margin-top: 30px;
      padding: 45px 30px;
      border-radius: 16px;
      background: linear-gradient(120deg, #0f4c81, #2575c4);
      color: #fff;
      text-align: center;
      box-shadow: 0 12px 30px rgba(15, 76, 129, 0.25);
      position: relative;
    }

    .hero a { color: #fff; text-decoration: underline; }

    .figure-wrap {
      width: 100%;
      margin: 18px auto;
    }

    .responsive-img {
      width: 100%;
      height: auto;
      display: block;
      border-radius: 10px;
      box-shadow: 0 10px 30px rgba(0, 0, 0, 0.08);
      background: #fff;
      padding: 6px;
    }

    .link-grid {
      max-width: 680px;
      margin: 18px auto 35px;
      padding: 15px;
      border-radius: 14px;
      background: #fff;
      box-shadow: 0 8px 18px rgba(31, 42, 55, 0.08);
    }

    .link-grid .link-item {
      margin-bottom: 12px;
    }

    .link-grid a {
      display: block;
      text-decoration: none;
      padding: 12px 6px;
      border-radius: 12px;
      font-weight: 600;
      color: #0f4c81;
      border: 1px solid rgba(37, 117, 196, 0.25);
      transition: box-shadow 0.2s ease, transform 0.2s ease;
    }

    .link-grid a:hover {
      transform: translateY(-2px);
      box-shadow: 0 10px 18px rgba(15, 76, 129, 0.15);
    }

    .link-grid svg {
      width: 38px;
      height: 38px;
      display: block;
      margin: 0 auto 5px;
    }

    .section-card {
      background: #fff;
      border-radius: 14px;
      padding: 16px 22px;
      margin-bottom: 18px;
      box-shadow: 0 10px 30px rgba(31, 42, 55, 0.08);
    }

    .figure-card {
      background: #fff;
      border-radius: 14px;
      padding: 16px 22px;
      box-shadow: 0 10px 30px rgba(31, 42, 55, 0.08);
    }

    .note-box {
      border-left: 5px solid #2575c4;
      background: #edf4ff;
    }

    .note-danger {
      border-left: 5px solid #d9534f;
      background: #ffeceb;
    }

    .table-latex {
      width: 100%;
      max-width: 854px;
      border-collapse: collapse;
      margin: 15px auto;
      font-size: 16px;
    }

    .table-latex.table-wide {
      max-width: 1151px;
    }

    .table-wrapper {
      width: 100%;
      overflow-x: auto;
      -webkit-overflow-scrolling: touch;
    }

    /* Mobile responsive styles */
    @media (max-width: 768px) {
      body {
        font-size: 16px;
      }

      .hero {
        padding: 25px 15px;
        margin-top: 15px;
      }

      .hero h1 {
        font-size: 1.5rem;
      }

      .hero p {
        font-size: 14px !important;
      }

      .section-card,
      .figure-card {
        padding: 12px 14px;
      }

      .link-grid {
        padding: 10px;
      }

      .link-grid a {
        padding: 10px 4px;
        font-size: 14px;
      }

      .link-grid svg {
        width: 28px;
        height: 28px;
      }

      .table-latex {
        font-size: 12px;
      }

      .table-latex th,
      .table-latex td {
        padding: 4px 5px;
      }

      h2 {
        font-size: 1.4rem;
      }

      h3 {
        font-size: 1.2rem;
      }

      /* Make formulas fit mobile screen */
      mjx-container {
        overflow-x: auto;
        max-width: 100%;
        font-size: 80% !important;
      }

      .formula-wrap {
        overflow-x: auto;
        -webkit-overflow-scrolling: touch;
      }
    }

    .table-latex th,
    .table-latex td {
      border: none;
      padding: 6px 10px;
      text-align: center;
    }

    .table-latex th:first-child,
    .table-latex td:first-child {
      text-align: left;
    }

    .table-latex thead tr:first-child th {
      border-bottom: 2px solid #000;
    }

    .table-latex thead tr:nth-child(2) th {
      border-bottom: 1px solid #000;
    }

    .table-latex tbody tr {
      border-bottom: 1px solid #bbb;
    }

    .table-latex tbody tr:last-child {
      border-bottom: 2px solid #000;
    }

    .table-latex td strong {
      font-weight: 700;
    }

    .theme-toggle {
      position: absolute;
      top: 16px;
      right: 16px;
      z-index: 2;
      padding: 6px 14px;
      border-radius: 999px;
      border: 1px solid rgba(15, 76, 129, 0.35);
      background: rgba(255, 255, 255, 0.9);
      color: #0f4c81;
      font-size: 14px;
      font-weight: 600;
      cursor: pointer;
      align-items: center;
      gap: 6px;
      display: inline-flex;
    }

    .theme-icon {
      width: 20px;
      height: 20px;
      fill: none;
      stroke: currentColor;
      stroke-width: 2;
      stroke-linecap: round;
      stroke-linejoin: round;
    }

    .icon-sun { display: none; }
    .icon-moon { display: inline-block; }

    body.theme-dark {
      background: #0f141b;
      color: #e6edf5;
    }

    body.theme-dark .theme-toggle {
      background: rgba(20, 27, 36, 0.9);
      color: #e6edf5;
      border-color: rgba(230, 237, 245, 0.3);
    }

    body.theme-dark .icon-sun { display: inline-block; }
    body.theme-dark .icon-moon { display: none; }

    body.theme-dark .hero {
      background: linear-gradient(120deg, #0b2336, #123b5c);
      box-shadow: 0 12px 30px rgba(0, 0, 0, 0.45);
    }

    body.theme-dark .hero a { color: #e6edf5; }

    body.theme-dark .link-grid,
    body.theme-dark .section-card,
    body.theme-dark .figure-card {
      background: #141b24;
      box-shadow: 0 10px 30px rgba(0, 0, 0, 0.4);
    }

    body.theme-dark .link-grid a {
      color: #cfe3f7;
      border-color: rgba(207, 227, 247, 0.25);
    }

    body.theme-dark .note-box {
      border-left-color: #5aa1dd;
      background: #1b2a36;
    }

    body.theme-dark .note-danger {
      border-left-color: #e07a73;
      background: #2a1b1c;
    }

    body.theme-dark .responsive-img {
      background: #10161e;
      box-shadow: 0 10px 30px rgba(0, 0, 0, 0.45);
    }

    body.theme-dark .table-latex thead tr:first-child th,
    body.theme-dark .table-latex thead tr:nth-child(2) th {
      border-bottom-color: #cbd5e1;
    }

    body.theme-dark .table-latex tbody tr {
      border-bottom-color: rgba(203, 213, 225, 0.4);
    }

    body.theme-dark .containersmall {
      background-color: #141b24 !important;
      color: #e6edf5;
    }

  </style>
</head>

<body>
  <div class="container hero">
    <button class="theme-toggle" type="button" id="themeToggle" aria-label="Switch to dark mode">
      <svg class="theme-icon icon-moon" viewBox="0 0 24 24" aria-hidden="true">
        <path d="M21 12.79A9 9 0 1 1 11.21 3a7 7 0 0 0 9.79 9.79z"></path>
      </svg>
      <svg class="theme-icon icon-sun" viewBox="0 0 24 24" aria-hidden="true">
        <circle cx="12" cy="12" r="5"></circle>
        <line x1="12" y1="1" x2="12" y2="3"></line>
        <line x1="12" y1="21" x2="12" y2="23"></line>
        <line x1="4.22" y1="4.22" x2="5.64" y2="5.64"></line>
        <line x1="18.36" y1="18.36" x2="19.78" y2="19.78"></line>
        <line x1="1" y1="12" x2="3" y2="12"></line>
        <line x1="21" y1="12" x2="23" y2="12"></line>
        <line x1="4.22" y1="19.78" x2="5.64" y2="18.36"></line>
        <line x1="18.36" y1="5.64" x2="19.78" y2="4.22"></line>
      </svg>
    </button>
    <h1>The Confidence Trap: Gender Bias and Predictive Certainty in LLMs</h1>
    <p style="font-size: 20px;">
      <a href="https://ahmed.jp/">Ahmed Sabir<sup>1</sup></a>,
      <a href="http://linkedin.com/in/markus-k%C3%A4ngsepp-10a95a142/">Markus Kängsepp<sup>1</sup></a>,
      <a href="https://rajeshsharmacss.github.io/">Rajesh Sharma<sup>1,2</sup></a>
    </p>
    <p style="font-size: 18px;">
      <sup>1</sup> Institute of Computer Science, University of Tartu, Estonia<br />
      <sup>2</sup> School of AI and CS, Plaksha University, India
    </p>
  </div>

  <div class="container link-grid text-center">
    <div class="row">
      <div class="col-sm-4 col-xs-12 link-item">
        <a href="https://arxiv.org/pdf/2601.07806" title="Paper">
          <svg viewBox="0 0 24 24">
            <path fill="currentColor"
              d="M16 0H8C6.9 0 6 .9 6 2V18C6 19.1 6.9 20 8 20H20C21.1 20 22 19.1 22 18V6L16 0M20 18H8V2H15V7H20V18M4 4V22H20V24H4C2.9 24 2 23.1 2 22V4H4M10 10V12H18V10H10M10 14V16H15V14H10Z" />
          </svg>
          Paper
        </a>
      </div>
      <div class="col-sm-4 col-xs-12 link-item">
        <a href="https://github.com/ahmedssabir/GECE" title="Code">
          <svg viewBox="0 0 65 65">
            <path fill="currentColor"
              d="M32 0a32.021 32.021 0 0 0-10.1 62.4c1.6.3 2.2-.7 2.2-1.5v-6c-8.9 1.9-10.8-3.8-10.8-3.8-1.5-3.7-3.6-4.7-3.6-4.7-2.9-2 .2-1.9.2-1.9 3.2.2 4.9 3.3 4.9 3.3 2.9 4.9 7.5 3.5 9.3 2.7a6.93 6.93 0 0 1 2-4.3c-7.1-.8-14.6-3.6-14.6-15.8a12.27 12.27 0 0 1 3.3-8.6 11.965 11.965 0 0 1 .3-8.5s2.7-.9 8.8 3.3a30.873 30.873 0 0 1 8-1.1 30.292 30.292 0 0 1 8 1.1c6.1-4.1 8.8-3.3 8.8-3.3a11.965 11.965 0 0 1 .3 8.5 12.1 12.1 0 0 1 3.3 8.6c0 12.3-7.5 15-14.6 15.8a7.746 7.746 0 0 1 2.2 5.9v8.8c0 .9.6 1.8 2.2 1.5A32.021 32.021 0 0 0 32 0z" />
          </svg>
          Code
        </a>
      </div>
      <div class="col-sm-4 col-xs-12 link-item">
        <a href="AAAI_slides_2026.pdf" title="Slides">
          <svg viewBox="0 0 24 24">
            <path fill="currentColor"
              d="M21,16.5C21,16.88 20.79,17.21 20.47,17.38L12.57,21.82C12.41,21.94 12.21,22 12,22C11.79,22 11.59,21.94 11.43,21.82L3.53,17.38C3.21,17.21 3,16.88 3,16.5V7.5C3,7.12 3.21,6.79 3.53,6.62L11.43,2.18C11.59,2.06 11.79,2 12,2C12.21,2 12.41,2.06 12.57,2.18L20.47,6.62C20.79,6.79 21,7.12 21,7.5V16.5M12,4.15L6.04,7.5L12,10.85L17.96,7.5L12,4.15M5,15.91L11,19.29V12.58L5,9.21V15.91M19,15.91V9.21L13,12.58V19.29L19,15.91Z" />
          </svg>
          Slides
        </a>
      </div>
    </div>
  </div>

  <div class="container">
    <h2 style="text-align: center;">Abstract</h2>
    <div class="section-card">
      <p align="justify">
        The increased use of Large Language Models (LLMs) in sensitive domains leads to growing interest in how their confidence
        scores correspond to fairness and bias. This study examines the alignment between LLM-predicted confidence and
        human-annotated bias judgments. Focusing on gender bias, the research investigates probability confidence calibration in
        contexts involving gendered pronoun resolution. The goal is to evaluate if calibration metrics based on predicted
        confidence scores effectively capture fairness-related disparities in LLMs. The results show that, among the six
        state-of-the-art models, Gemma-2 demonstrates the worst calibration according to the gender bias benchmark. The primary
        contribution of this work is a fairness-aware evaluation of LLMs’ confidence calibration, offering guidance for ethical
        deployment. In addition, we introduce a new calibration metric, Gender-ECE, designed to measure gender disparities in
        resolution tasks.
      </p>
    </div>
  </div>

  <div class="container">
    <h2 style="text-align: center;">Gender-ECE</h2>
    <div class="section-card">
      <p>
    
Calibration metrics include ECE, MacroCE, ICE, and the Brier score,
which are commonly used to evaluate calibration in LLMs. These metrics
typically evaluate binary prediction outcomes &mdash; in our case, two sentence
completions: one with a male pronoun and one with a female pronoun. However,
aggregate calibration measures such as ECE do not provide information about how
the model behaves differently with respect to these two outcomes, i.e., male
versus female pronouns. To address this limitation, we propose
<strong>Gender-Aware Group ECE</strong>, a metric that provides explicit information
about calibration disparities across gendered pronouns.
      </p>
    </div>
    <div class="formula-wrap" style="text-align: center; margin-bottom: 10px;">
      <p>
        $$ \text{Gender-ECE} = \frac{1}{2} (\text{ECE}_{\text{male}} + \text{ECE}_{\text{female}}) $$
      </p>
      <p>
        $$ \text{ECE}_{\text{male/female}} = \sum_{m=1}^{M} \frac{|B_m|}{n} \left| \text{acc}(B_m) - \text{conf}(B_m) \right|, \quad \forall \hat{y}_i \in \{1,0\} $$
      </p>
    </div>
  </div>

  <div class="container">
    <h2 style="text-align: center;">Experiment: Last Cloze Task</h2>
    <div class="section-card">
      <p>
        The primary focus of the analysis is to assess the models’ bias and confidence in predicting pronouns at the end of sentences,
        GenderLex dataset (last cloze), meaning that the model has access to the full context of the sentence before scoring a biased
        pronoun.
      </p>
    </div>

    <div class="table-wrapper">
      <table class="table-latex">
        <thead>
          <tr>
            <th rowspan="2">Model</th>
            <th colspan="4">Standard Calibration Metrics</th>
            <th colspan="3">Gender-ECE</th>
            <th rowspan="2">Human</th>
          </tr>
          <tr>
            <th>ECE</th>
            <th>MacroCE</th>
            <th>ICE</th>
            <th>Brier Score</th>
            <th>Group</th>
            <th>M</th>
            <th>F</th>
          </tr>
        </thead>
        <tbody>
          <tr>
            <td>GPT-J-6B</td>
            <td>0.076</td>
            <td>0.453</td>
            <td>0.374</td>
            <td>0.432</td>
            <td>0.076</td>
            <td>0.085</td>
            <td>0.066</td>
            <td>0.715</td>
          </tr>
          <tr>
            <td>LLAMA-3.1-8B</td>
            <td>0.111</td>
            <td>0.466</td>
            <td>0.371</td>
            <td>0.446</td>
            <td>0.111</td>
            <td>0.112</td>
            <td>0.109</td>
            <td><strong>0.727</strong></td>
          </tr>
          <tr>
            <td>Gemma-2-9B</td>
            <td><strong>0.327</strong></td>
            <td><strong>0.493</strong></td>
            <td>0.390</td>
            <td><strong>0.559</strong></td>
            <td><strong>0.267</strong></td>
            <td><strong>0.330</strong></td>
            <td>0.204</td>
            <td>0.617</td>
          </tr>
          <tr>
            <td>Qwen2.5-7B</td>
            <td>0.106</td>
            <td>0.476</td>
            <td>0.422</td>
            <td>0.385</td>
            <td>0.107</td>
            <td>0.052</td>
            <td>0.162</td>
            <td>0.637</td>
          </tr>
          <tr>
            <td>Falcon-3-7B</td>
            <td>0.161</td>
            <td>0.491</td>
            <td><strong>0.449</strong></td>
            <td>0.356</td>
            <td>0.149</td>
            <td>0.081</td>
            <td><strong>0.217</strong></td>
            <td>0.605</td>
          </tr>
          <tr>
            <td>DeepSeek-8B</td>
            <td>0.085</td>
            <td>0.461</td>
            <td>0.369</td>
            <td>0.470</td>
            <td>0.090</td>
            <td>0.074</td>
            <td>0.106</td>
            <td>0.686</td>
          </tr>
        </tbody>
      </table>
    </div>
    <div class="figure-wrap figure-card">
      <img src="evaluation_last_cloze_task.png" class="responsive-img" alt="Calibration histograms for different models" />
    </div>
    <div class="section-card note-danger" style="margin: 10px auto; max-width: 900px;">
      <p style="margin: 0;">
        <strong>Finding:</strong> GPT-J-6B exhibits the best calibration (lowest ECE), while Gemma-2-9B performs the worst overall, consistently producing
        incorrect outcomes with a high disparity toward the female group.
      </p>
    </div>
    <!-- <p align="center">
      <img src="Example.png" class="responsive-img" width="850" alt="Examples of gender bias predictions" />
    </p> -->
  </div>

  <div class="container">
    <h2 style="text-align: center;">Experiment: Conference Resolution</h2>
    <div class="section-card">
      <p>
        The main objective is to evaluate the model’s confidence in coreference resolution  (WinoBias dataset) by measuring the model’s preferences when resolving pronouns.
      </p>
    </div>
    <!-- Full WinoBias/WinoGender table kept for future reference
    <div class="table-wrapper">
      <table class="table-latex table-wide">
        ...
      </table>
    </div>
    -->
    <div class="table-wrapper">
      <table class="table-latex">
        <thead>
          <tr>
            <th rowspan="2">Model</th>
            <th colspan="4">Standard Metrics</th>
            <th colspan="3">Gender-ECE</th>
            <th rowspan="2">Human</th>
          </tr>
          <tr>
            <th>ECE</th>
            <th>MacroCE</th>
            <th>ICE</th>
            <th>Brier</th>
            <th>Group</th>
            <th>M</th>
            <th>F</th>
          </tr>
        </thead>
        <tbody>
          <tr>
            <td>GPT-J-6B</td>
            <td>0.157</td>
            <td>0.444</td>
            <td>0.356</td>
            <td>0.481</td>
            <td>0.164</td>
            <td>0.150</td>
            <td>0.179</td>
            <td>0.686</td>
          </tr>
          <tr>
            <td>LLAMA-3.1-8B</td>
            <td>0.193</td>
            <td>0.460</td>
            <td>0.377</td>
            <td>0.460</td>
            <td>0.214</td>
            <td>0.179</td>
              <td><strong>0.249</strong></td>
              <td>0.662</td>
            </tr>
            <tr>
              <td>Gemma-2-9B</td>
            <td><strong>0.429</strong></td>
            <td><strong>0.490</strong></td>
            <td><strong>0.482</strong></td>
            <td>0.467</td>
            <td><strong>0.297</strong></td>
            <td><strong>0.438</strong></td>
            <td>0.156</td>
            <td>0.509</td>
          </tr>
          <tr>
            <td>Qwen2.5-7B</td>
            <td>0.234</td>
            <td>0.442</td>
            <td>0.362</td>
            <td><strong>0.510</strong></td>
            <td>0.190</td>
            <td>0.259</td>
            <td>0.121</td>
            <td>0.630</td>
          </tr>
          <tr>
            <td>Falcon-3-7B</td>
            <td>0.154</td>
            <td>0.452</td>
            <td>0.357</td>
            <td>0.487</td>
            <td>0.149</td>
            <td>0.160</td>
            <td>0.138</td>
            <td>0.684</td>
          </tr>
          <tr>
            <td>DeepSeek-8B</td>
            <td>0.240</td>
            <td>0.478</td>
            <td>0.382</td>
            <td>0.496</td>
            <td>0.218</td>
            <td>0.255</td>
              <td>0.182</td>
              <td>0.648</td>
            </tr>
          </tbody>
      </table>
    </div>
    <div class="figure-wrap figure-card">
      <img src="evaluation_conference_resolution.png" class="responsive-img" alt="Calibration histograms for the conference resolution task" />
    </div>
    <div class="section-card note-danger" style="margin: 10px auto; max-width: 900px;">
      <p>
        <strong>Finding:</strong> Gemma-2-9B exhibits the worst calibration overall and a preference for male pronouns. GPT-J-6B and Falcon3-7B are the
        fairest models, with the lowest Gender-ECE and minimal differences between genders.
      </p>
    </div>
  </div>

  <div class="container">
    <h2 style="text-align: center;">Experiment: Gender-Neutral</h2>
    <div class="section-card">
      <p>
        
  We investigate model behavior under gender-neutral conditions by replacing 
  occupation titles in the GenderLex dataset with gender-neutral terms such as <em>person</em> and <em>someone</em>.
      </p>
    </div>
    <div class="table-wrapper">
      <table class="table-latex table-wide">
        <thead>
          <tr>
            <th rowspan="2">Metric</th>
            <th colspan="3">GPT-J-6B</th>
            <th colspan="3">LLAMA-3.1-8B</th>
            <th colspan="3">DeepSeek-R1-8B</th>
            <th colspan="3">Gemma-2-9B</th>
          </tr>
          <tr>
            <th>Someone</th>
            <th>Person</th>
            <th>Occ</th>
            <th>Someone</th>
            <th>Person</th>
            <th>Occ</th>
            <th>Someone</th>
            <th>Person</th>
            <th>Occ</th>
            <th>Someone</th>
            <th>Person</th>
            <th>Occ</th>
          </tr>
        </thead>
        <tbody>
          <tr>
            <td>ECE</td>
            <td><strong>0.144</strong></td>
            <td>0.063</td>
            <td>0.076</td>
            <td>0.134</td>
            <td><strong>0.138</strong></td>
            <td>0.111</td>
            <td><strong>0.139</strong></td>
            <td>0.130</td>
            <td>0.085</td>
            <td>0.364</td>
            <td><strong>0.367</strong></td>
            <td>0.327</td>
          </tr>
          <tr>
            <td>MacroCE</td>
            <td><strong>0.484</strong></td>
            <td>0.476</td>
            <td>0.453</td>
            <td><strong>0.483</strong></td>
            <td>0.478</td>
            <td>0.466</td>
            <td><strong>0.482</strong></td>
            <td>0.481</td>
            <td>0.461</td>
            <td>0.493</td>
            <td>0.493</td>
            <td><strong>0.494</strong></td>
          </tr>
          <tr>
            <td>ICE</td>
            <td><strong>0.454</strong></td>
            <td>0.442</td>
            <td>0.374</td>
            <td>0.436</td>
            <td><strong>0.445</strong></td>
            <td>0.371</td>
            <td><strong>0.452</strong></td>
            <td>0.443</td>
            <td>0.369</td>
            <td><strong>0.397</strong></td>
            <td>0.393</td>
            <td>0.390</td>
          </tr>
          <tr>
            <td>Brier Score</td>
            <td>0.331</td>
            <td>0.341</td>
            <td><strong>0.432</strong></td>
            <td>0.358</td>
            <td>0.348</td>
            <td><strong>0.446</strong></td>
            <td>0.352</td>
            <td>0.361</td>
            <td><strong>0.470</strong></td>
            <td>0.560</td>
            <td><strong>0.581</strong></td>
            <td>0.574</td>
          </tr>
          <tr>
            <td>Group-ECE</td>
            <td><strong>0.138</strong></td>
            <td>0.077</td>
            <td>0.076</td>
            <td>0.132</td>
            <td>0.111</td>
            <td><strong>0.138</strong></td>
            <td><strong>0.138</strong></td>
            <td>0.137</td>
            <td>0.097</td>
            <td><strong>0.450</strong></td>
            <td>0.351</td>
            <td>0.267</td>
          </tr>
          <tr>
            <td>+ Male</td>
            <td><strong>0.106</strong></td>
            <td>0.031</td>
            <td>0.085</td>
            <td><strong>0.115</strong></td>
            <td>0.071</td>
            <td>0.112</td>
            <td>0.048</td>
            <td>0.065</td>
            <td><strong>0.074</strong></td>
            <td>0.363</td>
            <td><strong>0.367</strong></td>
            <td>0.330</td>
          </tr>
          <tr>
            <td>+ Female</td>
            <td><strong>0.170</strong></td>
            <td>0.122</td>
            <td>0.066</td>
            <td><strong>0.148</strong></td>
            <td>0.109</td>
            <td>0.109</td>
            <td><strong>0.228</strong></td>
            <td>0.210</td>
            <td>0.216</td>
            <td><strong>0.536</strong></td>
            <td>0.335</td>
            <td>0.204</td>
          </tr>
          <tr>
            <td>Human alignment</td>
            <td>0.598</td>
            <td>0.616</td>
            <td><strong>0.715</strong></td>
            <td>0.638</td>
            <td>0.598</td>
            <td><strong>0.727</strong></td>
            <td>0.578</td>
            <td>0.596</td>
            <td><strong>0.687</strong></td>
            <td>0.603</td>
            <td>0.606</td>
            <td><strong>0.618</strong></td>
          </tr>
        </tbody>
      </table>
    </div>
    <div class="figure-wrap figure-card">
      <img src="evaluation_GN_someone.png" class="responsive-img" alt="Calibration histograms for gender-neutral prompts (someone/person/occupation)" />
    </div>
    <div class="section-card note-danger" style="margin: 10px auto; max-width: 900px;">
      <p>
        <strong>Finding:</strong> Explicit role titles improve model calibration, whereas gender-neutral
terms increase calibration error due to ambiguity.
      </p>
    </div>
  </div>

  

  <div class="container">
    <h2 style="text-align: center;">Calibration</h2>
    <div class="section-card">
      <p>
        Our findings show that most models are poorly calibrated, remaining over-confident despite frequent prediction errors. We therefore
        apply Beta calibration (Kull, 2017) using a balanced calibration set (50:50), which reduces ECE and better aligns predicted confidences
        with empirical accuracy.
      </p>
    </div>
    <div class="figure-wrap figure-card">
      <img src="calibration_diagnostics.png" class="responsive-img" alt="Calibration comparison histograms" />
    </div>
  </div>

  <div class="container">
    <h3 style="text-align: center;">Acknowledgment</h3>
    <div style="text-align: left; background-color: #ffffff; padding: 10px; border-radius: 5px;" class="containersmall">
      <p>
        This work was supported by the Estonian Research Council grant “Developing human-centric digital solutions” (TEM TA120) and by the
        Estonian Centre of Excellence in Artificial Intelligence (EXAI), funded by the Estonian Ministry of Education and Research and
        co-funded by the European Union and the Estonian Research Council via project TEM TA119. It was funded by the EU H2020 program under
        the SoBigData++ project (grant agreement No. 871042) and partially funded by the HAMISON project.
      </p>
    </div>
  </div>

  <div class="container">
    <div class="box" style="text-align: left;">
      <div class="section-card">
        <h3>Citation</h3>
        <pre class="citation" style="white-space: pre-wrap; overflow-wrap: break-word; text-align: left; direction: ltr; margin: 0;">
@article{sabir2026confidence,
  title={The Confidence Trap: Gender Bias and Predictive Certainty in LLMs},
  author={Sabir, Ahmed and K{\"a}ngsepp, Markus and Sharma, Rajesh},
  journal={arXiv preprint arXiv:2601.07806},
  year={2026}
}
        </pre>
      </div>
    </div>
    <!-- <div class="section-card">
      <p>Contact: <a href="mailto:blank@ut.ee">Ahmed Sabir</a></p>
    </div>
  </div> -->
    <div class="container">
    <div class="section-card">
      <p>
     This project page was inspired by the 
<a href="https://nerfies.github.io/">Nerfies</a> project page.
You are free to borrow the source <a href="https://github.com/sabirdvd/Academic-project-page-temp">code</a> of this website.
      </p>
    </div>
  </div>
  <script>
    (function () {
      var button = document.getElementById('themeToggle');
      if (!button) return;
      
      // Mobile starts in dark mode
      if (window.innerWidth <= 768) {
        document.body.classList.add('theme-dark');
        button.setAttribute('aria-label', 'Switch to light mode');
      }
      
      button.addEventListener('click', function () {
        var isDark = document.body.classList.toggle('theme-dark');
        button.setAttribute('aria-label', isDark ? 'Switch to light mode' : 'Switch to dark mode');
      });
    })();
  </script>
</body>
</html>
