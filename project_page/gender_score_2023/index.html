
<!DOCTYPE html PUBLIC "-//W3C//DTD XHTML 1.0 Transitional//EN" "http://www.w3.org/TR/xhtml1/DTD/xhtml1-transitional.dtd">
<html xmlns="http://www.w3.org/1999/xhtml">
<head>
<meta http-equiv="Content-Type" content="text/html; charset=utf-8" />
<title>Women Wearing Lipstick: Measuring the Bias Between Object and Its Related Gender</title>

  <link rel="stylesheet" href="https://maxcdn.bootstrapcdn.com/bootstrap/3.2.0/css/bootstrap.min.css">
  <link href='http://fonts.googleapis.com/css?family=Lato:400,700' rel='stylesheet' type='text/css'>
  <link href="style.css" rel="stylesheet" type="text/css" />

<script type="text/javascript" src="../js/hidebib.js"></script>
  
<style> 
  .image-spacing { 
    margin-right: 10px; 
  } 
</style
  <style type="text/css">
  td {
    padding: 0 15px;
  }

  .row {
    border:1px solid #0137400a; 
}
  
  <style type="text/css">
  td {
    padding: 0 15px;
  }
</style>


</head>

<body> 

<div class="container">
  <table border="0" align="center">
    <tr><td width="623" align="center" valign="middle"><h2>Women Wearing Lipstick: </br> Measuring the Bias Between Object and Its Related Gender</h2></td></tr>
<!--     <tr><td width="300" align="center" valign="middle">   <a href="https://www.cs.upc.edu/~asabir/">Ahmed Sabir</a>, <a href="https://www.iri.upc.edu/people/fmoreno/">Francesc Moreno-Noguer</a>, 
         <a href="https://www.cs.upc.edu/~padro/">Lluís Padró</a></td></tr> -->
         <tr><td width="300" align="center" valign="middle">   <a href="">Anonymous Authors</a>
<!-- 
    <tr><td width="300" align="center" valign="middle"> Universitat Politècnica de Catalunya, TALP Research Center</a><sup>1</sup></td></tr>
    <tr><td width="300" align="center" valign="middle"> Institut de Robòtica i Informàtica Industrial, CSIC-UPC</a><sup>2</sup></td></tr> -->
  </table>
<!-- 
</br>
-->

  


<!--
<p align="center"> <img src="overview.png" width="500" align="middle" /></p> 
-->
<p align="center"> <img src="overview_bias.png" width="700" align="middle" /></p>

  
 <!-- <p align="center"> <img src="overview_v2.png" width="500" align="middle" /></p> -->
  
  



  


</table>
<table width="400px" align="center">
  <tr>
    <td align=center width=100px>
      <center>
        <ul class="nav">
          <li class="nav-item text-center">
              <a href="https://arxiv.org/abs/2301.08784" class="nav-link" title="Temp link">
                  <svg style="width:48px;height:48px" viewBox="0 0 24 24">
                      <path fill="currentColor" d="M16 0H8C6.9 0 6 .9 6 2V18C6 19.1 6.9 20 8 20H20C21.1 20 22 19.1 22 18V6L16 0M20 18H8V2H15V7H20V18M4 4V22H20V24H4C2.9 24 2 23.1 2 22V4H4M10 10V12H18V10H10M10 14V16H15V14H10Z" />
                  </svg><br>
                  Paper
              </a>
      </center>
    </td>
    <td align=center width=200px>
      <center>
        
        <ul class="nav">
        <li class="nav-item text-center">
          <a href="https://github.com/ahmedssabir/Textual-Visual-Semantic-Dataset" class="nav-link">
              <svg style="width:48px;height:48px" viewBox="0 0 65 65">
                  <path fill="currentColor" d="M32 0a32.021 32.021 0 0 0-10.1 62.4c1.6.3 2.2-.7 2.2-1.5v-6c-8.9 1.9-10.8-3.8-10.8-3.8-1.5-3.7-3.6-4.7-3.6-4.7-2.9-2 .2-1.9.2-1.9 3.2.2 4.9 3.3 4.9 3.3 2.9 4.9 7.5 3.5 9.3 2.7a6.93 6.93 0 0 1 2-4.3c-7.1-.8-14.6-3.6-14.6-15.8a12.27 12.27 0 0 1 3.3-8.6 11.965 11.965 0 0 1 .3-8.5s2.7-.9 8.8 3.3a30.873 30.873 0 0 1 8-1.1 30.292 30.292 0 0 1 8 1.1c6.1-4.1 8.8-3.3 8.8-3.3a11.965 11.965 0 0 1 .3 8.5 12.1 12.1 0 0 1 3.3 8.6c0 12.3-7.5 15-14.6 15.8a7.746 7.746 0 0 1 2.2 5.9v8.8c0 .9.6 1.8 2.2 1.5A32.021 32.021 0 0 0 32 0z" />
              </svg> <br>
             
              Code
          </a>
      </center>
      <td align=center width=100px>
        <center>
          <ul class="nav">
          <li class="nav-item text-center">
            <a href="https://huggingface.co/datasets/AhmedSSabir/Textual-Image-Caption-Dataset" class="nav-link">
              <svg style="width:48px;height:48px" viewBox="0 0 24 24">
                <path fill="currentColor" d="M21,16.5C21,16.88 20.79,17.21 20.47,17.38L12.57,21.82C12.41,21.94 12.21,22 12,22C11.79,22 11.59,21.94 11.43,21.82L3.53,17.38C3.21,17.21 3,16.88 3,16.5V7.5C3,7.12 3.21,6.79 3.53,6.62L11.43,2.18C11.59,2.06 11.79,2 12,2C12.21,2 12.41,2.06 12.57,2.18L20.47,6.62C20.79,6.79 21,7.12 21,7.5V16.5M12,4.15L6.04,7.5L12,10.85L17.96,7.5L12,4.15M5,15.91L11,19.29V12.58L5,9.21V15.91M19,15.91V9.21L13,12.58V19.29L19,15.91Z" />
              </svg><br>
              data
            </a>
                  </ul>
        </center>
    </td>
</table>

  
</div>

</br>

<div class="container">
  <h2>Abstract</h2>

  

  




    <p align="justify">    In this paper, we investigate the impact of objects on gender bias in image caption systems. Our results show that only gender-specific objects have a strong gender bias (e.g., <em>woman-lipstick</em>). 
        In addition, we present a visual semantic-based gender score that measures the degree of bias  and can be used as a plug-in for any image captioning system. Our experiments demonstrate the utility of 
        the gender score, since we observe that our score can measure the bias relation between a caption and its related gender better than the existing approach Object Gender Co-Occurrence.

  
      <!-- Modern image captaining relies heavily on extracting knowledge, from images such as objects,
      to capture the concept of a static story in an image. 
      In this paper, we propose a textual visual context dataset for captioning, where the publicly available 
      dataset COCO Captions (<a href="https://arxiv.org/pdf/1405.0312.pdf">Lin et al., 2014</a>) 
      has been extended with information about the scene (such as objects in the image). Since this information has textual form, 
      it can be used to leverage any NLP task, such as text similarity or semantic relation methods, into captioning systems, 
      either as an end-to-end training strategy or a post-processing based approach.
      --> 
      
  </a></tr>
</div>

</br>


<div class="container" text-align="left">
  <h2>Overview</h2>
    <table border="0" align="left">
      <tr><td width="1000" align="justify"> 
        <p align="justify"> 
            In this work, we proposed two object-to-gender bias scores: (1) direct gender score, and (2) a Mask based gender score estimation.
            For the direct score, the model uses the visual context to predict the gender and thus predict the object related bias.
             Also, inspired by Mask Language Model, the model can estimate the Mask gender using the object context in the sentence.

          
          <!-- Finally, to take advantage 
          of the visual overlap  between caption and visual context,  
          and to extract global information, we use BERT followed by a shallow CNN (<a href="https://arxiv.org/abs/1408.5882">Kim, 2014</a>)
          to estimate the visual relatedness using the caption description. -->
       </br>
        </br>

        
        
  
        <!--
    Here is a  <a href="https://colab.research.google.com/drive/1N0JVa6y8FKGLLSpiG7hd_W75UYhHRe2j?usp=sharing">Colab</a> demo </td></tr>
    -->
        For quick start please have a look at the <a href="https://github.com/ahmedssabir/Textual-Visual-Semantic-Dataset/blob/main/BERT_CNN_Visual_re_ranker_demo.ipynb">demo</a></tr>

       
        
   
    
  
      <tr><td width="1000" align="left"> 
        <div class="paper" id="ap_agent">
          <h3>Proposed Approach</h3>
           <p>  The direct Gender Score and the Gender Estimation 
            Score  relies on the visual information to predict the related gender as follows: </p>
            <!--
            We also propose a strategy to estimate the most closely related/not-related visual
concepts using the caption description. In partuoular, We employ BERT followed by a shallow CNN
          to estimate the visual relatedness using the caption description.
-->
            
            
      <!--      We also proposed a data filtering strategy and visual semantic model to
   
            estimate the degree of the relatedness between the caption and its related visual context in the image. 

--> 
            
<!--
     In this work, we also propose a visual context-based COCO dataset and visual context 
            baseline that can measure the degree of the relatedness between the caption and its related visual context in the image. 
 -->
            
            
 <p><strong>Gender Score:</strong></p>
 <p align="center"> <img src="COCO_val2014_000000175024.jpg" width="300" align="middle" /></p>
 <a shape="rect" href="javascript:togglebib('ap_agent')" class="togglebib">Visual Classifier<sub>ResNet/CLIP</sub></a>
 <pre xml:space="preserve">
   visual_context_label = 'motor scooter'
   visual_context_prob = 0.2183
 </pre>       
</div>
<tr><td width="1000" align="left"> 
 <div class="paper" id="ap_role_scenario1">
   <a shape="rect" href="javascript:togglebib('ap_role_scenario1')" class="togglebib">Gender Score<sub>man/woman</sub> </a>
   <pre >
     a ____ riding a motorcycle on a road - man --> score 
     a ____ riding a motorcycle on a road - woman --> score 
    
   </pre>       
 </div>

<tr><td width="1000" align="left"> 
<div class="paper" id="ap_agent">
 <p><strong>Gender Score Estimation:</strong></p>
 <p align="center"> <img src="COCO_val2014_000000437325.jpg" width="300" align="middle" /></p>
 <a shape="rect" href="javascript:togglebib('ap_agent')" class="togglebib">Visual Classifier<sub>ResNet/CLIP</sub></a>
 <pre xml:space="preserve">
   visual_context_label = 'remote control'
   visual_context_prob = 0.2183
  </pre>       
</div>
<tr><td width="1000" align="left"> 
  <div class="paper" id="ap_role_scenario1">
    <a shape="rect" href="javascript:togglebib('ap_role_scenario1')" class="togglebib">Gender Score Estimation<sub>MASK</sub> </a>
    <pre >
      a <em> [MASK] </em> playing a video game in a living room (man/woman)
    </pre>          
            
         
</p>

    <tr><td width="1000" align="left"> 
    <div class="paper" id="ap_agent">
  
      <h3><strong>Comparison against GPT-2 and Cosine Score</strong></h3>
      <p align="justify"> Comparison result on the test set of the Gender Score bias. (toward  women <span style="color: #f5c6c6">■</span> or men <span style="color: #7ebcd6">■</span>)  between  two  different  pre-trained models  in  training dataset size BLIP 129M (unsupervised) and VilBERT 3.5M.
        Our proposed Belief Revision  (BR) based Gender Score <span style="color: #f5b87e">■</span> balance the amplified bias as the model getting bigger, the more amplified bias against man or woman. </p>
      <p align="center"> <img src="man_BLIP.png" width="300" align="middle"   class="image-spacing" />
    <img src="man_Vilbert.png" width="300" align="middle" /></p>
      <!-- <p align="center"> <img src="man_Transformer.png" width="300" align="middle" /></p> -->
      <p align="center"> <img src="woman_BLIP.png" width="300" align="middle" class="image-spacing" 
       align="center"> <img src="woman_Vilbert.png" width="300" align="middle" /></p>
      <!-- <p align="center"> <img src="woman_Transformer.png" width="300" align="middle" /></p> -->
  
      <!--
             <a shape="rect" href="javascript:togglebib('ap_agent')" class="togglebib">Beam Search<sub>baseline</sub></a>
         <pre xml:space="preserve">
       s
  </td></tr>
  -->
  
  </table> 


<!---      
<p align="center"> <img src="BERT-CNN.png" width="300" align="middle" /></p>
-->
<tr><td width="1000" align="left"> 
    <tr><td width="1000" align="left"> 
      <div class="paper" id="ap_agent">
        <h3><strong>Results</strong></h3>
        <P> The proposed score uses the correlation between the visual and
          its related gender. As shown in the figure, there is equal
          distribution of object-gender(man/woman), which indicate that not all
          object has a strong bias toward a specific gender.</P>
        <p align="center"> <img src="dist_figure.png" width="300" align="middle" /></p>
    <h3><strong>Examples</strong></h3>
    <P>Examples of Gender Score Estimation and
      Cosine Distance. The result shows that (Top) the score
      balances the bias (as man/woman have a similar bias
      with sport tennis), and (Bottom) adjusts the woman bias.</P>
    <p align="center"> <img src="gender.png" width="300" align="middle" /></p>
   
    


     
          
        
<!--
      <tr><td width="1000" align="left"> 
        <div class="paper" id="ap_role_scenario1">
          <a shape="rect" href="javascript:togglebib('ap_role_scenario1')" class="togglebib">unsupervised<sub>CC</sub> </a>
          <pre xml:space="preserve">
            some tea in a wooden bowl with a scoop and blue flowers next to it
            two women standing side by side at an event holding their certificates
            an elderly man rides a bicycle in the street while people walk around
            the soccer player has his arms up as he is celebrating
            a stream runs through a forested green and leafy area
            two teams of men playing a game in a basketball court
            three ladies looking at something while sitting next to each other
            there is a shark swimming in the blue water
            
          </pre>       
        </div>
      </td></tr>
-->
      
    </table> 
</div>

</table> 
</div>

</div>

</br>

<div style="text-align: left;" class="containersmall">
  <h3>Citation</h3>
<pre class='citation'>
 @article{sabir2023wo,
  title={Women Wearing Lipstick: Measuring the Bias Between Object and Its Related Gender},
  author={Anonymous Authors},
  journal={arXiv preprint arXiv:???},
  year={2023}
}
}
</div>
<div style="text-align: center;" class="containersmall">
  <p>Contact: <a href="mailto:asabir@cs.upc.edu">Ahmed Sabir</a></p>
</div>
<script xml:space="preserve" language="JavaScript">
hideallbibs();
<script xml:space="preserve" language="JavaScript">
hideallbibs();
</script>

</body>
</html>
