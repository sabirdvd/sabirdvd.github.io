<!DOCTYPE html PUBLIC "-//W3C//DTD XHTML 1.0 Transitional//EN" "http://www.w3.org/TR/xhtml1/DTD/xhtml1-transitional.dtd">
<html xmlns="http://www.w3.org/1999/xhtml">
<head>
<meta http-equiv="Content-Type" content="text/html; charset=utf-8" />
<title>Women Wearing Lipstick: Measuring the Bias Between Object and Its Related Gender</title>

  <link rel="stylesheet" href="https://maxcdn.bootstrapcdn.com/bootstrap/3.2.0/css/bootstrap.min.css">
  <link href='http://fonts.googleapis.com/css?family=Lato:400,700' rel='stylesheet' type='text/css'>
  <link href="style.css" rel="stylesheet" type="text/css" />

<script type="text/javascript" src="../js/hidebib.js"></script>
  
<style> 
pre, code{
    white-space:normal;
}
pre, code{
    white-space:normal;
}
  .image-spacing { 
    margin-right: 10px; 
  } 
</style>
  <style type="text/css">
  td {
    padding: 0 15px;
  }

  .row {
    border:1px solid #0137400a; 
}
  
  <style type="text/css">
  td {
    padding: 0 15px;
  }
</style>


</head>

<body> 

<div class="container">
  <table border="0" align="center">
    <tr><td width="623" align="center" valign="middle"><h2>Women Wearing Lipstick: </br> Measuring the Bias Between Object and Its Related Gender</h2></td></tr>
    <tr><td width="300" align="center" valign="middle">   <a href="https://www.cs.upc.edu/~asabir/">Ahmed Sabir</a> and <a href="https://www.cs.upc.edu/~padro/">Lluís Padró</a></td></tr> 
     

     <!--   <tr><td width="300" align="center" valign="middle">   <a href="">Anonymous Authors</a> --> 
    <tr><td width="300" align="center" valign="middle"> Universitat Politècnica de Catalunya, TALP Research Center</a></td></tr>
   <!--   <tr><td width="300" align="center" valign="middle"> Institut de Robòtica i Informàtica Industrial, CSIC-UPC</a><sup>2</sup></td></tr> -->

  </table>

</br>


  


<!--
<p align="center"> <img src="overview.png" width="500" align="middle" /></p> 
-->
<!-- <p align="center"> <img src="overview_bias.png" width="750" align="middle" /></p> -->
  <p align="center"> <img src="overview_bias.png" width="800" align="middle" /></p>
  
 <!-- <p align="center"> <img src="overview_v2.png" width="500" align="middle" /></p> -->
  
  



  


</table>
<table width="400px" align="center">
  <tr>
    <td align=center width=100px>
      <center>
        <ul class="nav">
          <li class="nav-item text-center">
              <a href="https://arxiv.or" class="nav-link" title="Temp link">
                  <svg style="width:48px;height:48px" viewBox="0 0 24 24">
                      <path fill="currentColor" d="M16 0H8C6.9 0 6 .9 6 2V18C6 19.1 6.9 20 8 20H20C21.1 20 22 19.1 22 18V6L16 0M20 18H8V2H15V7H20V18M4 4V22H20V24H4C2.9 24 2 23.1 2 22V4H4M10 10V12H18V10H10M10 14V16H15V14H10Z" />
                  </svg><br>
                  Paper
              </a>
      </center>
    </td>
    <td align=center width=200px>
      <center>
        
        <ul class="nav">
        <li class="nav-item text-center">
          <a href="https://github.com/ahmedssabir/GenderScore" class="nav-link">
              <svg style="width:48px;height:48px" viewBox="0 0 65 65">
                  <path fill="currentColor" d="M32 0a32.021 32.021 0 0 0-10.1 62.4c1.6.3 2.2-.7 2.2-1.5v-6c-8.9 1.9-10.8-3.8-10.8-3.8-1.5-3.7-3.6-4.7-3.6-4.7-2.9-2 .2-1.9.2-1.9 3.2.2 4.9 3.3 4.9 3.3 2.9 4.9 7.5 3.5 9.3 2.7a6.93 6.93 0 0 1 2-4.3c-7.1-.8-14.6-3.6-14.6-15.8a12.27 12.27 0 0 1 3.3-8.6 11.965 11.965 0 0 1 .3-8.5s2.7-.9 8.8 3.3a30.873 30.873 0 0 1 8-1.1 30.292 30.292 0 0 1 8 1.1c6.1-4.1 8.8-3.3 8.8-3.3a11.965 11.965 0 0 1 .3 8.5 12.1 12.1 0 0 1 3.3 8.6c0 12.3-7.5 15-14.6 15.8a7.746 7.746 0 0 1 2.2 5.9v8.8c0 .9.6 1.8 2.2 1.5A32.021 32.021 0 0 0 32 0z" />
              </svg> <br>
             
              Code
          </a>
      </center>
      <td align=center width=100px>
        <center>
          <ul class="nav">
          <li class="nav-item text-center">
            <a href="https://huggingface.co" class="nav-link">
              <svg style="width:48px;height:48px" viewBox="0 0 24 24">
                <path fill="currentColor" d="M21,16.5C21,16.88 20.79,17.21 20.47,17.38L12.57,21.82C12.41,21.94 12.21,22 12,22C11.79,22 11.59,21.94 11.43,21.82L3.53,17.38C3.21,17.21 3,16.88 3,16.5V7.5C3,7.12 3.21,6.79 3.53,6.62L11.43,2.18C11.59,2.06 11.79,2 12,2C12.21,2 12.41,2.06 12.57,2.18L20.47,6.62C20.79,6.79 21,7.12 21,7.5V16.5M12,4.15L6.04,7.5L12,10.85L17.96,7.5L12,4.15M5,15.91L11,19.29V12.58L5,9.21V15.91M19,15.91V9.21L13,12.58V19.29L19,15.91Z" />
              </svg><br>
              demo
            </a>
                  </ul>
        </center>
    </td>
</table>

  
</div>

</br>

<div class="container">
  <h2>Abstract</h2>

  

  




    <p align="justify">    

  In this paper, we investigate the impact of objects on gender bias in image captioning systems. Our results show that only gender-specific objects have a strong gender bias (<em>e.g.</em> woman-lipstick). In addition, 
      we propose a visual semantic-based gender score that measures the degree of bias and can be used as a plug-in for any image captioning system. Our experiments demonstrate the utility of the gender score, since 
      we observe that our score can measure the bias relation between a caption and its related gender; therefore, our score can be used as an additional metric to the existing Object Gender Co-Occ approach. 
  
    
      
  </a></tr>
</div>

</br>


<div class="container" text-align="left">
  <h2>Overview</h2>
    <table border="0" align="left">
      <tr><td width="1000" align="justify"> 
        <p align="justify"> 
            In this work, we proposed two object-to-gender bias scores: (1) direct gender score, and (2) a [ MASK ]  based gender score estimation.
            For the direct score, the model uses the visual context to predict the gender and thus predict the object related bias.
             Also, inspired by the Mask Language Model, the model can estimate the Mask gender using the object context in the sentence.

          
       </br>
        </br>

        
        
  
        <!--
    Here is a  <a href="https://colab.research.google.com/drive/1N0JVa6y8FKGLLSpiG7hd_W75UYhHRe2j?usp=sharing">Colab</a> demo </td></tr>
    -->
        For quick start please have a look at the <a href="https://ahmed.jp/project_page/gender_score_2023/demo_1.html">demo</a></tr>

       
        
   
    
  
      <tr><td width="1000" align="left"> 
        <div class="paper" id="ap_agent">
          <h3>Proposed Approach</h3>
           <p> The direct Gender Score and the Gender Estimation Score rely on the visual information to predict the related gender as follows: </p>
       

            
            
 <p><strong>Gender Score:</strong></p>
 <p align="center"> <img src="COCO_val2014_000000175024.jpg" width="300" align="middle" /></p>
 <a shape="rect" href="javascript:togglebib('ap_agent')" class="togglebib">Visual Classifier<sub>CLIP</sub></a>
 <pre xml:space="preserve">
   visual_context_label = 'motor scooter' </br>
   visual_context_prob = 0.2183
</div>
<tr><td width="1000" align="left"> 
 <div class="paper" id="ap_role_scenario1">
   <a shape="rect" href="javascript:togglebib('ap_role_scenario1')" class="togglebib">Gender Score<sub>man/woman</sub> </a>
   <pre >
     a ____ riding a motorcycle on a road - object bias ratio to-man -> <strong>53.17%</strong> </br>
     a ____ riding a motorcycle on a road - object bias ratio to-woman -> 46.82% 
   </pre>       
 </div>


<tr><td width="1000" align="left"> 
<div class="paper" id="ap_agent">
 <p><strong>Gender Score Estimation:</strong></p>
 <p align="center"> <img src="COCO_val2014_000000437325.jpg" width="300" align="middle" /></p>
 <a shape="rect" href="javascript:togglebib('ap_agent')" class="togglebib">Visual Classifier<sub>CLIP</sub></a>
 <pre xml:space="preserve">
   visual_context_label = 'joystick' </br>
   visual_context_prob = 0.2732
</div>
<tr><td width="1000" align="left"> 
  <div class="paper" id="ap_role_scenario1">
    <a shape="rect" href="javascript:togglebib('ap_role_scenario1')" class="togglebib">Gender Score Estimation<sub>MASK</sub> </a>
    <pre xml:space="preserve">
      a <em> [MASK] </em> playing a video game in a living room (object bias based prediction:<strong>man</strong>) 
    </pre>          
            
         
</p>
  
      <div class="paper" id="ap_agent">
<!--         <h3><strong>Experiments & Results </strong></h3> -->

 <h3><strong>Experiments & Results </strong></h3> 

   Comparison result (<em>i.e.</em> baseline gender output) between Object Gender Co-Occ and our gender score estimation on the Karpathy split. The proposed score measures gender bias more accurately, particularly when there is a strong gender-to-object relation.

         
</P>
     <p align="center"> <img src="gender_score.png" width="380"   align="middle" /></p>
     

        
 <h3><strong>Qualitative results </strong></h3>

        
        

        
        <P> The proposed score uses the correlation between the visual and its related gender. As shown in the (Left) figure, there is equal distribution of object-gender (man and woman),
          which indicate that not all object has a strong bias toward a specific gender. (Right) the figure shows examples of Gender Score 
          Estimation and Cosine Distance. The result shows that (Top) the score  balances the bias (as men and women have a similar bias with sport tennis), and (Bottom) adjusts the woman bias while maintaining the strong
bias towards men.
        
        </P>
        <p align="center"> <img src="dist_figure.png" width="270" align="middle" 
          align="center"> <img src="gender.png" width="360" align="middle" /></p>
    <!-- <h3><strong>Examples</strong></h3>
    <P></P>
    <p align="center"> <img src="gender.png" width="370" align="middle" /></p>
   
     -->
  <h3><strong>Examples </strong></h3>
     <P> The Table below shows that our score has similar results (bias ratio) to the existing Object Gender
      Co-Occ approach on the most biased objects toward men. Note that TraCLIPS-Reward (CLIPS+CIDEr) inherits biases from RL-CLIPS, resulting in distinct gender predictions and generates caption w/o a specific gender <em> i.e.</em> person, baseball player, <em>etc</em>.
     
     </P>


          
    <p align="center"> <img src="tabel_all.png" width="630" align="middle" /></p>
    
    

     <h3><strong>Comparison against GPT-2 and Cosine Distance Score</strong></h3>
     <p align="justify"> Comparison result on the test set of the Gender Score bias. (toward  women <span style="color: #f5c6c6">■</span> or men <span style="color: #7ebcd6">■</span>)  between  two  different  pre-trained models  in  training dataset size BLIP 129M (unsupervised) and VilBERT 3.5M.
       Our proposed visual bias likelihood revision aka Belief Revision  (BR) based Gender Score <span style="color: #f5b87e">■</span> balance the amplified bias as the model getting bigger, the more amplified bias against men or women. </p>
     <p align="center"> <img src="man_BLIP.png" width="300" align="middle"   class="image-spacing" />
   <img src="man_Vilbert.png" width="300" align="middle" /></p>
     <!-- <p align="center"> <img src="man_Transformer.png" width="300" align="middle" /></p> -->
     <p align="center"> <img src="woman_BLIP.png" width="300" align="middle" class="image-spacing" 
      align="center"> <img src="woman_Vilbert.png" width="300" align="middle" /></p>
     <!-- <p align="center"> <img src="woman_Transformer.png" width="300" align="middle" /></p> -->



<div class="paper" id="ap_agent">
  <h3><strong>Case study</strong></h3> 
      We also apply our proposed gender score to general tasks such as short text Twitter, we utilized a subset of the Twitter user  <a href="https://www.kaggle.com/datasets/crowdflower/twitter-user-gender-classification">gender classification dataset</a>. We use a BERT based keyword extractor to extract the biased context from the sentence (<em>e.g.</em> travel-man, woman-family), and we then employ
the cloze probability to extract the probability of the context. We observe that some keywords have a strong
bias: women are associated with keywords such as novel, beauty, and hometown. Meanwhile, men are more frequently related to words such as gaming, coffee, and inspiration. The table below shows the Gender Bias score (highlighted in
<span style="color: red;">red</span> color) for each Tweet (X posts).


   
  
</P>
     <!-- <p align="center"> <img src="gender_score.png" width="380"   align="middle" /></p> -->
     <p align="center"> <img src="twitter.png" width="670"   align="middle" /></p>
     
    



        
     <!--
            <a shape="rect" href="javascript:togglebib('ap_agent')" class="togglebib">Beam Search<sub>baseline</sub></a>
        <pre xml:space="preserve">
      s
 </td></tr>
 -->
 
 </table> 
          
        



      
    </table> 
</div>

</table> 
</div>

</div>

</br>

<div style="text-align: left;" class="containersmall">
  <h3>Citation</h3>
<pre class='citation'>
 @article{sabir2023wo, </br>
  title={Women Wearing Lipstick: Measuring the Bias Between Object and Its Related Gender}, </br>
  author={Anonymous Authors}, </br>
  journal={arXiv preprint arXiv:???},</br>
  year={2023}</br>
}
</div>
<div style="text-align: center;" class="containersmall">
  <p>Contact: <a href="mailto:asabir@cs.upc.edu">Ahmed Sabir</a></p>
</div>
<script xml:space="preserve" language="JavaScript">
hideallbibs();
<script xml:space="preserve" language="JavaScript">
hideallbibs();
</script>

</body>
</html>
